{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup \n",
    "- `TA TODO`: Setup based on your environment. Reach out to me if you face any issues. Also, any feedback/improvements to the setup process for the students based on your experience setting up here would be very appreciated! (PS: I am still figuring out the best way to do this.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local \n",
    "\n",
    "The assignment is designed in a manner that you can do most of the work `implementation` locally. We would recommend that you pass all the tests locally using the `hw4_data_subset` we've provided before moving to a GPU runtime. To do this simply:\n",
    "- Create a new conda environment with `conda create -n hw4 python=3.12.4`\n",
    "- Activate the conda environment with `conda activate hw4`\n",
    "- Install the dependencies with `pip install -r requirements.txt`\n",
    "- Ensure that your notebook is in the same directory as the `handout` folder. (See the expected directory structure in the `README.md`)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Colab (`TA TODO`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Get Repo (TA-Only, will be handout for students)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `INTERNAL TODO`: Need to switch this to handout upload for students."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GITHUB_USERNAME = \"puru-samal\"\n",
    "REPO_NAME = \"IDL-HW4\"\n",
    "BRANCH_NAME = \"TA\"\n",
    "ACCESS_TOKEN = \"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\"\n",
    "repo_url = f\"https://{GITHUB_USERNAME}:{ACCESS_TOKEN}@github.com/{GITHUB_USERNAME}/{REPO_NAME}.git\"\n",
    "!git clone -b {BRANCH_NAME} {repo_url}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### If I announce a new commit, please delete and re-clone the repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf IDL-HW4/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Get Data\n",
    "- `INTERNAL TODO`: Need to switch this download from kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gdown 1-0e9Gnl4nm6wbIuE_Yxl2wRZI8yGxHm6 --output hw4_data.tar.gz\n",
    "!tar -xf hw4_data.tar.gz\n",
    "!rm -rf hw4_data.tar.gz\n",
    "!du -h max-depth=3 hw4_data_kaggle/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Install Dependencies\n",
    "- `NOTE`: Colab may prompt you to restart your runtime. Do so then proceed to the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --no-deps -r IDL-HW4/colab_requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Move to Project Directory\n",
    "- `NOTE`: You may have to repeat this on restarting your runtime. You can do a `pwd` to check if you are in the right directory.\n",
    "- `NOTE`: Your data directory should be one level up from your project directory. Keep this in mind when you are setting your `root` in the config file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('IDL-HW4')\n",
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PSC (`TA TODO`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Preliminaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `Step 0:` ssh into Bridges2 with `ssh username@bridges2.psc.edu`\n",
    "- `Step 1:` cd into your project directory with `cd $PROJECT`\n",
    "- `Step 2:` Load the anaconda module with `module load anaconda3`\n",
    "- `Step 3:` Create a new conda environment with `conda create -n your_env_name python=3.12.4`\n",
    "- `Step 4:` Activate the conda environment with `conda activate your_env_name` (Make sure to deactivate any existing conda environment first with `conda deactivate`)\n",
    "- `Step 5:` Install dependencies with `pip install -r /ocean/projects/cis240101p/psamal/psc_requirements.txt`\n",
    "- `Step 6:` Get a compute node with `interact -p GPU-shared --gres=gpu:v100-32:1 -t 8:00:00`\n",
    "- `Step 7:` Node allocation might have caused your conda environment to be deactivated, so follow `Step 4` again\n",
    "- `Step 8:` Now follow your usual steps to start a jupyter notebook. For me this is:\n",
    "  - Start a jupyter notebook with `jupyter notebook --no-browser --ip=0.0.0.0` \n",
    "  - On a separate terminal, start a tunnel with `ssh -L 8888:{hostname}:{port} bridges2.psc.edu -l username`\n",
    "  - Select the appropriate kernel on VSCode: Kernel -> Select Another Kernel -> Existing Jupyter Server -> `http://127.0.0.1:{port}/tree?token={token}`\n",
    "- `Step 9:` Now follow the instructions below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Get Repo (TA-Only, will be handout for students)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'IDL-HW4'...\n",
      "remote: Enumerating objects: 602, done.\u001b[K\n",
      "remote: Counting objects: 100% (602/602), done.\u001b[K\n",
      "remote: Compressing objects: 100% (514/514), done.\u001b[K\n",
      "remote: Total 602 (delta 150), reused 537 (delta 85), pack-reused 0 (from 0)\u001b[K\n",
      "Receiving objects: 100% (602/602), 14.07 MiB | 53.17 MiB/s, done.\n",
      "Resolving deltas: 100% (150/150), done.\n",
      "Updating files: 100% (410/410), done.\n"
     ]
    }
   ],
   "source": [
    "GITHUB_USERNAME = \"puru-samal\"\n",
    "REPO_NAME = \"IDL-HW4\"\n",
    "BRANCH_NAME = \"TA\"\n",
    "ACCESS_TOKEN = \"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\"\n",
    "repo_url = f\"https://{GITHUB_USERNAME}:{ACCESS_TOKEN}@github.com/{GITHUB_USERNAME}/{REPO_NAME}.git\"\n",
    "#!git clone -b {BRANCH_NAME} {repo_url} # TA ONLY\n",
    "!git clone {repo_url}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### If I announce a new commit, please delete and re-clone the repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf IDL-HW4/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Move to Project Directory\n",
    "- `NOTE`: You may have to repeat this on anytime you restart your runtime. You can do a `pwd` or `ls` to check if you are in the right directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "autograde-Makefile\thw4_data_subset  mytorch\n",
      "autograde.tar\t\thw4lib\t\t psc_requirements.txt\n",
      "colab_requirements.txt\tHW4P1_nb.ipynb\t README.md\n",
      "config.yaml\t\tHW4P2_nb.ipynb\t simulate_autolab.py\n",
      "download_data.py\thw4p2_sol.json\t tests\n",
      "expts\t\t\tMakefile\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir('IDL-HW4')\n",
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Get Data\n",
    "- `NOTE`: We are using `$LOCAL`: the scratch storage on local disk on the node running a job to store out data. Disk accesses are much faster than what you would get from `$PROJECT` storage, but `IT IS NOT PERSISTENT`. \n",
    "- `NOTE`: Make sure you have a node allocated to you with `interact -p GPU-shared --gres=gpu:v100-32:1 -t 8:00:00`\n",
    "- Read more about it PSC File Spaces [here](https://www.psc.edu/resources/bridges-2/user-guide#file-spaces)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=1-0e9Gnl4nm6wbIuE_Yxl2wRZI8yGxHm6\n",
      "From (redirected): https://drive.google.com/uc?id=1-0e9Gnl4nm6wbIuE_Yxl2wRZI8yGxHm6&confirm=t&uuid=ec73fa9f-649f-4e12-9748-d804fe194fb0\n",
      "To: /local/hw4_data.tar.gz\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12.7G/12.7G [03:25<00:00, 61.9MB/s]\n",
      "hw4_data.tar.gz\n",
      "28440\t/local/hw4_data_kaggle/hw4p1_data/val\n",
      "28428\t/local/hw4_data_kaggle/hw4p1_data/test\n",
      "1082348\t/local/hw4_data_kaggle/hw4p1_data/train\n",
      "1139216\t/local/hw4_data_kaggle/hw4p1_data\n",
      "14328964\t/local/hw4_data_kaggle/hw4p2_data/train-clean-100\n",
      "765972\t/local/hw4_data_kaggle/hw4p2_data/test-clean\n",
      "775116\t/local/hw4_data_kaggle/hw4p2_data/dev-clean\n",
      "15870056\t/local/hw4_data_kaggle/hw4p2_data\n",
      "17009272\t/local/hw4_data_kaggle\n",
      "17009272\t/local/\n"
     ]
    }
   ],
   "source": [
    "!gdown 1-0e9Gnl4nm6wbIuE_Yxl2wRZI8yGxHm6 --output $LOCAL/hw4_data.tar.gz\n",
    "!ls $LOCAL/\n",
    "!tar -xf $LOCAL/hw4_data.tar.gz -C $LOCAL/\n",
    "!rm -rf $LOCAL/hw4_data.tar.gz\n",
    "!du --max-depth=3 $LOCAL/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "from hw4lib.data import (\n",
    "    H4Tokenizer,\n",
    "    LMDataset,\n",
    "    verify_dataloader\n",
    ")\n",
    "from hw4lib.model import (\n",
    "    CausalMask,\n",
    "    PadMask,\n",
    "    PositionalEncoding,\n",
    "    DecoderOnlyTransformer\n",
    ")\n",
    "from hw4lib.trainers import (\n",
    "    LMTrainer,\n",
    ")\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import yaml\n",
    "import gc\n",
    "import torch\n",
    "from torchinfo import summary\n",
    "import os\n",
    "import json\n",
    "import tarfile\n",
    "import shutil\n",
    "import wandb\n",
    "import yaml\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementations\n",
    "- `TA TODO`: \n",
    "  - `MANDATORY`: Run these cells to verify that the testing works in your chosen environment. Lmk if it doesn't.\n",
    "  - `OPTIONAL`: Do read through the implementations. Any feedback regarding them would be very appreciated!\n",
    "- `NOTE`: All of these implementations have detailed specification, implementation details, and hints in their respective source files. Make sure to read all of them in their entirety to understand the implementation details!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MyTorch Implementations\n",
    "- `TODO`: Copy-paste your implementation of `Linear` from previous assignments into `mytorch/nn/linear.py`.\n",
    "- `TODO`: Implement the generic `Softmax` activation function in `mytorch/nn/activation.py`.\n",
    "- `TODO`: Implement the `ScaledDotProductAttention` class in `mytorch/nn/scaled_dot_product_attention.py`.\n",
    "- `TODO`: Implement the `MultiHeadAttention` class in `mytorch/nn/multi_head_attention.py`.\n",
    "- `TODO`: Run the cell below to check your implementation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[95m================================================================================\n",
      "Running tests for category: Softmax\n",
      "--------------------------------------------------------------------------------\u001b[0m\n",
      "\n",
      "\u001b[94m[01/01]    Running:  Softmax Tests\u001b[0m\n",
      "Testing Softmax ...\n",
      "Test Passed: Softmax Forward\n",
      "Test Passed: Softmax Backward\n",
      "\u001b[92m[01/01]    PASSED:   Softmax Tests\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[95m================================================================================\n",
      "Running tests for category: ScaledDotProductAttention\n",
      "--------------------------------------------------------------------------------\u001b[0m\n",
      "\n",
      "\u001b[94m[01/01]    Running:  ScaledDotProductAttention Tests\u001b[0m\n",
      "Testing Scaled Dot Product Attention ...\n",
      "Test Passed: Scaled Dot Product Attention Forward\n",
      "Test Passed: Scaled Dot Product Attention Backward\n",
      "\u001b[92m[01/01]    PASSED:   ScaledDotProductAttention Tests\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[95m================================================================================\n",
      "Running tests for category: MultiHeadAttention\n",
      "--------------------------------------------------------------------------------\u001b[0m\n",
      "\n",
      "\u001b[94m[01/01]    Running:  MultiHeadAttention Tests\u001b[0m\n",
      "Testing Multi Head Attention ...\n",
      "Test Passed: Multi Head Attention Forward\n",
      "Test Passed: Multi Head Attention Backward\n",
      "\u001b[92m[01/01]    PASSED:   MultiHeadAttention Tests\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[95m================================================================================\n",
      "                                  Test Summary                                  \n",
      "================================================================================\u001b[0m\n",
      "\u001b[93mCategory:    Softmax                       \n",
      "Results:     1/1 tests passed (100.0%)\u001b[0m\n",
      "\u001b[93mCategory:    ScaledDotProductAttention     \n",
      "Results:     1/1 tests passed (100.0%)\u001b[0m\n",
      "\u001b[93mCategory:    MultiHeadAttention            \n",
      "Results:     1/1 tests passed (100.0%)\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python -m tests.test_mytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Implementation\n",
    "- `TODO`: First, Familiarize yourself with the `tokenize`, `encode`, and `decode` methods of the `H4Tokenizer` class in `hw4lib/data/tokenizer.py`. You will need to make use of these methods in both `HW4P1` and `HW4P2` both in the dataset implementations and during decoding. \n",
    "- `TODO`: Implement the `LMDataset` class in `hw4lib/data/lm_dataset.py`. \n",
    "- You will have to implement parts of `__init__` and completely implement the `__len__`, `__getitem__` and `collate_fn` methods. \n",
    "- `TODO`: Then run the cell below to check your implementation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading transcripts for train partition...\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 267/267 [00:00<00:00, 2503.35it/s]\n",
      "\n",
      "\u001b[95m================================================================================\n",
      "Running tests for category: LMDataset\n",
      "--------------------------------------------------------------------------------\u001b[0m\n",
      "\n",
      "\u001b[94m[01/01]    Running:  Test the LMDataset class\u001b[0m\n",
      "Testing __init__ method ...\n",
      "Test Passed: Text files are sorted.\n",
      "Test Passed: Shifted and golden transcripts are aligned.\n",
      "Test Passed: All transcripts are decoded correctly after removing SOS and EOS tokens.\n",
      "Testing __getitem__ method ...\n",
      "Test Passed: SOS and EOS tokens are correctly placed for samples.\n",
      "Test Passed: All transcripts are decoded correctly after removing SOS and EOS tokens.\n",
      "Testing collate_fn method ...\n",
      "Test Passed: Transcript batch has correct dimensions (2D tensor).\n",
      "Test Passed: Transcript lengths are consistent with the batch size.\n",
      "Test Passed: All sequences are padded to the same length.\n",
      "Test Passed: Padding values are correct.\n",
      "Test Passed: Batch transcripts are of correct data type.\n",
      "\u001b[92m[01/01]    PASSED:   Test the LMDataset class\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[95m================================================================================\n",
      "                                  Test Summary                                  \n",
      "================================================================================\u001b[0m\n",
      "\u001b[93mCategory:    LMDataset                     \n",
      "Results:     1/1 tests passed (100.0%)\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python -m tests.test_dataset_lm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Implementations\n",
    "#### Overview:\n",
    "- `TODO`: Implement the `CausalMask` and `PadMask` functions in `hw4lib/modules/masks.py`.\n",
    "- `TODO`: Implement the `PositionalEncoding` class in `hw4lib/model/positional_encoding.py`.\n",
    "- `TODO`: Implement the `SelfAttentionLayer` and `FeedForwardLayer` classes in `hw4lib/model/sublayers.py`.\n",
    "- `TODO`: Implement the `SelfAttentionDecoderLayer` class in `hw4lib/model/decoder_layers.py`.\n",
    "- `TODO`: Implement the `DecoderOnlyTransformer` class in `hw4lib/model/transformers.py`.\n",
    "- `TODO`: Then run the cell below to check your implementation.\n",
    "- `NOTE`: Besides the `DecoderOnlyTransformer` (P1 mandatory, P2 optional), you will use all of the above implementations in both `HW4P1` and `HW4P2`!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Masks \n",
    "- `TODO`: Implement the `PadMask` and `CausalMask` functions in `hw4lib/modules/masks.py`.\n",
    "- `TODO`: Then run the cell below to check your implementation.\n",
    "- `NOTE`: You will need to make use of these masks in both `HW4P1` and `HW4P2`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Causal Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m tests.test_mask_causal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Padding Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m tests.test_mask_padding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optional: Visualize your Masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dummy data    \n",
    "_d_model   = 64\n",
    "_x         = torch.zeros(4, 20, _d_model)\n",
    "_x_len     = torch.tensor([5, 15, 10, 20])\n",
    "_x_causal  = CausalMask(_x)\n",
    "_x_padding = PadMask(_x, _x_len)\n",
    "\n",
    "# Create figure with two subplots side by side\n",
    "fig, mask_axs = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Plot masks\n",
    "masks_and_titles = [\n",
    "    (_x_padding, \"Padding Mask\"),\n",
    "    (_x_causal, \"Causal Mask\")\n",
    "]\n",
    "\n",
    "# Plot each mask\n",
    "images = []\n",
    "for i, (mask, title) in enumerate(masks_and_titles):\n",
    "    im = mask_axs[i].imshow(mask, cmap=\"gray\", aspect='auto')\n",
    "    mask_axs[i].set_title(title, fontsize=8)\n",
    "    images.append(im)\n",
    "\n",
    "# Add colorbar at the bottom\n",
    "fig.subplots_adjust(bottom=0.2)  # Make space for colorbar\n",
    "cbar_ax = fig.add_axes([0.15, 0.1, 0.7, 0.02])  # [left, bottom, width, height]\n",
    "cbar = plt.colorbar(images[0], cax=cbar_ax, orientation='horizontal')\n",
    "cbar.ax.set_xlabel('Mask Values', labelpad=5, fontsize=8)\n",
    "cbar.set_ticks([0, 1])\n",
    "cbar.set_ticklabels(['Attend (0)', 'Ignore/Mask (1)'])\n",
    "cbar.ax.tick_params(labelsize=6)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positional Encoding\n",
    "- `TODO`: Implement the `PositionalEncoding` class in `hw4lib/model/positional_encoding.py`.\n",
    "- `TODO`: Then run the cell below to check your implementation.\n",
    "- `NOTE`: You will need to make use of this positional encoding in both `HW4P1` and `HW4P2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m tests.test_positional_encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optional: Visualize your Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample positional encoding\n",
    "d_model = 64\n",
    "max_len = 100\n",
    "pos_encoding = PositionalEncoding(d_model=d_model, max_len=max_len)\n",
    "pe = pos_encoding.pe.squeeze(0).numpy()  # Remove batch dimension and convert to numpy\n",
    "\n",
    "# Create figure with two subplots side by side\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Plot 1: Positional encoding matrix\n",
    "im = ax1.imshow(pe, aspect='auto', cmap='RdBu', \n",
    "                extent=[0, d_model, max_len, 0])  # Flip y-axis to show position top-to-bottom\n",
    "plt.colorbar(im, ax=ax1, label='Encoding Value')\n",
    "ax1.set_xlabel('Dimension')\n",
    "ax1.set_ylabel('Position')\n",
    "ax1.set_title('Positional Encoding Matrix')\n",
    "ax1.grid(False)\n",
    "\n",
    "# Plot 2: Sinusoidal patterns\n",
    "dimensions = [0, 15, 31, 47, 63]  # Plot first few dimensions\n",
    "for dim in dimensions:\n",
    "    ax2.plot(pe[:, dim], label=f'dim {dim}')\n",
    "ax2.set_xlabel('Position')\n",
    "ax2.set_ylabel('Encoding Value')\n",
    "ax2.set_title('Sinusoidal Patterns for Different Dimensions')\n",
    "ax2.legend()\n",
    "ax2.grid(True)\n",
    "\n",
    "# Adjust layout to prevent overlap\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer Sublayers\n",
    "- `TODO`: Implement the `SelfAttentionLayer`, and `FeedForwardLayer` classes in `hw4lib/model/sublayers.py`.\n",
    "- `TODO`: Then run the cell below to check your implementation.\n",
    "- `NOTE`: You will need to make use of all of these sublayers in both `HW4P1` and `HW4P2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m tests.test_sublayer_selfattention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m tests.test_sublayer_feedforward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer Self-Attention Decoder Layer\n",
    "- `TODO`: Implement the `SelfAttentionDecoderLayer` class in `hw4lib/model/decoder_layers.py`.\n",
    "- `TODO`: Then run the cell below to check your implementation.\n",
    "- `NOTE`: You will need to make use of this sublayer in `HW4P2`.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m tests.test_decoderlayer_selfattention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder-Only Transformer\n",
    "\n",
    "- `TODO`: Implement the  `DecoderOnlyTransformer` class in `hw4lib/model/transformers.py`.\n",
    "- `TODO`: Then run the cell below to check your implementation.\n",
    "- `NOTE`: You will need to make use of in `HW4P1` and optionally `HW4P2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m tests.test_transformer_decoder_only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoding Implementations \n",
    "- `TODO`: Implement the `generate_greedy` method of the `SequenceGenerator` class in `hw4lib/decoding/sequence_generator.py`.\n",
    "- `TODO`: Then run the cell below to check your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[95m================================================================================\n",
      "Running tests for category: Decoding\n",
      "--------------------------------------------------------------------------------\u001b[0m\n",
      "\n",
      "\u001b[94m[01/01]    Running:  Test greedy decoding\u001b[0m\n",
      "Testing Single Batch Greedy Search ...\n",
      "Generated: HELLO WORLD  | Expected: HELLO WORLD \n",
      "Testing Multi Batch Greedy Search ...\n",
      "Batch 0  : Generated: HELLO WORLD  | Expected: HELLO WORLD \n",
      "Batch 1  : Generated: GOOD BYE     | Expected: GOOD BYE    \n",
      "\u001b[92m[01/01]    PASSED:   Test greedy decoding\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[95m================================================================================\n",
      "                                  Test Summary                                  \n",
      "================================================================================\u001b[0m\n",
      "\u001b[93mCategory:    Decoding                      \n",
      "Results:     1/1 tests passed (100.0%)\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python -m tests.test_decoding --mode greedy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments\n",
    "- `TA TODO`: Simply running sequentially shoud work. But keep an eye out for other `TA TODO`s. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config\n",
    "- `TA TODO`: You can use the `config.yaml` file to set your config for your ablation study.\n",
    "- `NOTE`: For the values not provided, feel free to set as you see fit or use defaults. \n",
    "- `NOTE`: There is one experimental setup for the optimizer configuration, i.e Pattern-matching to group parameters by their names and apply different learning rates to them. \n",
    "- Eg. `self_attn` will match all parameters containing `self_attn` in their names. \n",
    "- See `hw4lib/utils/create_optimizer.py` for more details. Again, experiment if you want with it but I am still testing it out. \n",
    "- Motivation is to use it to set lower learning rates for `self-attn` and `ffn` modules while initializing an Encoder-Decoder Transformer with weights from a pre-trained Decoder-Only Transformer.\n",
    "- This is for Internal Testing only, wont be available for student use for simplicity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting config.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile config.yaml\n",
    "\n",
    "Name                      : \"Puru\"\n",
    "\n",
    "###### Tokenization ------------------------------------------------------------\n",
    "tokenization:\n",
    "  token_type                : \"1k\"       # [char, 1k, 5k, 10k]\n",
    "  token_map :\n",
    "      'char': 'hw4lib/data/tokenizer_jsons/tokenizer_char.json'\n",
    "      '1k'  : 'hw4lib/data/tokenizer_jsons/tokenizer_1000.json'\n",
    "      '5k'  : 'hw4lib/data/tokenizer_jsons/tokenizer_5000.json'\n",
    "      '10k' : 'hw4lib/data/tokenizer_jsons/tokenizer_10000.json'\n",
    "\n",
    "###### Dataset -----------------------------------------------------------------\n",
    "data:                    # Currently setup for PSC $LOCAL\n",
    "  root                 : \"/local/hw4_data_kaggle/hw4p1_data\"  # TODO: Set the root path of your data\n",
    "  train_partition      : \"train\"  # unpaired text for LM pre-training\n",
    "  val_partition        : \"val\"    # unpaired text for LM pre-training\n",
    "  test_partition       : \"test\"   # unpaired text for LM pre-trainin\n",
    "  subset               : 1.0          # Load a subset of the data (for debugging, testing, etc\n",
    "  batch_size           : 256          #   \n",
    "  NUM_WORKERS          : 2            # Set to 0 for CPU\n",
    "\n",
    "###### Network Specs -------------------------------------------------------------\n",
    "model: # Decoder-Only Language Model (HW4P1)\n",
    "  d_model                   : 256\n",
    "  d_ff                      : 2048\n",
    "  num_layers                : 6\n",
    "  num_heads                 : 4\n",
    "  dropout                   : 0.0\n",
    "  layer_drop_rate           : 0.0\n",
    "  weight_tying              : False\n",
    "\n",
    "###### Common Training Parameters ------------------------------------------------\n",
    "training:\n",
    "  use_wandb                   : True\n",
    "  wandb_run_id                : \"none\" # \"none\" or \"run_id\"\n",
    "  resume                      : False\n",
    "  epochs                      : 5\n",
    "  gradient_accumulation_steps : 1\n",
    "  wandb_project               : \"S25-HW4P1-TA\"\n",
    "\n",
    "###### Loss ----------------------------------------------------------------------\n",
    "loss: # Just good ol' CrossEntropy\n",
    "  label_smoothing: 0.0\n",
    "\n",
    "###### Optimizer -----------------------------------------------------------------\n",
    "optimizer:\n",
    "  name: \"adamw\" # Options: sgd, adam, adamw\n",
    "  lr: 0.0002  # Base learning rate\n",
    "\n",
    "  # Common parameters\n",
    "  weight_decay: 0.0002\n",
    "\n",
    "  # Parameter groups\n",
    "  param_groups:\n",
    "    - name: self_attn\n",
    "      patterns: [\"self_attn\"]  # Will match all parameters containing \"encoder\"\n",
    "      lr: 0.0002  # LR for self_attn\n",
    "      layer_decay:\n",
    "        enabled: False\n",
    "        decay_rate: 0.8\n",
    "    \n",
    "    - name: ffn\n",
    "      patterns: [\"ffn\"]\n",
    "      lr: 0.0002  # LR for ffn\n",
    "      layer_decay:\n",
    "        enabled: False\n",
    "        decay_rate: 0.8\n",
    "  \n",
    "  # Layer-wise learning rates\n",
    "  layer_decay:\n",
    "    enabled: False\n",
    "    decay_rate: 0.75\n",
    "\n",
    "  # SGD specific parameters\n",
    "  sgd:\n",
    "    momentum: 0.9\n",
    "    nesterov: True\n",
    "    dampening: 0\n",
    "\n",
    "  # Adam specific parameters\n",
    "  adam:\n",
    "    betas: [0.9, 0.999]\n",
    "    eps: 1.0e-8\n",
    "    amsgrad: False\n",
    "\n",
    "  # AdamW specific parameters\n",
    "  adamw:\n",
    "    betas: [0.9, 0.999]\n",
    "    eps: 1.0e-8\n",
    "    amsgrad: False\n",
    "\n",
    "###### Scheduler -----------------------------------------------------------------\n",
    "scheduler:\n",
    "  name: \"cosine\"  # Options: reduce_lr, cosine, cosine_warm\n",
    "\n",
    "  # ReduceLROnPlateau specific parameters\n",
    "  reduce_lr:\n",
    "    mode: \"min\"  # Options: min, max\n",
    "    factor: 0.1  # Factor to reduce learning rate by\n",
    "    patience: 10  # Number of epochs with no improvement after which LR will be reduced\n",
    "    threshold: 0.0001  # Threshold for measuring the new optimum\n",
    "    threshold_mode: \"rel\"  # Options: rel, abs\n",
    "    cooldown: 0  # Number of epochs to wait before resuming normal operation\n",
    "    min_lr: 0.0000001  # Minimum learning rate\n",
    "    eps: 1e-8  # Minimal decay applied to lr\n",
    "\n",
    "  # CosineAnnealingLR specific parameters\n",
    "  cosine:\n",
    "    T_max: 15  # Maximum number of iterations\n",
    "    eta_min: 0.0000001  # Minimum learning rate\n",
    "    last_epoch: -1\n",
    "\n",
    "  # CosineAnnealingWarmRestarts specific parameters\n",
    "  cosine_warm:\n",
    "    T_0: 4  # Number of iterations for the first restart\n",
    "    T_mult: 4  # Factor increasing T_i after each restart\n",
    "    eta_min: 0.0000001  # Minimum learning rate\n",
    "    last_epoch: -1\n",
    "\n",
    "  # Warmup parameters (can be used with any scheduler)\n",
    "  warmup:\n",
    "    enabled: True\n",
    "    type: \"exponential\"  # Options: linear, exponential\n",
    "    epochs: 5\n",
    "    start_factor: 0.01\n",
    "    end_factor: 1.0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('config.yaml', 'r') as file:\n",
    "    config = yaml.safe_load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "                          Tokenizer Configuration (1k)                          \n",
      "--------------------------------------------------------------------------------\n",
      "Vocabulary size:     1000\n",
      "\n",
      "Special Tokens:\n",
      "PAD:              0\n",
      "UNK:              1\n",
      "MASK:             2\n",
      "SOS:              3\n",
      "EOS:              4\n",
      "BLANK:            5\n",
      "\n",
      "Validation Example:\n",
      "--------------------------------------------------------------------------------\n",
      "Input text:  [SOS]HI DEEP LEARNERS[EOS]\n",
      "Tokens:      ['[SOS]', 'H', 'I', 'Ä DE', 'EP', 'Ä LE', 'AR', 'N', 'ERS', '[EOS]']\n",
      "Token IDs:   [3, 14, 15, 159, 290, 228, 71, 20, 214, 4]\n",
      "Decoded:     [SOS]HI DEEP LEARNERS[EOS]\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "Tokenizer = H4Tokenizer(\n",
    "    token_map  = config['tokenization']['token_map'], \n",
    "    token_type = config['tokenization']['token_type']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading transcripts for train partition...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 267178/267178 [00:48<00:00, 5461.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading transcripts for val partition...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7031/7031 [00:01<00:00, 4982.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading transcripts for test partition...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7032/7032 [00:01<00:00, 5536.01it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1694"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset  = LMDataset(\n",
    "    partition  = config['data']['train_partition'],\n",
    "    config     = config['data'],\n",
    "    tokenizer  = Tokenizer\n",
    ")\n",
    "\n",
    "val_dataset    = LMDataset(\n",
    "    partition  = config['data']['val_partition'],\n",
    "    config     = config['data'],\n",
    "    tokenizer  = Tokenizer\n",
    ")\n",
    "\n",
    "test_dataset   = LMDataset(\n",
    "    partition  = config['data']['test_partition'],\n",
    "    config     = config['data'],\n",
    "    tokenizer  = Tokenizer\n",
    ")   \n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader    = DataLoader(\n",
    "    dataset     = train_dataset,\n",
    "    batch_size  = config['data']['batch_size'],\n",
    "    shuffle     = True,\n",
    "    num_workers = config['data']['NUM_WORKERS'] if device == 'cuda' else 0,\n",
    "    pin_memory  = True,\n",
    "    collate_fn  = train_dataset.collate_fn   \n",
    ")\n",
    "\n",
    "val_loader      = DataLoader(\n",
    "    dataset     = val_dataset,\n",
    "    batch_size  = config['data']['batch_size'],\n",
    "    shuffle     = False,\n",
    "    num_workers = config['data']['NUM_WORKERS'] if device == 'cuda' else 0,\n",
    "    pin_memory  = True,\n",
    "    collate_fn  = val_dataset.collate_fn   \n",
    ")\n",
    "\n",
    "test_loader     = DataLoader(\n",
    "    dataset     = test_dataset,\n",
    "    batch_size  = config['data']['batch_size'],\n",
    "    shuffle     = False,\n",
    "    num_workers = config['data']['NUM_WORKERS'] if device == 'cuda' else 0,\n",
    "    pin_memory  = True,\n",
    "    collate_fn  = test_dataset.collate_fn   \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataloader Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "             Dataloader Verification              \n",
      "==================================================\n",
      "Dataloader Partition     : train\n",
      "--------------------------------------------------\n",
      "Number of Batches        : 1044\n",
      "Batch Size               : 256\n",
      "--------------------------------------------------\n",
      "Checking shapes of the data...                    \n",
      "\n",
      "Shifted Transcript Shape : [256, 102]\n",
      "Golden Transcript Shape  : [256, 102]\n",
      "Transcript Lengths Shape : [256]\n",
      "--------------------------------------------------\n",
      "Max Transcript Length    : 192\n",
      "Avg. Chars per Token     : 3.13\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "verify_dataloader(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "             Dataloader Verification              \n",
      "==================================================\n",
      "Dataloader Partition     : val\n",
      "--------------------------------------------------\n",
      "Number of Batches        : 28\n",
      "Batch Size               : 256\n",
      "--------------------------------------------------\n",
      "Checking shapes of the data...                    \n",
      "\n",
      "Shifted Transcript Shape : [256, 108]\n",
      "Golden Transcript Shape  : [256, 108]\n",
      "Transcript Lengths Shape : [256]\n",
      "--------------------------------------------------\n",
      "Max Transcript Length    : 137\n",
      "Avg. Chars per Token     : 3.13\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "verify_dataloader(val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "             Dataloader Verification              \n",
      "==================================================\n",
      "Dataloader Partition     : test\n",
      "--------------------------------------------------\n",
      "Number of Batches        : 28\n",
      "Batch Size               : 256\n",
      "--------------------------------------------------\n",
      "Checking shapes of the data...                    \n",
      "\n",
      "Shifted Transcript Shape : [256, 101]\n",
      "Golden Transcript Shape  : [256, 101]\n",
      "Transcript Lengths Shape : [256]\n",
      "--------------------------------------------------\n",
      "Max Transcript Length    : 131\n",
      "Avg. Chars per Token     : 3.13\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "verify_dataloader(test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Max Transcript Length \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating the maximum transcript length across your dataset is a crucial step when working with certain transformer models. \n",
    "-  We'll use sinusoidal positional encodings that must be precomputed up to a fixed maximum length.\n",
    "- This maximum length is a hyperparameter that determines:\n",
    "  - How long of a sequence your model can process\n",
    "  - The size of your positional encoding matrix\n",
    "  - Memory requirements during training and inference\n",
    "- `Requirements`: For this assignment, ensure your positional encodings can accommodate at least the longest sequence in your dataset to prevent truncation. However, you can set this value higher if you anticipate using your language model to work with longer sequences in future tasks (hint: this might be useful for P2! ðŸ˜‰)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Global Max Transcript Length   : 192\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "max_transcript_length = max(train_dataset.text_max_len, val_dataset.text_max_len, test_dataset.text_max_len)\n",
    "print(\"=\"*50)\n",
    "print(f\"{'Global Max Transcript Length':<30} : {max_transcript_length}\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of shifted_transcripts :  torch.Size([256, 94])\n",
      "Shape of golden_transcripts  :  torch.Size([256, 94])\n",
      "Shape of transcript_lengths  :  torch.Size([256])\n",
      "===============================================================================================\n",
      "Layer (type:depth-idx)                        Output Shape              Param #\n",
      "===============================================================================================\n",
      "DecoderOnlyTransformer                        [256, 94, 1000]           --\n",
      "â”œâ”€Embedding: 1-1                              [256, 94, 256]            256,000\n",
      "â”œâ”€PositionalEncoding: 1-2                     [256, 94, 256]            --\n",
      "â”œâ”€Dropout: 1-3                                [256, 94, 256]            --\n",
      "â”œâ”€ModuleList: 1-4                             --                        --\n",
      "â”‚    â””â”€SelfAttentionDecoderLayer: 2-1         [256, 94, 256]            --\n",
      "â”‚    â”‚    â””â”€SelfAttentionLayer: 3-1           [256, 94, 256]            263,680\n",
      "â”‚    â”‚    â””â”€FeedForwardLayer: 3-2             [256, 94, 256]            1,051,392\n",
      "â”‚    â””â”€SelfAttentionDecoderLayer: 2-2         [256, 94, 256]            --\n",
      "â”‚    â”‚    â””â”€SelfAttentionLayer: 3-3           [256, 94, 256]            263,680\n",
      "â”‚    â”‚    â””â”€FeedForwardLayer: 3-4             [256, 94, 256]            1,051,392\n",
      "â”‚    â””â”€SelfAttentionDecoderLayer: 2-3         [256, 94, 256]            --\n",
      "â”‚    â”‚    â””â”€SelfAttentionLayer: 3-5           [256, 94, 256]            263,680\n",
      "â”‚    â”‚    â””â”€FeedForwardLayer: 3-6             [256, 94, 256]            1,051,392\n",
      "â”‚    â””â”€SelfAttentionDecoderLayer: 2-4         [256, 94, 256]            --\n",
      "â”‚    â”‚    â””â”€SelfAttentionLayer: 3-7           [256, 94, 256]            263,680\n",
      "â”‚    â”‚    â””â”€FeedForwardLayer: 3-8             [256, 94, 256]            1,051,392\n",
      "â”‚    â””â”€SelfAttentionDecoderLayer: 2-5         [256, 94, 256]            --\n",
      "â”‚    â”‚    â””â”€SelfAttentionLayer: 3-9           [256, 94, 256]            263,680\n",
      "â”‚    â”‚    â””â”€FeedForwardLayer: 3-10            [256, 94, 256]            1,051,392\n",
      "â”‚    â””â”€SelfAttentionDecoderLayer: 2-6         [256, 94, 256]            --\n",
      "â”‚    â”‚    â””â”€SelfAttentionLayer: 3-11          [256, 94, 256]            263,680\n",
      "â”‚    â”‚    â””â”€FeedForwardLayer: 3-12            [256, 94, 256]            1,051,392\n",
      "â”œâ”€LayerNorm: 1-5                              [256, 94, 256]            512\n",
      "â”œâ”€Linear: 1-6                                 [256, 94, 1000]           257,000\n",
      "===============================================================================================\n",
      "Total params: 8,403,944\n",
      "Trainable params: 8,403,944\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (Units.GIGABYTES): 1.75\n",
      "===============================================================================================\n",
      "Input size (MB): 0.19\n",
      "Forward/backward pass size (MB): 3543.76\n",
      "Params size (MB): 27.30\n",
      "Estimated Total Size (MB): 3571.26\n",
      "===============================================================================================\n"
     ]
    }
   ],
   "source": [
    "model_config = config['model']\n",
    "model_config.update({\n",
    "    'max_len': max_transcript_length,\n",
    "    'num_classes': Tokenizer.vocab_size\n",
    "})\n",
    "model = DecoderOnlyTransformer(**model_config)  \n",
    "\n",
    "# Get some inputs from the text loader\n",
    "for batch in train_loader:\n",
    "    shifted_transcripts, golden_transcripts, transcript_lengths = batch\n",
    "    print(\"Shape of shifted_transcripts : \", shifted_transcripts.shape)\n",
    "    print(\"Shape of golden_transcripts  : \", golden_transcripts.shape)\n",
    "    print(\"Shape of transcript_lengths  : \", transcript_lengths.shape)\n",
    "    break \n",
    "\n",
    "model_stats = summary(model, input_data=[shifted_transcripts, transcript_lengths])\n",
    "print(model_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpsamal\u001b[0m (\u001b[33mpsamal-carnegie-mellon-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /jet/home/psamal/.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login(key=\"31888e0ba72a18d4a57ea02c19a9687bc4481f37\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainer\n",
    "You will have to do some minor in-filling for the `LMTrainer` class in `hw4lib/trainers/lm_trainer.py` before you can use it.\n",
    "- `TODO`: Fill in the `TODO`s in the `__init__`.\n",
    "- `TODO`: Fill in the `TODO`s in the `_train_epoch`.\n",
    "- `TODO`: Fill in the `TODO`s in the `_validate_epoch`.\n",
    "- `TODO`: Fill in the `TODO`s in the `train` method.\n",
    "- `TODO`: Fill in the `TODO`s in the `evaluate` method.\n",
    "- `TODO`: Fill in the `TODO`s in the `generate` method.\n",
    "\n",
    "\n",
    "Every time you run the trainer, it will create a new directory in the `expts` folder with the following structure:\n",
    "```\n",
    "expts/\n",
    "    â””â”€â”€ {run_name}/\n",
    "        â”œâ”€â”€ config.yaml\n",
    "        â”œâ”€â”€ model_arch.txt\n",
    "        â”œâ”€â”€ checkpoints/\n",
    "        â”‚   â”œâ”€â”€ checkpoint-best-metric-model.pth\n",
    "        â”‚   â””â”€â”€ checkpoint-last-epoch-model.pth\n",
    "        â”œâ”€â”€ attn/\n",
    "        â”‚   â””â”€â”€ {attention visualizations}\n",
    "        â””â”€â”€ text/\n",
    "            â””â”€â”€ {generated text outputs}\n",
    "```\n",
    "\n",
    "- `TA TODO`: Please change the run name to the name that was assigned to you in the ablation sheet for easy referencing. Also, `MAKE SURE TO KEEP THESE DIRECTORIES`, We will need it for `HW4P2` runs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "ðŸ”§ Configuring Optimizer:\n",
      "â”œâ”€â”€ Type: ADAMW\n",
      "â”œâ”€â”€ Base LR: 0.0002\n",
      "â”œâ”€â”€ Weight Decay: 0.0002\n",
      "â”œâ”€â”€ Parameter Groups:\n",
      "â”‚   â”œâ”€â”€ Group: self_attn\n",
      "â”‚   â”‚   â”œâ”€â”€ LR: 0.0002\n",
      "â”‚   â”‚   â””â”€â”€ Patterns: ['self_attn']\n",
      "â”‚   â”œâ”€â”€ Group: ffn\n",
      "â”‚   â”‚   â”œâ”€â”€ LR: 0.0002\n",
      "â”‚   â”‚   â””â”€â”€ Patterns: ['ffn']\n",
      "â”‚   â””â”€â”€ Default Group (unmatched parameters)\n",
      "â””â”€â”€ AdamW Specific:\n",
      "    â”œâ”€â”€ Betas: [0.9, 0.999]\n",
      "    â”œâ”€â”€ Epsilon: 1e-08\n",
      "    â””â”€â”€ AMSGrad: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/ocean/projects/cis220031p/psamal/IDL-HW4/wandb/run-20250302_055904-n63le4k3</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/psamal-carnegie-mellon-university/S25-HW4P1-TA/runs/n63le4k3' target=\"_blank\">test-lm</a></strong> to <a href='https://wandb.ai/psamal-carnegie-mellon-university/S25-HW4P1-TA' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/psamal-carnegie-mellon-university/S25-HW4P1-TA' target=\"_blank\">https://wandb.ai/psamal-carnegie-mellon-university/S25-HW4P1-TA</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/psamal-carnegie-mellon-university/S25-HW4P1-TA/runs/n63le4k3' target=\"_blank\">https://wandb.ai/psamal-carnegie-mellon-university/S25-HW4P1-TA/runs/n63le4k3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer = LMTrainer(\n",
    "    model=model,\n",
    "    tokenizer=Tokenizer,\n",
    "    config=config,\n",
    "    run_name=\"test-lm\",\n",
    "    config_file=\"config.yaml\",\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train\n",
    "- `TA TODO`: You can set your epochs here or in the config. If you set in config, make sure you remove the epoch argument here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“ˆ Configuring Learning Rate Scheduler:\n",
      "â”œâ”€â”€ Type: COSINE\n",
      "â”œâ”€â”€ Cosine Annealing Settings:\n",
      "â”‚   â”œâ”€â”€ T_max: 15 epochs (15660 steps)\n",
      "â”‚   â””â”€â”€ Min LR: 1e-07\n",
      "â”œâ”€â”€ Warmup Settings:\n",
      "â”‚   â”œâ”€â”€ Duration: 5 epochs (5220 steps)\n",
      "â”‚   â”œâ”€â”€ Start Factor: 0.01\n",
      "â”‚   â””â”€â”€ End Factor: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating with greedy search...\n",
      "\n",
      "ðŸ“Š Metrics (Epoch 0):\n",
      "â”œâ”€â”€ TRAIN:\n",
      "â”‚   â”œâ”€â”€ ce_loss_char: 1.8528\n",
      "â”‚   â”œâ”€â”€ ce_loss_token: 5.8006\n",
      "â”‚   â”œâ”€â”€ perplexity_char: 6.3774\n",
      "â”‚   â””â”€â”€ perplexity_token: 330.4923\n",
      "â””â”€â”€ VAL:\n",
      "    â”œâ”€â”€ ce_loss_char: 1.5480\n",
      "    â”œâ”€â”€ ce_loss_token: 4.8500\n",
      "    â”œâ”€â”€ perplexity_char: 4.7020\n",
      "    â””â”€â”€ perplexity_token: 127.7350\n",
      "â””â”€â”€ TRAINING:\n",
      "    â””â”€â”€ learning_rate: 0.000042\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save(\"/mnt/folder/file.h5\", base_path=\"/mnt\")\n",
      "                                                                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating with greedy search...\n",
      "\n",
      "ðŸ“Š Metrics (Epoch 1):\n",
      "â”œâ”€â”€ TRAIN:\n",
      "â”‚   â”œâ”€â”€ ce_loss_char: 1.4275\n",
      "â”‚   â”œâ”€â”€ ce_loss_token: 4.4691\n",
      "â”‚   â”œâ”€â”€ perplexity_char: 4.1682\n",
      "â”‚   â””â”€â”€ perplexity_token: 87.2801\n",
      "â””â”€â”€ VAL:\n",
      "    â”œâ”€â”€ ce_loss_char: 1.3169\n",
      "    â”œâ”€â”€ ce_loss_token: 4.1260\n",
      "    â”œâ”€â”€ perplexity_char: 3.7320\n",
      "    â””â”€â”€ perplexity_token: 61.9320\n",
      "â””â”€â”€ TRAINING:\n",
      "    â””â”€â”€ learning_rate: 0.000081\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating with greedy search...\n",
      "\n",
      "ðŸ“Š Metrics (Epoch 2):\n",
      "â”œâ”€â”€ TRAIN:\n",
      "â”‚   â”œâ”€â”€ ce_loss_char: 1.2479\n",
      "â”‚   â”œâ”€â”€ ce_loss_token: 3.9069\n",
      "â”‚   â”œâ”€â”€ perplexity_char: 3.4831\n",
      "â”‚   â””â”€â”€ perplexity_token: 49.7466\n",
      "â””â”€â”€ VAL:\n",
      "    â”œâ”€â”€ ce_loss_char: 1.2009\n",
      "    â”œâ”€â”€ ce_loss_token: 3.7624\n",
      "    â”œâ”€â”€ perplexity_char: 3.3230\n",
      "    â””â”€â”€ perplexity_token: 43.0522\n",
      "â””â”€â”€ TRAINING:\n",
      "    â””â”€â”€ learning_rate: 0.000121\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating with greedy search...\n",
      "\n",
      "ðŸ“Š Metrics (Epoch 3):\n",
      "â”œâ”€â”€ TRAIN:\n",
      "â”‚   â”œâ”€â”€ ce_loss_char: 1.1691\n",
      "â”‚   â”œâ”€â”€ ce_loss_token: 3.6602\n",
      "â”‚   â”œâ”€â”€ perplexity_char: 3.2191\n",
      "â”‚   â””â”€â”€ perplexity_token: 38.8674\n",
      "â””â”€â”€ VAL:\n",
      "    â”œâ”€â”€ ce_loss_char: 1.1514\n",
      "    â”œâ”€â”€ ce_loss_token: 3.6073\n",
      "    â”œâ”€â”€ perplexity_char: 3.1625\n",
      "    â””â”€â”€ perplexity_token: 36.8672\n",
      "â””â”€â”€ TRAINING:\n",
      "    â””â”€â”€ learning_rate: 0.000160\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Training LM]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1043/1044 [01:28<00:00, 11.59it/s, acc_step=1/1, ce_loss_token=3.5321, perplexity_token=34.1943]/jet/home/psamal/.local/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "                                                                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating with greedy search...\n",
      "\n",
      "ðŸ“Š Metrics (Epoch 4):\n",
      "â”œâ”€â”€ TRAIN:\n",
      "â”‚   â”œâ”€â”€ ce_loss_char: 1.1282\n",
      "â”‚   â”œâ”€â”€ ce_loss_token: 3.5321\n",
      "â”‚   â”œâ”€â”€ perplexity_char: 3.0900\n",
      "â”‚   â””â”€â”€ perplexity_token: 34.1942\n",
      "â””â”€â”€ VAL:\n",
      "    â”œâ”€â”€ ce_loss_char: 1.1224\n",
      "    â”œâ”€â”€ ce_loss_token: 3.5165\n",
      "    â”œâ”€â”€ perplexity_char: 3.0722\n",
      "    â””â”€â”€ perplexity_token: 33.6676\n",
      "â””â”€â”€ TRAINING:\n",
      "    â””â”€â”€ learning_rate: 0.000200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating with greedy search...\n",
      "\n",
      "ðŸ“Š Metrics (Epoch 5):\n",
      "â”œâ”€â”€ TRAIN:\n",
      "â”‚   â”œâ”€â”€ ce_loss_char: 1.0992\n",
      "â”‚   â”œâ”€â”€ ce_loss_token: 3.4413\n",
      "â”‚   â”œâ”€â”€ perplexity_char: 3.0017\n",
      "â”‚   â””â”€â”€ perplexity_token: 31.2272\n",
      "â””â”€â”€ VAL:\n",
      "    â”œâ”€â”€ ce_loss_char: 1.0994\n",
      "    â”œâ”€â”€ ce_loss_token: 3.4445\n",
      "    â”œâ”€â”€ perplexity_char: 3.0024\n",
      "    â””â”€â”€ perplexity_token: 31.3276\n",
      "â””â”€â”€ TRAINING:\n",
      "    â””â”€â”€ learning_rate: 0.000198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating with greedy search...\n",
      "\n",
      "ðŸ“Š Metrics (Epoch 6):\n",
      "â”œâ”€â”€ TRAIN:\n",
      "â”‚   â”œâ”€â”€ ce_loss_char: 1.0763\n",
      "â”‚   â”œâ”€â”€ ce_loss_token: 3.3696\n",
      "â”‚   â”œâ”€â”€ perplexity_char: 2.9338\n",
      "â”‚   â””â”€â”€ perplexity_token: 29.0675\n",
      "â””â”€â”€ VAL:\n",
      "    â”œâ”€â”€ ce_loss_char: 1.0834\n",
      "    â”œâ”€â”€ ce_loss_token: 3.3945\n",
      "    â”œâ”€â”€ perplexity_char: 2.9548\n",
      "    â””â”€â”€ perplexity_token: 29.7984\n",
      "â””â”€â”€ TRAINING:\n",
      "    â””â”€â”€ learning_rate: 0.000191\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating with greedy search...\n",
      "\n",
      "ðŸ“Š Metrics (Epoch 7):\n",
      "â”œâ”€â”€ TRAIN:\n",
      "â”‚   â”œâ”€â”€ ce_loss_char: 1.0584\n",
      "â”‚   â”œâ”€â”€ ce_loss_token: 3.3137\n",
      "â”‚   â”œâ”€â”€ perplexity_char: 2.8819\n",
      "â”‚   â””â”€â”€ perplexity_token: 27.4870\n",
      "â””â”€â”€ VAL:\n",
      "    â”œâ”€â”€ ce_loss_char: 1.0713\n",
      "    â”œâ”€â”€ ce_loss_token: 3.3565\n",
      "    â”œâ”€â”€ perplexity_char: 2.9192\n",
      "    â””â”€â”€ perplexity_token: 28.6894\n",
      "â””â”€â”€ TRAINING:\n",
      "    â””â”€â”€ learning_rate: 0.000181\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating with greedy search...\n",
      "\n",
      "ðŸ“Š Metrics (Epoch 8):\n",
      "â”œâ”€â”€ TRAIN:\n",
      "â”‚   â”œâ”€â”€ ce_loss_char: 1.0433\n",
      "â”‚   â”œâ”€â”€ ce_loss_token: 3.2663\n",
      "â”‚   â”œâ”€â”€ perplexity_char: 2.8386\n",
      "â”‚   â””â”€â”€ perplexity_token: 26.2153\n",
      "â””â”€â”€ VAL:\n",
      "    â”œâ”€â”€ ce_loss_char: 1.0623\n",
      "    â”œâ”€â”€ ce_loss_token: 3.3283\n",
      "    â”œâ”€â”€ perplexity_char: 2.8931\n",
      "    â””â”€â”€ perplexity_token: 27.8921\n",
      "â””â”€â”€ TRAINING:\n",
      "    â””â”€â”€ learning_rate: 0.000167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating with greedy search...\n",
      "\n",
      "ðŸ“Š Metrics (Epoch 9):\n",
      "â”œâ”€â”€ TRAIN:\n",
      "â”‚   â”œâ”€â”€ ce_loss_char: 1.0300\n",
      "â”‚   â”œâ”€â”€ ce_loss_token: 3.2246\n",
      "â”‚   â”œâ”€â”€ perplexity_char: 2.8010\n",
      "â”‚   â””â”€â”€ perplexity_token: 25.1446\n",
      "â””â”€â”€ VAL:\n",
      "    â”œâ”€â”€ ce_loss_char: 1.0550\n",
      "    â”œâ”€â”€ ce_loss_token: 3.3055\n",
      "    â”œâ”€â”€ perplexity_char: 2.8720\n",
      "    â””â”€â”€ perplexity_token: 27.2609\n",
      "â””â”€â”€ TRAINING:\n",
      "    â””â”€â”€ learning_rate: 0.000150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating with greedy search...\n",
      "\n",
      "ðŸ“Š Metrics (Epoch 10):\n",
      "â”œâ”€â”€ TRAIN:\n",
      "â”‚   â”œâ”€â”€ ce_loss_char: 1.0180\n",
      "â”‚   â”œâ”€â”€ ce_loss_token: 3.1871\n",
      "â”‚   â”œâ”€â”€ perplexity_char: 2.7676\n",
      "â”‚   â””â”€â”€ perplexity_token: 24.2186\n",
      "â””â”€â”€ VAL:\n",
      "    â”œâ”€â”€ ce_loss_char: 1.0490\n",
      "    â”œâ”€â”€ ce_loss_token: 3.2866\n",
      "    â”œâ”€â”€ perplexity_char: 2.8548\n",
      "    â””â”€â”€ perplexity_token: 26.7519\n",
      "â””â”€â”€ TRAINING:\n",
      "    â””â”€â”€ learning_rate: 0.000131\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating with greedy search...\n",
      "\n",
      "ðŸ“Š Metrics (Epoch 11):\n",
      "â”œâ”€â”€ TRAIN:\n",
      "â”‚   â”œâ”€â”€ ce_loss_char: 1.0069\n",
      "â”‚   â”œâ”€â”€ ce_loss_token: 3.1524\n",
      "â”‚   â”œâ”€â”€ perplexity_char: 2.7371\n",
      "â”‚   â””â”€â”€ perplexity_token: 23.3927\n",
      "â””â”€â”€ VAL:\n",
      "    â”œâ”€â”€ ce_loss_char: 1.0435\n",
      "    â”œâ”€â”€ ce_loss_token: 3.2694\n",
      "    â”œâ”€â”€ perplexity_char: 2.8391\n",
      "    â””â”€â”€ perplexity_token: 26.2944\n",
      "â””â”€â”€ TRAINING:\n",
      "    â””â”€â”€ learning_rate: 0.000110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating with greedy search...\n",
      "\n",
      "ðŸ“Š Metrics (Epoch 12):\n",
      "â”œâ”€â”€ TRAIN:\n",
      "â”‚   â”œâ”€â”€ ce_loss_char: 0.9966\n",
      "â”‚   â”œâ”€â”€ ce_loss_token: 3.1201\n",
      "â”‚   â”œâ”€â”€ perplexity_char: 2.7090\n",
      "â”‚   â””â”€â”€ perplexity_token: 22.6480\n",
      "â””â”€â”€ VAL:\n",
      "    â”œâ”€â”€ ce_loss_char: 1.0402\n",
      "    â”œâ”€â”€ ce_loss_token: 3.2589\n",
      "    â”œâ”€â”€ perplexity_char: 2.8297\n",
      "    â””â”€â”€ perplexity_token: 26.0213\n",
      "â””â”€â”€ TRAINING:\n",
      "    â””â”€â”€ learning_rate: 0.000090\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating with greedy search...\n",
      "\n",
      "ðŸ“Š Metrics (Epoch 13):\n",
      "â”œâ”€â”€ TRAIN:\n",
      "â”‚   â”œâ”€â”€ ce_loss_char: 0.9870\n",
      "â”‚   â”œâ”€â”€ ce_loss_token: 3.0901\n",
      "â”‚   â”œâ”€â”€ perplexity_char: 2.6832\n",
      "â”‚   â””â”€â”€ perplexity_token: 21.9793\n",
      "â””â”€â”€ VAL:\n",
      "    â”œâ”€â”€ ce_loss_char: 1.0376\n",
      "    â”œâ”€â”€ ce_loss_token: 3.2508\n",
      "    â”œâ”€â”€ perplexity_char: 2.8224\n",
      "    â””â”€â”€ perplexity_token: 25.8112\n",
      "â””â”€â”€ TRAINING:\n",
      "    â””â”€â”€ learning_rate: 0.000069\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating with greedy search...\n",
      "\n",
      "ðŸ“Š Metrics (Epoch 14):\n",
      "â”œâ”€â”€ TRAIN:\n",
      "â”‚   â”œâ”€â”€ ce_loss_char: 0.9782\n",
      "â”‚   â”œâ”€â”€ ce_loss_token: 3.0626\n",
      "â”‚   â”œâ”€â”€ perplexity_char: 2.6598\n",
      "â”‚   â””â”€â”€ perplexity_token: 21.3837\n",
      "â””â”€â”€ VAL:\n",
      "    â”œâ”€â”€ ce_loss_char: 1.0356\n",
      "    â”œâ”€â”€ ce_loss_token: 3.2445\n",
      "    â”œâ”€â”€ perplexity_char: 2.8167\n",
      "    â””â”€â”€ perplexity_token: 25.6489\n",
      "â””â”€â”€ TRAINING:\n",
      "    â””â”€â”€ learning_rate: 0.000050\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating with greedy search...\n",
      "\n",
      "ðŸ“Š Metrics (Epoch 15):\n",
      "â”œâ”€â”€ TRAIN:\n",
      "â”‚   â”œâ”€â”€ ce_loss_char: 0.9705\n",
      "â”‚   â”œâ”€â”€ ce_loss_token: 3.0385\n",
      "â”‚   â”œâ”€â”€ perplexity_char: 2.6394\n",
      "â”‚   â””â”€â”€ perplexity_token: 20.8748\n",
      "â””â”€â”€ VAL:\n",
      "    â”œâ”€â”€ ce_loss_char: 1.0344\n",
      "    â”œâ”€â”€ ce_loss_token: 3.2410\n",
      "    â”œâ”€â”€ perplexity_char: 2.8135\n",
      "    â””â”€â”€ perplexity_token: 25.5583\n",
      "â””â”€â”€ TRAINING:\n",
      "    â””â”€â”€ learning_rate: 0.000033\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating with greedy search...\n",
      "\n",
      "ðŸ“Š Metrics (Epoch 16):\n",
      "â”œâ”€â”€ TRAIN:\n",
      "â”‚   â”œâ”€â”€ ce_loss_char: 0.9641\n",
      "â”‚   â”œâ”€â”€ ce_loss_token: 3.0184\n",
      "â”‚   â”œâ”€â”€ perplexity_char: 2.6224\n",
      "â”‚   â””â”€â”€ perplexity_token: 20.4584\n",
      "â””â”€â”€ VAL:\n",
      "    â”œâ”€â”€ ce_loss_char: 1.0336\n",
      "    â”œâ”€â”€ ce_loss_token: 3.2385\n",
      "    â”œâ”€â”€ perplexity_char: 2.8113\n",
      "    â””â”€â”€ perplexity_token: 25.4953\n",
      "â””â”€â”€ TRAINING:\n",
      "    â””â”€â”€ learning_rate: 0.000019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating with greedy search...\n",
      "\n",
      "ðŸ“Š Metrics (Epoch 17):\n",
      "â”œâ”€â”€ TRAIN:\n",
      "â”‚   â”œâ”€â”€ ce_loss_char: 0.9591\n",
      "â”‚   â”œâ”€â”€ ce_loss_token: 3.0028\n",
      "â”‚   â”œâ”€â”€ perplexity_char: 2.6094\n",
      "â”‚   â””â”€â”€ perplexity_token: 20.1428\n",
      "â””â”€â”€ VAL:\n",
      "    â”œâ”€â”€ ce_loss_char: 1.0335\n",
      "    â”œâ”€â”€ ce_loss_token: 3.2382\n",
      "    â”œâ”€â”€ perplexity_char: 2.8110\n",
      "    â””â”€â”€ perplexity_token: 25.4871\n",
      "â””â”€â”€ TRAINING:\n",
      "    â””â”€â”€ learning_rate: 0.000009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating with greedy search...\n",
      "\n",
      "ðŸ“Š Metrics (Epoch 18):\n",
      "â”œâ”€â”€ TRAIN:\n",
      "â”‚   â”œâ”€â”€ ce_loss_char: 0.9557\n",
      "â”‚   â”œâ”€â”€ ce_loss_token: 2.9922\n",
      "â”‚   â”œâ”€â”€ perplexity_char: 2.6006\n",
      "â”‚   â””â”€â”€ perplexity_token: 19.9297\n",
      "â””â”€â”€ VAL:\n",
      "    â”œâ”€â”€ ce_loss_char: 1.0335\n",
      "    â”œâ”€â”€ ce_loss_token: 3.2381\n",
      "    â”œâ”€â”€ perplexity_char: 2.8109\n",
      "    â””â”€â”€ perplexity_token: 25.4843\n",
      "â””â”€â”€ TRAINING:\n",
      "    â””â”€â”€ learning_rate: 0.000002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating with greedy search...\n",
      "\n",
      "ðŸ“Š Metrics (Epoch 19):\n",
      "â”œâ”€â”€ TRAIN:\n",
      "â”‚   â”œâ”€â”€ ce_loss_char: 0.9539\n",
      "â”‚   â”œâ”€â”€ ce_loss_token: 2.9864\n",
      "â”‚   â”œâ”€â”€ perplexity_char: 2.5958\n",
      "â”‚   â””â”€â”€ perplexity_token: 19.8143\n",
      "â””â”€â”€ VAL:\n",
      "    â”œâ”€â”€ ce_loss_char: 1.0336\n",
      "    â”œâ”€â”€ ce_loss_token: 3.2384\n",
      "    â”œâ”€â”€ perplexity_char: 2.8112\n",
      "    â””â”€â”€ perplexity_token: 25.4917\n",
      "â””â”€â”€ TRAINING:\n",
      "    â””â”€â”€ learning_rate: 0.000000\n"
     ]
    }
   ],
   "source": [
    "trainer.train(train_loader, val_loader, epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š Metrics (Epoch 19):\n",
      "â””â”€â”€ TEST:\n",
      "    â”œâ”€â”€ ce_loss_char: 1.0321\n",
      "    â”œâ”€â”€ ce_loss_token: 3.2346\n",
      "    â”œâ”€â”€ perplexity_char: 2.8070\n",
      "    â””â”€â”€ perplexity_token: 25.3965\n",
      "â””â”€â”€ TRAINING:\n",
      "    â””â”€â”€ learning_rate: 0.000000\n",
      "Generating with greedy search...\n",
      "Generating with beam search...\n",
      "Generating with sampling...\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>learning_rate</td><td>â–‚â–„â–…â–‡â–ˆâ–ˆâ–ˆâ–‡â–‡â–†â–†â–…â–„â–ƒâ–ƒâ–‚â–‚â–â–â–</td></tr><tr><td>test/ce_loss_char</td><td>â–</td></tr><tr><td>test/ce_loss_token</td><td>â–</td></tr><tr><td>test/perplexity_char</td><td>â–</td></tr><tr><td>test/perplexity_token</td><td>â–</td></tr><tr><td>train/ce_loss_char</td><td>â–ˆâ–…â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–</td></tr><tr><td>train/ce_loss_token</td><td>â–ˆâ–…â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–</td></tr><tr><td>train/perplexity_char</td><td>â–ˆâ–„â–ƒâ–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–</td></tr><tr><td>train/perplexity_token</td><td>â–ˆâ–ƒâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–</td></tr><tr><td>val/ce_loss_char</td><td>â–ˆâ–…â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–</td></tr><tr><td>val/ce_loss_token</td><td>â–ˆâ–…â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–</td></tr><tr><td>val/perplexity_char</td><td>â–ˆâ–„â–ƒâ–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–</td></tr><tr><td>val/perplexity_token</td><td>â–ˆâ–ƒâ–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>learning_rate</td><td>0.0</td></tr><tr><td>test/ce_loss_char</td><td>1.03212</td></tr><tr><td>test/ce_loss_token</td><td>3.23461</td></tr><tr><td>test/perplexity_char</td><td>2.80701</td></tr><tr><td>test/perplexity_token</td><td>25.39651</td></tr><tr><td>train/ce_loss_char</td><td>0.95389</td></tr><tr><td>train/ce_loss_token</td><td>2.9864</td></tr><tr><td>train/perplexity_char</td><td>2.59578</td></tr><tr><td>train/perplexity_token</td><td>19.81426</td></tr><tr><td>val/ce_loss_char</td><td>1.0336</td></tr><tr><td>val/ce_loss_token</td><td>3.23835</td></tr><tr><td>val/perplexity_char</td><td>2.81118</td></tr><tr><td>val/perplexity_token</td><td>25.49167</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">test-lm</strong> at: <a href='https://wandb.ai/psamal-carnegie-mellon-university/S25-HW4P1-TA/runs/n63le4k3' target=\"_blank\">https://wandb.ai/psamal-carnegie-mellon-university/S25-HW4P1-TA/runs/n63le4k3</a><br> View project at: <a href='https://wandb.ai/psamal-carnegie-mellon-university/S25-HW4P1-TA' target=\"_blank\">https://wandb.ai/psamal-carnegie-mellon-university/S25-HW4P1-TA</a><br>Synced 5 W&B file(s), 41 media file(s), 0 artifact file(s) and 25 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250302_055904-n63le4k3/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_metrics, test_generation_results = trainer.evaluate(test_loader)\n",
    "# Cleanup\n",
    "trainer.cleanup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission\n",
    "To submit your assignment, you will need to create a `handin.tar` with the following directory structure:\n",
    "\n",
    "```\n",
    "handin/\n",
    "â”œâ”€â”€ mytorch/                    # Your implemented modules\n",
    "â”‚   â”œâ”€â”€ __init__.py\n",
    "â”‚   â”œâ”€â”€ softmax.py              # Softmax implementation\n",
    "â”‚   â”œâ”€â”€ attention.py            # Attention implementations\n",
    "â”‚   â””â”€â”€ ...\n",
    "â”œâ”€â”€ test_metrics.json     # Results from evaluation\n",
    "â”œâ”€â”€ test_generated_results.json    # Sample text generations\n",
    "â””â”€â”€ model_arch.txt              # Model architecture summary\n",
    "```\n",
    "\n",
    "- `TA TODO`: Simply run the cell below once you are satisfied with your current state and this will create the `handin.tar` file.\n",
    "- `TA TODO`: Upload the `handin.tar` file to the `HW4P1` assignment on Autolab.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created handin.tar successfully!\n"
     ]
    }
   ],
   "source": [
    "# Create temporary handin directory\n",
    "if os.path.exists('handin'):\n",
    "    shutil.rmtree('handin')\n",
    "os.makedirs('handin')\n",
    "\n",
    "# Copy mytorch directory\n",
    "shutil.copytree('mytorch', 'handin/mytorch')\n",
    "\n",
    "# Save final results\n",
    "with open('handin/test_metrics.json', 'w') as f:\n",
    "    json.dump(test_metrics, f, indent=4)\n",
    "\n",
    "with open('handin/test_generated_results.json', 'w') as f:\n",
    "    json.dump(test_generation_results['greedy'], f, indent=4)\n",
    "\n",
    "# Save model architecture\n",
    "with open('handin/model_arch.txt', 'w') as f:\n",
    "    f.write(str(model_stats))\n",
    "\n",
    "# Create tar file with all exclusions handled by filter\n",
    "with tarfile.open('handin.tar', 'w') as tar:\n",
    "    def filter_files(tarinfo):\n",
    "        # Skip unwanted files\n",
    "        if any(pattern in tarinfo.name for pattern in [\n",
    "            '.DS_Store',\n",
    "            '__pycache__',\n",
    "            '.pyc'\n",
    "        ]):\n",
    "            return None\n",
    "        return tarinfo\n",
    "    \n",
    "    tar.add('handin', arcname='handin', filter=filter_files)\n",
    "\n",
    "# Cleanup\n",
    "shutil.rmtree('handin')\n",
    "\n",
    "print(\"Created handin.tar successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/ocean/projects/cis220031p/psamal/IDL-HW4\n",
      "colab_requirements.txt\thw4_data_subset  mytorch\t       tests\n",
      "config.yaml\t\thw4lib\t\t psc_requirements.txt  wandb\n",
      "expts\t\t\thw4p2_sol.json\t README.md\n"
     ]
    }
   ],
   "source": [
    "# You should see the handin.tar file in the current directory\n",
    "!pwd\n",
    "!ls "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hw4_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
