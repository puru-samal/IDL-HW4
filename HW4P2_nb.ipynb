{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup \n",
    "- `TA TODO`: Setup based on your environment. Reach out to me if you face any issues. Also, any feedback/improvements to the setup process for the students based on your experience setting up here would be very appreciated! (PS: I am still figuring out the best way to do this.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local \n",
    "\n",
    "The assignment is designed in a manner that you can do most of the work `implementation` locally. We would recommend that you pass all the tests locally using the `hw4_data_subset` we've provided before moving to a GPU runtime. To do this simply:\n",
    "- Create a new conda environment with `conda create -n hw4 python=3.12.4`\n",
    "- Activate the conda environment with `conda activate hw4`\n",
    "- Install the dependencies with `pip install -r requirements.txt`\n",
    "- Ensure that your notebook is in the same directory as the `handout` folder. (See the expected directory structure in the `README.md`)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Colab (`TA TODO`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Get Repo (TA-Only, will be handout for students)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `INTERNAL TODO`: Need to switch this to handout upload for students."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'IDL-HW4'...\n",
      "remote: Enumerating objects: 625, done.\u001b[K\n",
      "remote: Counting objects: 100% (625/625), done.\u001b[K\n",
      "remote: Compressing objects: 100% (536/536), done.\u001b[K\n",
      "remote: Total 625 (delta 170), reused 541 (delta 86), pack-reused 0 (from 0)\u001b[K\n",
      "Receiving objects: 100% (625/625), 14.09 MiB | 50.62 MiB/s, done.\n",
      "Resolving deltas: 100% (170/170), done.\n",
      "Updating files: 100% (403/403), done.\n"
     ]
    }
   ],
   "source": [
    "GITHUB_USERNAME = \"puru-samal\"\n",
    "REPO_NAME = \"IDL-HW4\"\n",
    "BRANCH_NAME = \"TA\"\n",
    "ACCESS_TOKEN = \"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\"\n",
    "repo_url = f\"https://{GITHUB_USERNAME}:{ACCESS_TOKEN}@github.com/{GITHUB_USERNAME}/{REPO_NAME}.git\"\n",
    "!git clone -b {BRANCH_NAME} {repo_url}\n",
    "#!git clone {repo_url}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### If I announce a new commit, please delete and re-clone the repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf IDL-HW4/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Get Data\n",
    "- `INTERNAL TODO`: Need to switch this download from kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gdown 1-0e9Gnl4nm6wbIuE_Yxl2wRZI8yGxHm6 --output hw4_data.tar.gz\n",
    "!tar -xf hw4_data.tar.gz\n",
    "!rm -rf hw4_data.tar.gz\n",
    "!du -h max-depth=3 hw4_data_kaggle/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Install Dependencies\n",
    "- `NOTE`: Colab may prompt you to restart your runtime. Do so then proceed to the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --no-deps -r IDL-HW4/colab_requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Move to Project Directory\n",
    "- `NOTE`: You may have to repeat this on restarting your runtime. You can do a `pwd` to check if you are in the right directory.\n",
    "- `NOTE`: Your data directory should be one level up from your project directory. Keep this in mind when you are setting your `root` in the config file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "colab_requirements.txt\thw4lib\t\tmytorch\t\t      README.md\n",
      "hw4_data_subset\t\thw4p2_sol.json\tpsc_requirements.txt  tests\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir('IDL-HW4')\n",
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PSC (`TA TODO`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Preliminaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `Step 0:` ssh into Bridges2 with `ssh username@bridges2.psc.edu`\n",
    "- `Step 1:` cd into your project directory with `cd $PROJECT`\n",
    "- `Step 2:` Load the anaconda module with `module load anaconda3`\n",
    "- `Step 3:` Create a new conda environment with `conda create -n your_env_name python=3.12.4`\n",
    "- `Step 4:` Activate the conda environment with `conda activate your_env_name` (Make sure to deactivate any existing conda environment first with `conda deactivate`)\n",
    "- `Step 5:` Install dependencies with `pip install -r /ocean/projects/cis240101p/psamal/psc_requirements.txt`\n",
    "- `Step 6:` Get a compute node with `interact -p GPU-shared --gres=gpu:v100-32:1 -t 8:00:00`\n",
    "- `Step 7:` Node allocation might have caused your conda environment to be deactivated, so follow `Step 4` again\n",
    "- `Step 8:` Now follow your usual steps to start a jupyter notebook. For me this is:\n",
    "  - Start a jupyter notebook with `jupyter notebook --no-browser --ip=0.0.0.0` \n",
    "  - On a separate terminal, start a tunnel with `ssh -L 8888:{hostname}:{port} bridges2.psc.edu -l username`\n",
    "  - Select the appropriate kernel on VSCode: Kernel -> Select Another Kernel -> Existing Jupyter Server -> `http://127.0.0.1:{port}/tree?token={token}`\n",
    "- `Step 9:` Now follow the instructions below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Get Repo (TA-Only, will be handout for students)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'IDL-HW4'...\n",
      "remote: Enumerating objects: 625, done.\u001b[K\n",
      "remote: Counting objects: 100% (625/625), done.\u001b[K\n",
      "remote: Compressing objects: 100% (536/536), done.\u001b[K\n",
      "remote: Total 625 (delta 170), reused 541 (delta 86), pack-reused 0 (from 0)\u001b[K\n",
      "Receiving objects: 100% (625/625), 14.09 MiB | 52.46 MiB/s, done.\n",
      "Resolving deltas: 100% (170/170), done.\n",
      "Updating files: 100% (410/410), done.\n"
     ]
    }
   ],
   "source": [
    "GITHUB_USERNAME = \"puru-samal\"\n",
    "REPO_NAME = \"IDL-HW4\"\n",
    "BRANCH_NAME = \"TA\"\n",
    "ACCESS_TOKEN = \"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\"\n",
    "repo_url = f\"https://{GITHUB_USERNAME}:{ACCESS_TOKEN}@github.com/{GITHUB_USERNAME}/{REPO_NAME}.git\"\n",
    "#!git clone -b {BRANCH_NAME} {repo_url} # TA ONLY\n",
    "!git clone {repo_url}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### If I announce a new commit, please delete and re-clone the repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf IDL-HW4/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Move to Project Directory\n",
    "- `NOTE`: You may have to repeat this on anytime you restart your runtime. You can do a `pwd` or `ls` to check if you are in the right directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "autograde-Makefile\thw4lib\t\tmytorch\n",
      "autograde.tar\t\tHW4P1_nb.ipynb\tpsc_requirements.txt\n",
      "colab_requirements.txt\tHW4P2_nb.ipynb\tREADME.md\n",
      "download_data.py\thw4p2_sol.json\tsimulate_autolab.py\n",
      "hw4_data_subset\t\tMakefile\ttests\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir('IDL-HW4')\n",
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Get Data\n",
    "- `NOTE`: We are using `$LOCAL`: the scratch storage on local disk on the node running a job to store out data. Disk accesses are much faster than what you would get from `$PROJECT` storage, but `IT IS NOT PERSISTENT`. \n",
    "- `NOTE`: Make sure you have a node allocated to you with `interact -p GPU-shared --gres=gpu:v100-32:1 -t 8:00:00`\n",
    "- Read more about it PSC File Spaces [here](https://www.psc.edu/resources/bridges-2/user-guide#file-spaces)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=1-0e9Gnl4nm6wbIuE_Yxl2wRZI8yGxHm6\n",
      "From (redirected): https://drive.google.com/uc?id=1-0e9Gnl4nm6wbIuE_Yxl2wRZI8yGxHm6&confirm=t&uuid=523fa81f-d514-4479-9d92-564e32ebd9a8\n",
      "To: /local/hw4_data.tar.gz\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12.7G/12.7G [02:37<00:00, 80.7MB/s]\n",
      "hw4_data.tar.gz\n",
      "28440\t/local/hw4_data_kaggle/hw4p1_data/val\n",
      "28428\t/local/hw4_data_kaggle/hw4p1_data/test\n",
      "1082352\t/local/hw4_data_kaggle/hw4p1_data/train\n",
      "1139220\t/local/hw4_data_kaggle/hw4p1_data\n",
      "14328964\t/local/hw4_data_kaggle/hw4p2_data/train-clean-100\n",
      "765972\t/local/hw4_data_kaggle/hw4p2_data/test-clean\n",
      "775120\t/local/hw4_data_kaggle/hw4p2_data/dev-clean\n",
      "15870060\t/local/hw4_data_kaggle/hw4p2_data\n",
      "17009280\t/local/hw4_data_kaggle\n",
      "17009280\t/local/\n"
     ]
    }
   ],
   "source": [
    "!gdown 1-0e9Gnl4nm6wbIuE_Yxl2wRZI8yGxHm6 --output $LOCAL/hw4_data.tar.gz\n",
    "!ls $LOCAL/\n",
    "!tar -xf $LOCAL/hw4_data.tar.gz -C $LOCAL/\n",
    "!rm -rf $LOCAL/hw4_data.tar.gz\n",
    "!du --max-depth=3 $LOCAL/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "from hw4lib.data import (\n",
    "    H4Tokenizer,\n",
    "    ASRDataset,\n",
    "    verify_dataloader\n",
    ")\n",
    "from hw4lib.model import (\n",
    "    EncoderDecoderTransformer\n",
    ")\n",
    "from hw4lib.trainers import (\n",
    "    ASRTrainer,\n",
    "    ProgressiveTrainer\n",
    ")\n",
    "from torch.utils.data import DataLoader\n",
    "import yaml\n",
    "import gc\n",
    "import torch\n",
    "from torchinfo import summary\n",
    "import os\n",
    "import json\n",
    "import tarfile\n",
    "import shutil\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementations\n",
    "- `TA TODO`: \n",
    "  - `MANDATORY`: Run these cells to verify that the testing works in your chosen environment. Lmk if it doesn't.\n",
    "  - `OPTIONAL`: Do read through the implementations. Any feedback regarding them would be very appreciated!\n",
    "- `NOTE`: All of these implementations have detailed specification, implementation details, and hints in their respective source files. Make sure to read all of them in their entirety to understand the implementation details!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Implementation\n",
    "- `TODO`: Implement the `ASRDataset` class in `hw4lib/data/asr_dataset.py`. \n",
    "- You will have to implement parts of `__init__` and completely implement the `__len__`, `__getitem__` and `collate_fn` methods. \n",
    "- `TODO`: Then run the cell below to check your implementation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m tests.test_dataset_asr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Implementations\n",
    "\n",
    "- `TODO`: Implement the `CrossAttentionLayer` class in `hw4lib/model/sublayers.py`.\n",
    "- `TODO`: Implement the `CrossAttentionDecoderLayer` class in `hw4lib/model/decoder_layers.py`.\n",
    "- `TODO`: Implement the `SelfAttentionEncoderLayer` class in `hw4lib/model/encoder_layers.py`. This will be mostly a copy-paste of the `SelfAttentionDecoderLayer` class in `hw4lib/model/decoder_layers.py` with one minor diffrence: it can attend to all positions in the input sequence.\n",
    "- `TODO`: Implement the `EncoderDecoderTransformer` class in `hw4lib/model/transformers.py`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer Sublayers\n",
    "- `TODO`: Now, Implement the `CrossAttentionLayer` class in `hw4lib/model/sublayers.py`.\n",
    "- `NOTE`: You should have already implemented the `SelfAttentionLayer`, and `FeedForwardLayer` classes in `hw4lib/model/sublayers.py`.\n",
    "- `TODO`: Then run the cell below to check your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m tests.test_sublayer_crossattention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer Cross-Attention Decoder Layer\n",
    "- `TODO`: Implement the `CrossAttentionDecoderLayer` class in `hw4lib/model/decoder_layers.py`.\n",
    "- `TODO`: Then run the cell below to check your implementation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m tests.test_decoderlayer_crossattention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer Self-Attention Encoder Layer\n",
    "- `TODO`: Implement the `SelfAttentionEncoderLayer` class in `hw4lib/model/encoder_layers.py`.\n",
    "- `TODO`: Then run the cell below to check your implementation.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m tests.test_encoderlayer_selfattention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder-Decoder Transformer\n",
    "\n",
    "- `TODO`: Implement the  `EncoderDecoderTransformer` class in `hw4lib/model/transformers.py`.\n",
    "- `TODO`: Then run the cell below to check your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m tests.test_transformer_encoder_decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoding Implementations \n",
    "- `TODO`: We highly recommend you to implement the `generate_beam` method of the `SequenceGenerator` class in `hw4lib/decoding/sequence_generator.py`.\n",
    "- `TODO`: Then run the cell below to check your implementation.\n",
    "- `NOTE`: This is an optional but highly recommended task for `HW4P2` to ease the journey to high cutoffs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m tests.test_decoding --mode beam  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config \n",
    "\n",
    "- Set your config for your ablation study. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing config.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile config.yaml\n",
    "\n",
    "Name                      : \"Puru\"\n",
    "\n",
    "###### Tokenization ------------------------------------------------------------\n",
    "tokenization:\n",
    "  token_type                : \"1k\"       # [char, 1k, 5k, 10k]\n",
    "  token_map :\n",
    "      'char': 'hw4lib/data/tokenizer_jsons/tokenizer_char.json'\n",
    "      '1k'  : 'hw4lib/data/tokenizer_jsons/tokenizer_1000.json'\n",
    "      '5k'  : 'hw4lib/data/tokenizer_jsons/tokenizer_5000.json'\n",
    "      '10k' : 'hw4lib/data/tokenizer_jsons/tokenizer_10000.json'\n",
    "\n",
    "###### Dataset -----------------------------------------------------------------\n",
    "data:\n",
    "  root                 : \"/local/hw4_data_kaggle/hw4p2_data\"  # TODO: Set the root path of your data\n",
    "  train_partition      : \"train-clean-100\"  # paired text-speech for ASR pre-training\n",
    "  val_partition        : \"dev-clean\"    # paired text-speech for ASR pre-training\n",
    "  test_partition       : \"test-clean\"   # paired text-speech for ASR pre-trainin\n",
    "  subset               : 0.25           # Load a subset of the data (for debugging, testing, etc\n",
    "  batch_size           : 32           #   \n",
    "  NUM_WORKERS          : 2            # Set to 0 for CPU\n",
    "  norm                 : 'global_mvn' # ['global_mvn', 'cepstral', 'none']\n",
    "  num_feats            : 80\n",
    "\n",
    "  ###### SpecAugment ---------------------------------------------------------------\n",
    "  specaug                   : False\n",
    "  specaug_conf:\n",
    "    apply_freq_mask         : True\n",
    "    freq_mask_width_range   : 4\n",
    "    num_freq_mask           : 2\n",
    "    apply_time_mask         : True\n",
    "    time_mask_width_range   : 10\n",
    "    num_time_mask           : 4\n",
    "\n",
    "###### Network Specs -------------------------------------------------------------\n",
    "model: # Encoder-Decoder Transformer (HW4P2)\n",
    "  # Speech embedding parameters\n",
    "  input_dim: 80              # Speech feature dimension\n",
    "  time_reduction: 2          # Time dimension downsampling factor\n",
    "  reduction_method: 'both'   # The source_embedding reduction method ['lstm', 'conv', 'both']\n",
    "  \n",
    "  # Architecture parameters\n",
    "  d_model: 256           # Model dimension\n",
    "  num_encoder_layers: 2  # Number of encoder layers\n",
    "  num_decoder_layers: 2  # Number of decoder layers\n",
    "  num_encoder_heads: 2   # Number of encoder attention heads\n",
    "  num_decoder_heads: 2   # Number of decoder attention heads\n",
    "  d_ff_encoder: 512     # Feed-forward dimension for encoder\n",
    "  d_ff_decoder: 512     # Feed-forward dimension for decoder\n",
    "  skip_encoder_pe: False # Whether to skip positional encoding for encoder\n",
    "  skip_decoder_pe: False # Whether to skip positional encoding for decoder\n",
    "  \n",
    "  # Common parameters\n",
    "  dropout: 0.0          # Dropout rate\n",
    "  layer_drop_rate: 0.1  # Layer dropout rate\n",
    "  weight_tying: False   # Whether to use weight tying\n",
    "  \n",
    "###### Common Training Parameters ------------------------------------------------\n",
    "training:\n",
    "  use_wandb                   : False\n",
    "  wandb_run_id                : \"none\" # \"none\" or \"run_id\"\n",
    "  resume                      : False\n",
    "  epochs                      : 70\n",
    "  gradient_accumulation_steps : 4\n",
    "  wandb_project               : \"S25-HW4P2-TA\"\n",
    "\n",
    "###### Loss ----------------------------------------------------------------------\n",
    "loss: # Just good ol' CrossEntropy\n",
    "  label_smoothing: 0.0\n",
    "  ctc_weight: 0.3\n",
    "\n",
    "###### Optimizer -----------------------------------------------------------------\n",
    "optimizer:\n",
    "  name: \"adam\" # Options: sgd, adam, adamw\n",
    "  lr: 0.0002  # Base learning rate\n",
    "\n",
    "  # Common parameters\n",
    "  weight_decay: 0.0001\n",
    "\n",
    "  # Parameter groups\n",
    "  param_groups:\n",
    "    - name: self_attn\n",
    "      patterns: [\"self_attn\"]  # Will match all parameters containing \"encoder\"\n",
    "      lr: 0.0002  # LR for self_attn\n",
    "      layer_decay:\n",
    "        enabled: False\n",
    "        decay_rate: 0.8\n",
    "    \n",
    "    - name: ffn\n",
    "      patterns: [\"ffn\"]\n",
    "      lr: 0.0002  # LR for ffn\n",
    "      layer_decay:\n",
    "        enabled: False\n",
    "        decay_rate: 0.8\n",
    "  \n",
    "  # Layer-wise learning rates\n",
    "  layer_decay:\n",
    "    enabled: False\n",
    "    decay_rate: 0.75\n",
    "\n",
    "  # SGD specific parameters\n",
    "  sgd:\n",
    "    momentum: 0.9\n",
    "    nesterov: True\n",
    "    dampening: 0\n",
    "\n",
    "  # Adam specific parameters\n",
    "  adam:\n",
    "    betas: [0.9, 0.999]\n",
    "    eps: 1.0e-8\n",
    "    amsgrad: False\n",
    "\n",
    "  # AdamW specific parameters\n",
    "  adamw:\n",
    "    betas: [0.9, 0.999]\n",
    "    eps: 1.0e-8\n",
    "    amsgrad: False\n",
    "\n",
    "###### Scheduler -----------------------------------------------------------------\n",
    "scheduler:\n",
    "  name: \"cosine\"  # Options: reduce_lr, cosine, cosine_warm\n",
    "\n",
    "  # ReduceLROnPlateau specific parameters\n",
    "  reduce_lr:\n",
    "    mode: \"min\"  # Options: min, max\n",
    "    factor: 0.1  # Factor to reduce learning rate by\n",
    "    patience: 10  # Number of epochs with no improvement after which LR will be reduced\n",
    "    threshold: 0.0001  # Threshold for measuring the new optimum\n",
    "    threshold_mode: \"rel\"  # Options: rel, abs\n",
    "    cooldown: 0  # Number of epochs to wait before resuming normal operation\n",
    "    min_lr: 0.0000001  # Minimum learning rate\n",
    "    eps: 1e-8  # Minimal decay applied to lr\n",
    "\n",
    "  # CosineAnnealingLR specific parameters\n",
    "  cosine:\n",
    "    T_max: 35  # Maximum number of iterations\n",
    "    eta_min: 0.0000001  # Minimum learning rate\n",
    "    last_epoch: -1\n",
    "\n",
    "  # CosineAnnealingWarmRestarts specific parameters\n",
    "  cosine_warm:\n",
    "    T_0: 10    # Number of iterations for the first restart\n",
    "    T_mult: 10 # Factor increasing T_i after each restart\n",
    "    eta_min: 0.0000001  # Minimum learning rate\n",
    "    last_epoch: -1\n",
    "\n",
    "  # Warmup parameters (can be used with any scheduler)\n",
    "  warmup:\n",
    "    enabled: False\n",
    "    type: \"exponential\"  # Options: linear, exponential\n",
    "    epochs: 5\n",
    "    start_factor: 0.1\n",
    "    end_factor: 1.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('config.yaml', 'r') as file:\n",
    "    config = yaml.safe_load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "                          Tokenizer Configuration (1k)                          \n",
      "--------------------------------------------------------------------------------\n",
      "Vocabulary size:     1000\n",
      "\n",
      "Special Tokens:\n",
      "PAD:              0\n",
      "UNK:              1\n",
      "MASK:             2\n",
      "SOS:              3\n",
      "EOS:              4\n",
      "BLANK:            5\n",
      "\n",
      "Validation Example:\n",
      "--------------------------------------------------------------------------------\n",
      "Input text:  [SOS]HI DEEP LEARNERS[EOS]\n",
      "Tokens:      ['[SOS]', 'H', 'I', 'Ä DE', 'EP', 'Ä LE', 'AR', 'N', 'ERS', '[EOS]']\n",
      "Token IDs:   [3, 14, 15, 159, 290, 228, 71, 20, 214, 4]\n",
      "Decoded:     [SOS]HI DEEP LEARNERS[EOS]\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "Tokenizer = H4Tokenizer(\n",
    "    token_map  = config['tokenization']['token_map'], \n",
    "    token_type = config['tokenization']['token_type']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data for train-clean-100 partition...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7134/7134 [00:05<00:00, 1245.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global stats computed from training set.\n",
      "Loading data for dev-clean partition...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 675/675 [00:00<00:00, 2272.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data for test-clean partition...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 655/655 [00:00<00:00, 3953.81it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "744"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = ASRDataset(\n",
    "    partition=config['data']['train_partition'],\n",
    "    config=config['data'],\n",
    "    tokenizer=Tokenizer,\n",
    "    isTrainPartition=True,\n",
    "    global_stats=None  # Will compute stats from training data\n",
    ")\n",
    "\n",
    "# TODO: Get the computed global stats from training set\n",
    "global_stats = None\n",
    "if config['data']['norm'] == 'global_mvn':\n",
    "    global_stats = (train_dataset.global_mean, train_dataset.global_std)\n",
    "    print(f\"Global stats computed from training set.\")\n",
    "\n",
    "val_dataset = ASRDataset(\n",
    "    partition=config['data']['val_partition'],\n",
    "    config=config['data'],\n",
    "    tokenizer=Tokenizer,\n",
    "    isTrainPartition=False,\n",
    "    global_stats=global_stats\n",
    ")\n",
    "\n",
    "test_dataset = ASRDataset(\n",
    "    partition=config['data']['test_partition'],\n",
    "    config=config['data'],\n",
    "    tokenizer=Tokenizer,\n",
    "    isTrainPartition=False,\n",
    "    global_stats=global_stats\n",
    ")\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader    = DataLoader(\n",
    "    dataset     = train_dataset,\n",
    "    batch_size  = config['data']['batch_size'],\n",
    "    shuffle     = True,\n",
    "    num_workers = config['data']['NUM_WORKERS'] if device == 'cuda' else 0,\n",
    "    pin_memory  = True,\n",
    "    collate_fn  = train_dataset.collate_fn   \n",
    ")\n",
    "\n",
    "val_loader      = DataLoader(\n",
    "    dataset     = val_dataset,\n",
    "    batch_size  = config['data']['batch_size'],\n",
    "    shuffle     = False,\n",
    "    num_workers = config['data']['NUM_WORKERS'] if device == 'cuda' else 0,\n",
    "    pin_memory  = True,\n",
    "    collate_fn  = val_dataset.collate_fn   \n",
    ")\n",
    "\n",
    "test_loader     = DataLoader(\n",
    "    dataset     = test_dataset,\n",
    "    batch_size  = config['data']['batch_size'],\n",
    "    shuffle     = False,\n",
    "    num_workers = config['data']['NUM_WORKERS'] if device == 'cuda' else 0,\n",
    "    pin_memory  = True,\n",
    "    collate_fn  = test_dataset.collate_fn   \n",
    ")\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataloader Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "             Dataloader Verification              \n",
      "==================================================\n",
      "Dataloader Partition     : train-clean-100\n",
      "--------------------------------------------------\n",
      "Number of Batches        : 223\n",
      "Batch Size               : 32\n",
      "--------------------------------------------------\n",
      "Checking shapes of the data...                    \n",
      "\n",
      "Feature Shape            : [32, 2051, 80]\n",
      "Shifted Transcript Shape : [32, 91]\n",
      "Golden Transcript Shape  : [32, 91]\n",
      "Feature Lengths Shape    : [32]\n",
      "Transcript Lengths Shape : [32]\n",
      "--------------------------------------------------\n",
      "Max Feature Length       : 2160\n",
      "Max Transcript Length    : 115\n",
      "Avg. Chars per Token     : 3.13\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "verify_dataloader(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "             Dataloader Verification              \n",
      "==================================================\n",
      "Dataloader Partition     : dev-clean\n",
      "--------------------------------------------------\n",
      "Number of Batches        : 22\n",
      "Batch Size               : 32\n",
      "--------------------------------------------------\n",
      "Checking shapes of the data...                    \n",
      "\n",
      "Feature Shape            : [32, 3676, 80]\n",
      "Shifted Transcript Shape : [32, 132]\n",
      "Golden Transcript Shape  : [32, 132]\n",
      "Feature Lengths Shape    : [32]\n",
      "Transcript Lengths Shape : [32]\n",
      "--------------------------------------------------\n",
      "Max Feature Length       : 3922\n",
      "Max Transcript Length    : 139\n",
      "Avg. Chars per Token     : 3.03\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "verify_dataloader(val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "             Dataloader Verification              \n",
      "==================================================\n",
      "Dataloader Partition     : test-clean\n",
      "--------------------------------------------------\n",
      "Number of Batches        : 21\n",
      "Batch Size               : 32\n",
      "--------------------------------------------------\n",
      "Checking shapes of the data...                    \n",
      "\n",
      "Feature Shape            : [32, 2099, 80]\n",
      "Feature Lengths Shape    : [32]\n",
      "--------------------------------------------------\n",
      "Max Feature Length       : 4239\n",
      "Max Transcript Length    : 0\n",
      "Avg. Chars per Token     : 0.00\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "verify_dataloader(test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Max Lengths\n",
    "Calculating the maximum transcript length across your dataset is a crucial step when working with certain transformer models. \n",
    "-  We'll use sinusoidal positional encodings that must be precomputed up to a fixed maximum length.\n",
    "- This maximum length is a hyperparameter that determines:\n",
    "  - How long of a sequence your model can process\n",
    "  - The size of your positional encoding matrix\n",
    "  - Memory requirements during training and inference\n",
    "- `Requirements`: For this assignment, ensure your positional encodings can accommodate at least the longest sequence in your dataset to prevent truncation. However, you can set this value higher if you anticipate using your languagemodel to work with longer sequences in future tasks (hint: this might be useful for P2! ðŸ˜‰).\n",
    "- `NOTE`: We'll be using the same positional encoding matrix for all sequences in your dataset. Take this into account when setting your maximum length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Max Feature Length             : 4239\n",
      "Max Transcript Length          : 139\n",
      "Overall Max Length             : 4239\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "max_feat_len       = max(train_dataset.feat_max_len, val_dataset.feat_max_len, test_dataset.feat_max_len)\n",
    "max_transcript_len = max(train_dataset.text_max_len, val_dataset.text_max_len, test_dataset.text_max_len)\n",
    "max_len            = max(max_feat_len, max_transcript_len)\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(f\"{'Max Feature Length':<30} : {max_feat_len}\")\n",
    "print(f\"{'Max Transcript Length':<30} : {max_transcript_len}\")\n",
    "print(f\"{'Overall Max Length':<30} : {max_len}\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Default Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================================================================================\n",
      "Layer (type:depth-idx)                        Output Shape              Param #\n",
      "===============================================================================================\n",
      "EncoderDecoderTransformer                     [32, 100, 1000]           --\n",
      "â”œâ”€SpeechEmbedding: 1-1                        [32, 1011, 256]           --\n",
      "â”‚    â””â”€Conv2DSubsampling: 2-1                 [32, 2022, 256]           --\n",
      "â”‚    â”‚    â””â”€Sequential: 3-1                   [32, 256, 2022, 76]       592,640\n",
      "â”‚    â”‚    â””â”€Linear: 3-2                       [32, 2022, 256]           4,980,992\n",
      "â”‚    â”‚    â””â”€Dropout: 3-3                      [32, 2022, 256]           --\n",
      "â”‚    â””â”€StackedBLSTMEmbedding: 2-2             [32, 1011, 256]           --\n",
      "â”‚    â”‚    â””â”€LSTM: 3-4                         [43074, 256]              395,264\n",
      "â”‚    â”‚    â””â”€MaxPool1d: 3-5                    [32, 256, 1011]           --\n",
      "â”‚    â”‚    â””â”€LSTM: 3-6                         [21529, 256]              395,264\n",
      "â”‚    â”‚    â””â”€MaxPool1d: 3-7                    [32, 256, 1011]           --\n",
      "â”‚    â”‚    â””â”€Linear: 3-8                       [32, 1011, 256]           65,792\n",
      "â”‚    â”‚    â””â”€Dropout: 3-9                      [32, 1011, 256]           --\n",
      "â”œâ”€PositionalEncoding: 1-2                     [32, 1011, 256]           --\n",
      "â”œâ”€Dropout: 1-3                                [32, 1011, 256]           --\n",
      "â”œâ”€ModuleList: 1-4                             --                        --\n",
      "â”‚    â””â”€SelfAttentionEncoderLayer: 2-3         [32, 1011, 256]           --\n",
      "â”‚    â”‚    â””â”€SelfAttentionLayer: 3-10          [32, 1011, 256]           263,680\n",
      "â”‚    â”‚    â””â”€FeedForwardLayer: 3-11            [32, 1011, 256]           263,424\n",
      "â”‚    â””â”€SelfAttentionEncoderLayer: 2-4         [32, 1011, 256]           --\n",
      "â”‚    â”‚    â””â”€SelfAttentionLayer: 3-12          [32, 1011, 256]           263,680\n",
      "â”‚    â”‚    â””â”€FeedForwardLayer: 3-13            [32, 1011, 256]           263,424\n",
      "â”œâ”€LayerNorm: 1-5                              [32, 1011, 256]           512\n",
      "â”œâ”€Sequential: 1-6                             [1011, 32, 1000]          --\n",
      "â”‚    â””â”€Linear: 2-5                            [1011, 32, 1000]          257,000\n",
      "â”‚    â””â”€LogSoftmax: 2-6                        [1011, 32, 1000]          --\n",
      "â”œâ”€Embedding: 1-7                              [32, 100, 256]            256,000\n",
      "â”œâ”€PositionalEncoding: 1-8                     [32, 100, 256]            --\n",
      "â”œâ”€Dropout: 1-9                                [32, 100, 256]            --\n",
      "â”œâ”€ModuleList: 1-10                            --                        --\n",
      "â”‚    â””â”€CrossAttentionDecoderLayer: 2-7        [32, 100, 256]            --\n",
      "â”‚    â”‚    â””â”€SelfAttentionLayer: 3-14          [32, 100, 256]            263,680\n",
      "â”‚    â”‚    â””â”€CrossAttentionLayer: 3-15         [32, 100, 256]            263,680\n",
      "â”‚    â”‚    â””â”€FeedForwardLayer: 3-16            [32, 100, 256]            263,424\n",
      "â”‚    â””â”€CrossAttentionDecoderLayer: 2-8        [32, 100, 256]            --\n",
      "â”‚    â”‚    â””â”€SelfAttentionLayer: 3-17          [32, 100, 256]            263,680\n",
      "â”‚    â”‚    â””â”€CrossAttentionLayer: 3-18         [32, 100, 256]            263,680\n",
      "â”‚    â”‚    â””â”€FeedForwardLayer: 3-19            [32, 100, 256]            263,424\n",
      "â”œâ”€LayerNorm: 1-11                             [32, 100, 256]            512\n",
      "â”œâ”€Linear: 1-12                                [32, 100, 1000]           257,000\n",
      "===============================================================================================\n",
      "Total params: 9,836,752\n",
      "Trainable params: 9,836,752\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (Units.TERABYTES): 9.45\n",
      "===============================================================================================\n",
      "Input size (MB): 20.77\n",
      "Forward/backward pass size (MB): 21853.42\n",
      "Params size (MB): 33.03\n",
      "Estimated Total Size (MB): 21907.22\n",
      "===============================================================================================\n"
     ]
    }
   ],
   "source": [
    "model_config = config['model'].copy()\n",
    "model_config.update({\n",
    "    'max_len': max_len,\n",
    "    'num_classes': Tokenizer.vocab_size\n",
    "})\n",
    "\n",
    "model = EncoderDecoderTransformer(**model_config)\n",
    "\n",
    "# Get some inputs from the train dataloader\n",
    "for batch in train_loader:\n",
    "    padded_feats, padded_shifted, padded_golden, feat_lengths, transcript_lengths = batch\n",
    "    break\n",
    "\n",
    "\n",
    "model_stats = summary(model, input_data=[padded_feats, padded_shifted, feat_lengths, transcript_lengths])\n",
    "print(model_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder-Only Initialized Load\n",
    "\n",
    "- `TA TODO`: If you have been assigned this task, make sure to set the `decoder_checkpoint` in the config to the path of the decoder checkpoint you trained during your `HW4P1` ablation study. \n",
    "- Also, it goes without saying that your Decoder-Only transformer's parameters should be compatible with the Encoder-Decoder Transformer's parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = config['model'].copy()\n",
    "\n",
    "# `TA TODO`: Set the path to the decoder checkpoint.\n",
    "decoder_checkpoint = \"/path/to/decoder/checkpoint.pth\"\n",
    "freeze_transferred = True\n",
    "decoder_lr_factor  = 0.1\n",
    "model_config.update({\n",
    "    'max_len': max_len,\n",
    "    'num_classes': Tokenizer.vocab_size\n",
    "})\n",
    "\n",
    "model, pretrained_params = EncoderDecoderTransformer.from_pretrained_decoder(\n",
    "    decoder_checkpoint_path=decoder_checkpoint,\n",
    "    config=model_config,\n",
    "    freeze_transferred=freeze_transferred,\n",
    "    decoder_lr_factor=decoder_lr_factor\n",
    ")\n",
    "\n",
    "model.pretrained_params = pretrained_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Strategy 1: Cold-Start Trainer\n",
    "You will have to do some minor in-filling for the `ASRTrainer` class in `hw4lib/trainers/asr_trainer.py` before you can use it.\n",
    "- `TODO`: Fill in the `TODO`s in the `__init__`.\n",
    "- `TODO`: Fill in the `TODO`s in the `_train_epoch`.\n",
    "- `TODO`: Fill in the `TODO`s in the `recognize` method.\n",
    "- `TODO`: Fill in the `TODO`s in the `_validate_epoch`.\n",
    "- `TODO`: Fill in the `TODO`s in the `train` method.\n",
    "- `TODO`: Fill in the `TODO`s in the `evaluate` method.\n",
    "\n",
    "Every time you run the trainer, it will create a new directory in the `expts` folder with the following structure:\n",
    "```\n",
    "expts/\n",
    "    â””â”€â”€ {run_name}/\n",
    "        â”œâ”€â”€ config.yaml\n",
    "        â”œâ”€â”€ model_arch.txt\n",
    "        â”œâ”€â”€ checkpoints/\n",
    "        â”‚   â”œâ”€â”€ checkpoint-best-metric-model.pth\n",
    "        â”‚   â””â”€â”€ checkpoint-last-epoch-model.pth\n",
    "        â”œâ”€â”€ attn/\n",
    "        â”‚   â””â”€â”€ {attention visualizations}\n",
    "        â””â”€â”€ text/\n",
    "            â””â”€â”€ {generated text outputs}\n",
    "```\n",
    "\n",
    "- `TA TODO`: Please change the run name to the name that was assigned to you in the ablation sheet for easy referencing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "ðŸ”§ Configuring Optimizer:\n",
      "â”œâ”€â”€ Type: ADAM\n",
      "â”œâ”€â”€ Base LR: 0.0002\n",
      "â”œâ”€â”€ Weight Decay: 0.0001\n",
      "â”œâ”€â”€ Parameter Groups:\n",
      "â”‚   â”œâ”€â”€ Group: self_attn\n",
      "â”‚   â”‚   â”œâ”€â”€ LR: 0.0002\n",
      "â”‚   â”‚   â””â”€â”€ Patterns: ['self_attn']\n",
      "â”‚   â”œâ”€â”€ Group: ffn\n",
      "â”‚   â”‚   â”œâ”€â”€ LR: 0.0002\n",
      "â”‚   â”‚   â””â”€â”€ Patterns: ['ffn']\n",
      "â”‚   â””â”€â”€ Default Group (unmatched parameters)\n",
      "â””â”€â”€ Adam Specific:\n",
      "    â”œâ”€â”€ Betas: [0.9, 0.999]\n",
      "    â”œâ”€â”€ Epsilon: 1e-08\n",
      "    â””â”€â”€ AMSGrad: False\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Failed to run torchinfo. See above stack traces for more details. Executed layers up to: [Conv2DSubsampling: 2, Sequential: 3, Conv2d: 4, GELU: 4, Conv2d: 4, GELU: 4, Linear: 3, Dropout: 3]",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/torchinfo/torchinfo.py:295\u001b[39m, in \u001b[36mforward_pass\u001b[39m\u001b[34m(model, x, batch_dim, cache_forward_pass, device, mode, **kwargs)\u001b[39m\n\u001b[32m    294\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[32m--> \u001b[39m\u001b[32m295\u001b[39m     _ = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    296\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mdict\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1844\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1843\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1844\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1845\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m   1846\u001b[39m     \u001b[38;5;66;03m# run always called hooks if they have not already been run\u001b[39;00m\n\u001b[32m   1847\u001b[39m     \u001b[38;5;66;03m# For now only forward hooks have the always_call option but perhaps\u001b[39;00m\n\u001b[32m   1848\u001b[39m     \u001b[38;5;66;03m# this functionality should be added to full backward hooks as well.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1790\u001b[39m, in \u001b[36mModule._call_impl.<locals>.inner\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m   1788\u001b[39m     args = bw_hook.setup_input_hook(args)\n\u001b[32m-> \u001b[39m\u001b[32m1790\u001b[39m result = \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1791\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ocean/projects/cis220031p/psamal/IDL-HW4/IDL-HW4/hw4lib/model/transformers.py:392\u001b[39m, in \u001b[36mEncoderDecoderTransformer.forward\u001b[39m\u001b[34m(self, padded_sources, padded_targets, source_lengths, target_lengths)\u001b[39m\n\u001b[32m    391\u001b[39m \u001b[38;5;66;03m# TODO: Encode the source sequence\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m392\u001b[39m encoder_output, pad_mask_src, enc_running_att, ctc_inputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpadded_sources\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msource_lengths\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    394\u001b[39m \u001b[38;5;66;03m# TODO: Decode using encoder output\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ocean/projects/cis220031p/psamal/IDL-HW4/IDL-HW4/hw4lib/model/transformers.py:283\u001b[39m, in \u001b[36mEncoderDecoderTransformer.encode\u001b[39m\u001b[34m(self, padded_sources, source_lengths)\u001b[39m\n\u001b[32m    282\u001b[39m \u001b[38;5;66;03m# TODO: Apply speech embedding, positional encoding, and dropout\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m283\u001b[39m x_enc, x_enc_lengths = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msource_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpadded_sources\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msource_lengths\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    284\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.skip_encoder_pe:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1844\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1843\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1844\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1845\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m   1846\u001b[39m     \u001b[38;5;66;03m# run always called hooks if they have not already been run\u001b[39;00m\n\u001b[32m   1847\u001b[39m     \u001b[38;5;66;03m# For now only forward hooks have the always_call option but perhaps\u001b[39;00m\n\u001b[32m   1848\u001b[39m     \u001b[38;5;66;03m# this functionality should be added to full backward hooks as well.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1790\u001b[39m, in \u001b[36mModule._call_impl.<locals>.inner\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m   1788\u001b[39m     args = bw_hook.setup_input_hook(args)\n\u001b[32m-> \u001b[39m\u001b[32m1790\u001b[39m result = \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1791\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ocean/projects/cis220031p/psamal/IDL-HW4/IDL-HW4/hw4lib/model/speech_embedding.py:329\u001b[39m, in \u001b[36mSpeechEmbedding.forward\u001b[39m\u001b[34m(self, x, x_len)\u001b[39m\n\u001b[32m    328\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.blstm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m329\u001b[39m     x, x_len = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mblstm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_len\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    330\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m x, x_len\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1844\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1843\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1844\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1845\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m   1846\u001b[39m     \u001b[38;5;66;03m# run always called hooks if they have not already been run\u001b[39;00m\n\u001b[32m   1847\u001b[39m     \u001b[38;5;66;03m# For now only forward hooks have the always_call option but perhaps\u001b[39;00m\n\u001b[32m   1848\u001b[39m     \u001b[38;5;66;03m# this functionality should be added to full backward hooks as well.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1790\u001b[39m, in \u001b[36mModule._call_impl.<locals>.inner\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m   1788\u001b[39m     args = bw_hook.setup_input_hook(args)\n\u001b[32m-> \u001b[39m\u001b[32m1790\u001b[39m result = \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1791\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ocean/projects/cis220031p/psamal/IDL-HW4/IDL-HW4/hw4lib/model/speech_embedding.py:144\u001b[39m, in \u001b[36mStackedBLSTMEmbedding.forward\u001b[39m\u001b[34m(self, x, x_len)\u001b[39m\n\u001b[32m    143\u001b[39m \u001b[38;5;66;03m# First BLSTM\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m144\u001b[39m packed_input = \u001b[43mpack_padded_sequence\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_len\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_first\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menforce_sorted\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    145\u001b[39m packed_output, _ = \u001b[38;5;28mself\u001b[39m.blstm1(packed_input)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/torch/nn/utils/rnn.py:338\u001b[39m, in \u001b[36mpack_padded_sequence\u001b[39m\u001b[34m(input, lengths, batch_first, enforce_sorted)\u001b[39m\n\u001b[32m    336\u001b[39m     \u001b[38;5;28minput\u001b[39m = \u001b[38;5;28minput\u001b[39m.index_select(batch_dim, sorted_indices)\n\u001b[32m--> \u001b[39m\u001b[32m338\u001b[39m data, batch_sizes = \u001b[43m_VF\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_pack_padded_sequence\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlengths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_first\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    339\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m _packed_sequence_init(data, batch_sizes, sorted_indices, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[31mRuntimeError\u001b[39m: Length of all samples has to be greater than 0, but found an element in 'lengths' that is <= 0",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m trainer = \u001b[43mASRTrainer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mTokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mPuru-Test\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig_file\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mconfig.yaml\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ocean/projects/cis220031p/psamal/IDL-HW4/IDL-HW4/hw4lib/trainers/asr_trainer.py:18\u001b[39m, in \u001b[36mASRTrainer.__init__\u001b[39m\u001b[34m(self, model, tokenizer, config, run_name, config_file, device)\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, model, tokenizer, config, run_name, config_file, device=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     20\u001b[39m     \u001b[38;5;66;03m# TODO: Initialize CE loss\u001b[39;00m\n\u001b[32m     21\u001b[39m     \u001b[38;5;28mself\u001b[39m.ce_criterion = nn.CrossEntropyLoss(\n\u001b[32m     22\u001b[39m         ignore_index=\u001b[38;5;28mself\u001b[39m.tokenizer.pad_id,\n\u001b[32m     23\u001b[39m         label_smoothing=\u001b[38;5;28mself\u001b[39m.config[\u001b[33m'\u001b[39m\u001b[33mloss\u001b[39m\u001b[33m'\u001b[39m].get(\u001b[33m'\u001b[39m\u001b[33mlabel_smoothing\u001b[39m\u001b[33m'\u001b[39m, \u001b[32m0.0\u001b[39m)\n\u001b[32m     24\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ocean/projects/cis220031p/psamal/IDL-HW4/IDL-HW4/hw4lib/trainers/base_trainer.py:100\u001b[39m, in \u001b[36mBaseTrainer.__init__\u001b[39m\u001b[34m(self, model, tokenizer, config, run_name, config_file, device)\u001b[39m\n\u001b[32m     97\u001b[39m \u001b[38;5;28mself\u001b[39m.use_wandb = config[\u001b[33m'\u001b[39m\u001b[33mtraining\u001b[39m\u001b[33m'\u001b[39m].get(\u001b[33m'\u001b[39m\u001b[33muse_wandb\u001b[39m\u001b[33m'\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m     98\u001b[39m \u001b[38;5;66;03m# Initialize experiment directories\u001b[39;00m\n\u001b[32m     99\u001b[39m \u001b[38;5;28mself\u001b[39m.expt_root, \u001b[38;5;28mself\u001b[39m.checkpoint_dir, \u001b[38;5;28mself\u001b[39m.attn_dir, \u001b[38;5;28mself\u001b[39m.text_dir, \\\n\u001b[32m--> \u001b[39m\u001b[32m100\u001b[39m \u001b[38;5;28mself\u001b[39m.best_model_path, \u001b[38;5;28mself\u001b[39m.last_model_path = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_init_experiment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrun_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    102\u001b[39m \u001b[38;5;66;03m# Training state\u001b[39;00m\n\u001b[32m    103\u001b[39m \u001b[38;5;28mself\u001b[39m.current_epoch = \u001b[32m0\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ocean/projects/cis220031p/psamal/IDL-HW4/IDL-HW4/hw4lib/trainers/base_trainer.py:165\u001b[39m, in \u001b[36mBaseTrainer._init_experiment\u001b[39m\u001b[34m(self, run_name, config_file)\u001b[39m\n\u001b[32m    163\u001b[39m dtypes = [torch.float32, torch.long, torch.long, torch.long]\n\u001b[32m    164\u001b[39m \u001b[38;5;66;03m# Generate the summary\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m165\u001b[39m model_summary = \u001b[43msummary\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    166\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    167\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Adjust these dimensions based on your model's input\u001b[39;49;00m\n\u001b[32m    168\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtypes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtypes\u001b[49m\n\u001b[32m    169\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    170\u001b[39m \u001b[38;5;66;03m# Write the summary string to file\u001b[39;00m\n\u001b[32m    171\u001b[39m f.write(\u001b[38;5;28mstr\u001b[39m(model_summary))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/torchinfo/torchinfo.py:223\u001b[39m, in \u001b[36msummary\u001b[39m\u001b[34m(model, input_size, input_data, batch_dim, cache_forward_pass, col_names, col_width, depth, device, dtypes, mode, row_settings, verbose, **kwargs)\u001b[39m\n\u001b[32m    216\u001b[39m validate_user_params(\n\u001b[32m    217\u001b[39m     input_data, input_size, columns, col_width, device, dtypes, verbose\n\u001b[32m    218\u001b[39m )\n\u001b[32m    220\u001b[39m x, correct_input_size = process_input(\n\u001b[32m    221\u001b[39m     input_data, input_size, batch_dim, device, dtypes\n\u001b[32m    222\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m223\u001b[39m summary_list = \u001b[43mforward_pass\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    224\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_forward_pass\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    225\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    226\u001b[39m formatting = FormattingOptions(depth, verbose, columns, col_width, rows)\n\u001b[32m    227\u001b[39m results = ModelStatistics(\n\u001b[32m    228\u001b[39m     summary_list, correct_input_size, get_total_memory_used(x), formatting\n\u001b[32m    229\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/torchinfo/torchinfo.py:304\u001b[39m, in \u001b[36mforward_pass\u001b[39m\u001b[34m(model, x, batch_dim, cache_forward_pass, device, mode, **kwargs)\u001b[39m\n\u001b[32m    302\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    303\u001b[39m     executed_layers = [layer \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m summary_list \u001b[38;5;28;01mif\u001b[39;00m layer.executed]\n\u001b[32m--> \u001b[39m\u001b[32m304\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    305\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mFailed to run torchinfo. See above stack traces for more details. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    306\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mExecuted layers up to: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexecuted_layers\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    307\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    308\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    309\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m hooks:\n",
      "\u001b[31mRuntimeError\u001b[39m: Failed to run torchinfo. See above stack traces for more details. Executed layers up to: [Conv2DSubsampling: 2, Sequential: 3, Conv2d: 4, GELU: 4, Conv2d: 4, GELU: 4, Linear: 3, Dropout: 3]"
     ]
    }
   ],
   "source": [
    "trainer = ASRTrainer(\n",
    "    model=model,\n",
    "    tokenizer=Tokenizer,\n",
    "    config=config,\n",
    "    run_name=\"Puru-Test\",\n",
    "    config_file=\"config.yaml\",\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train\n",
    "- `TA TODO`: You can set your epochs here or in the config. If you set in config, make sure you remove the epoch argument here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train(train_loader, val_loader, epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"hw4p2_sol.json\", \"r\") as f:\n",
    "    solution = json.load(f)\n",
    "\n",
    "results = trainer.evaluate(test_loader, solution[:len(test_dataset)])\n",
    "\n",
    "for config_name, metrics in results.items():\n",
    "    print(\"-\"*50)\n",
    "    print(f\"Config: {config_name}\")\n",
    "    print(f\"WER: {metrics['wer']:.2f}%\")\n",
    "    print(f\"CER: {metrics['cer']:.2f}%\")\n",
    "    print(f\"Word Distance: {metrics['word_dist']:.2f}\")\n",
    "    print(\"-\"*50)\n",
    "\n",
    "# Cleanup\n",
    "trainer.cleanup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Progressive Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = ProgressiveTrainer(\n",
    "    model=model,\n",
    "    tokenizer=Tokenizer,\n",
    "    config=config,\n",
    "    run_name=\"Puru\",\n",
    "    config_file=\"config.yaml\",\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define your training stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Example with a model with 6 encoder and 6 decoder layers\n",
    "stages = [\n",
    "            {\n",
    "                'name': 'Initial (2 layers)',\n",
    "                'epochs': 5,\n",
    "                'encoder_active_layers': [0, 5],  # layers 1 and 6\n",
    "                'decoder_active_layers': [0, 5],  # layers 1 and 6\n",
    "                'time_reduction': 8,\n",
    "                'dropout': 0.0,\n",
    "                'label_smoothing': 0.0,\n",
    "                'data_subset': 0.25\n",
    "            },\n",
    "            {\n",
    "                'name': 'Enable dropout',\n",
    "                'epochs': 5,\n",
    "                'encoder_active_layers': [0, 5],\n",
    "                'decoder_active_layers': [0, 5],\n",
    "                'time_reduction': 8,\n",
    "                'dropout': 0.1,\n",
    "                'label_smoothing': 0.0,\n",
    "                'data_subset': 0.25\n",
    "            },\n",
    "            {\n",
    "                'name': '3 layers',\n",
    "                'epochs': 5,\n",
    "                'encoder_active_layers': [0, 1, 5],\n",
    "                'decoder_active_layers': [0, 1, 5],\n",
    "                'time_reduction': 8,\n",
    "                'dropout': 0.1,\n",
    "                'label_smoothing': 0.0,\n",
    "                'data_subset': 0.25\n",
    "            },\n",
    "            {\n",
    "                'name': '4 layers',\n",
    "                'epochs': 5,\n",
    "                'encoder_active_layers': [0, 1, 2, 5],\n",
    "                'decoder_active_layers': [0, 1, 2, 5],\n",
    "                'time_reduction': 8,\n",
    "                'dropout': 0.1,\n",
    "                'label_smoothing': 0.0,\n",
    "                'data_subset': 0.25\n",
    "            },\n",
    "            {\n",
    "                'name': '5 layers',\n",
    "                'epochs': 5,\n",
    "                'encoder_active_layers': [0, 1, 2, 3, 5],\n",
    "                'decoder_active_layers': [0, 1, 2, 3, 5],\n",
    "                'time_reduction': 8,\n",
    "                'dropout': 0.1,\n",
    "                'label_smoothing': 0.0,\n",
    "                'data_subset': 0.25\n",
    "            },\n",
    "            {\n",
    "                'name': 'All 6 layers',\n",
    "                'epochs': 5,\n",
    "                'encoder_active_layers': list(range(6)),\n",
    "                'decoder_active_layers': list(range(6)),\n",
    "                'time_reduction': 8,\n",
    "                'dropout': 0.1,\n",
    "                'label_smoothing': 0.0,\n",
    "                'data_subset': 0.25\n",
    "            },\n",
    "            {\n",
    "                'name': 'Reduce time stride',\n",
    "                'epochs': 5,\n",
    "                'encoder_active_layers': list(range(6)),\n",
    "                'decoder_active_layers': list(range(6)),\n",
    "                'time_reduction': 2,\n",
    "                'dropout': 0.1,\n",
    "                'label_smoothing': 0.0,\n",
    "                'data_subset': 0.25\n",
    "            },\n",
    "            {\n",
    "                'name': 'Final (with label smoothing)',\n",
    "                'epochs': 5,\n",
    "                'encoder_active_layers': list(range(6)),\n",
    "                'decoder_active_layers': list(range(6)),\n",
    "                'time_reduction': 2,\n",
    "                'dropout': 0.1,\n",
    "                'label_smoothing': 0.1,\n",
    "                'data_subset': 0.25\n",
    "            }\n",
    "        ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Progressive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.progressive_train(train_loader, val_loader, stages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Full\n",
    "- `TA TODO`: You can set your epochs here or in the config. If you set in config, make sure you remove the epoch argument here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train(train_loader, val_loader, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"hw4p2_sol.json\", \"r\") as f:\n",
    "    solution = json.load(f)\n",
    "\n",
    "results = trainer.evaluate(test_loader, solution[:len(test_dataset)])\n",
    "\n",
    "for config_name, metrics in results.items():\n",
    "    print(\"-\"*50)\n",
    "    print(f\"Config: {config_name}\")\n",
    "    print(f\"WER: {metrics['wer']:.2f}%\")\n",
    "    print(f\"CER: {metrics['cer']:.2f}%\")\n",
    "    print(f\"Word Distance: {metrics['word_dist']:.2f}\")\n",
    "    print(\"-\"*50)\n",
    "\n",
    "# Cleanup\n",
    "trainer.cleanup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus: Shallow Fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
