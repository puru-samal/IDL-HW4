{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Colab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Get Repo (TA-Only, will be handout for students)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GITHUB_USERNAME = \"puru-samal\"\n",
    "REPO_NAME = \"IDL-HW4\"\n",
    "ACCESS_TOKEN = \"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\"\n",
    "repo_url = f\"https://{GITHUB_USERNAME}:{ACCESS_TOKEN}@github.com/{GITHUB_USERNAME}/{REPO_NAME}.git\"\n",
    "!git clone {repo_url}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python download_data.py\n",
    "!tar -xf hw4_data.tar.gz\n",
    "!rm -rf hw4_data.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --no-deps -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Move to Project Directory\n",
    "- `NOTE`: You may have to repeat this on restarting your runtime. You can do a `pwd` to check if you are in the right directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('IDL-HW4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PSC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "from hw4lib.data import (\n",
    "    H4Tokenizer,\n",
    "    ASRDataset,\n",
    "    verify_dataloader\n",
    ")\n",
    "from hw4lib.model import (\n",
    "    EncoderDecoderTransformer\n",
    ")\n",
    "from hw4lib.trainers import (\n",
    "    ASRTrainer\n",
    ")\n",
    "from torch.utils.data import DataLoader\n",
    "import yaml\n",
    "import gc\n",
    "import torch\n",
    "from torchinfo import summary\n",
    "import os\n",
    "import json\n",
    "import tarfile\n",
    "import shutil\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementations\n",
    "- `NOTE`: All of these implementations have detailed specification, implementation details, and hints in their respective source files. Make sure to read all of them in their entirety to understand the implementation details!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Implementation\n",
    "- `TODO`: Implement the `ASRDataset` class in `hw4lib/data/asr_dataset.py`. \n",
    "- You will have to implement parts of `__init__` and completely implement the `__len__`, `__getitem__` and `collate_fn` methods. \n",
    "- `TODO`: Then run the cell below to check your implementation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data for train-clean-100 partition...\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28/28 [00:00<00:00, 582.63it/s]\n",
      "Loading data for test-clean partition...\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 1616.93it/s]\n",
      "\n",
      "\u001b[95m================================================================================\n",
      "Running tests for category: ASRDataset Train\n",
      "--------------------------------------------------------------------------------\u001b[0m\n",
      "\n",
      "\u001b[94m[01/01]    Running:  Test a Train instance of ASRDataset class\u001b[0m\n",
      "Testing __init__ method ...\n",
      "Test Passed: Dataset length matches FBANK files.\n",
      "Test Passed: Dataset length matches TRANSCRIPT files.\n",
      "Test Passed: Order alignment between FBANK files and TRANSCRIPT files is correct.\n",
      "Test Passed: Alignment between features and transcripts is correct.\n",
      "Test Passed: All features have the correct number of dimensions (num_feats).\n",
      "Test Passed: All transcripts are decoded correctly after removing SOS and EOS tokens.\n",
      "Testing __getitem__ method ...\n",
      "Test Passed: All samples have correct feature dimensions and transcript alignment.\n",
      "Testing collate_fn method ...\n",
      "Test Passed: Feature batch has correct dimensions (3D tensor).\n",
      "Test Passed: All sequences are padded to the same length.\n",
      "Test Passed: All transcripts are padded to the same length.\n",
      "Test Passed: Padding values are correct.\n",
      "\u001b[92m[01/01]    PASSED:   Test a Train instance of ASRDataset class\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[95m================================================================================\n",
      "Running tests for category: ASRDataset Test\n",
      "--------------------------------------------------------------------------------\u001b[0m\n",
      "\n",
      "\u001b[94m[01/01]    Running:  Test a Test instance of ASRDataset class\u001b[0m\n",
      "Testing __init__ method ...\n",
      "Test Passed: Dataset length matches FBANK files.\n",
      "Test Passed: All features have the correct number of dimensions (num_feats).\n",
      "Testing __getitem__ method ...\n",
      "Test Passed: Transcripts are None for 'test-clean' at index 0.\n",
      "Test Passed: Transcripts are None for 'test-clean' at index 1.\n",
      "Test Passed: All samples have correct feature dimensions and transcript alignment.\n",
      "Testing collate_fn method ...\n",
      "Test Passed: Feature batch has correct dimensions (3D tensor).\n",
      "Test Passed: All sequences are padded to the same length.\n",
      "Test Passed: Transcripts and lengths are correctly set to None for 'test-clean'.\n",
      "\u001b[92m[01/01]    PASSED:   Test a Test instance of ASRDataset class\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[95m================================================================================\n",
      "                                  Test Summary                                  \n",
      "================================================================================\u001b[0m\n",
      "\u001b[93mCategory:    ASRDataset Train              \n",
      "Results:     1/1 tests passed (100.0%)\u001b[0m\n",
      "\u001b[93mCategory:    ASRDataset Test               \n",
      "Results:     1/1 tests passed (100.0%)\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python -m tests.test_dataset_asr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Implementations\n",
    "\n",
    "- `TODO`: Implement the `CrossAttentionLayer` class in `hw4lib/model/sublayers.py`.\n",
    "- `TODO`: Implement the `CrossAttentionDecoderLayer` class in `hw4lib/model/decoder_layers.py`.\n",
    "- `TODO`: Implement the `SelfAttentionEncoderLayer` class in `hw4lib/model/encoder_layers.py`. This will be mostly a copy-paste of the `SelfAttentionDecoderLayer` class in `hw4lib/model/decoder_layers.py` with one minor diffrence: it can attend to all positions in the input sequence.\n",
    "- `TODO`: Implement the `EncoderDecoderTransformer` class in `hw4lib/model/transformers.py`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer Sublayers\n",
    "- `TODO`: Now, Implement the `CrossAttentionLayer` class in `hw4lib/model/sublayers.py`.\n",
    "- `NOTE`: You should have already implemented the `SelfAttentionLayer`, and `FeedForwardLayer` classes in `hw4lib/model/sublayers.py`.\n",
    "- `TODO`: Then run the cell below to check your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[95m================================================================================\n",
      "Running tests for category: CrossAttentionLayer\n",
      "--------------------------------------------------------------------------------\u001b[0m\n",
      "\n",
      "\u001b[94m[01/01]    Running:  Test the cross-attention sublayer\u001b[0m\n",
      "Testing initialization ...\n",
      "Test Passed: All layers exist and are instantiated correctly\n",
      "Testing forward shapes ...\n",
      "Test Passed: Forward pass returns the correct shapes\n",
      "Testing padding mask behaviour ...\n",
      "Test Passed: Padding mask is applied correctly\n",
      "Testing cross-attention behaviour ...\n",
      "Test Passed: Cross-attention behavior is correct\n",
      "Testing residual connection ...\n",
      "Test Passed: Residual connection is applied correctly\n",
      "\u001b[92m[01/01]    PASSED:   Test the cross-attention sublayer\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[95m================================================================================\n",
      "                                  Test Summary                                  \n",
      "================================================================================\u001b[0m\n",
      "\u001b[93mCategory:    CrossAttentionLayer           \n",
      "Results:     1/1 tests passed (100.0%)\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python -m tests.test_sublayer_crossattention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer Cross-Attention Decoder Layer\n",
    "- `TODO`: Implement the `CrossAttentionDecoderLayer` class in `hw4lib/model/decoder_layers.py`.\n",
    "- `TODO`: Then run the cell below to check your implementation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[95m================================================================================\n",
      "Running tests for category: CrossAttentionDecoderLayer\n",
      "--------------------------------------------------------------------------------\u001b[0m\n",
      "\n",
      "\u001b[94m[01/01]    Running:  Test the cross-attention decoder layer\u001b[0m\n",
      "Testing initialization ...\n",
      "Test Passed: All sublayers exist and are initialized correctly\n",
      "Testing forward shapes ...\n",
      "Test Passed: Forward shapes are as expected\n",
      "Testing sublayer integration ...\n",
      "Test Passed: Sublayers interact correctly\n",
      "Testing cross-attention behavior ...\n",
      "Test Passed: Cross-attention behaves correctly\n",
      "\u001b[92m[01/01]    PASSED:   Test the cross-attention decoder layer\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[95m================================================================================\n",
      "                                  Test Summary                                  \n",
      "================================================================================\u001b[0m\n",
      "\u001b[93mCategory:    CrossAttentionDecoderLayer    \n",
      "Results:     1/1 tests passed (100.0%)\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python -m tests.test_decoderlayer_crossattention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer Self-Attention Encoder Layer\n",
    "- `TODO`: Implement the `SelfAttentionEncoderLayer` class in `hw4lib/model/encoder_layers.py`.\n",
    "- `TODO`: Then run the cell below to check your implementation.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[95m================================================================================\n",
      "Running tests for category: SelfAttentionEncoderLayer\n",
      "--------------------------------------------------------------------------------\u001b[0m\n",
      "\n",
      "\u001b[94m[01/01]    Running:  Test the self-attention encoder layer\u001b[0m\n",
      "Testing initialization ...\n",
      "Test Passed: All sublayers exist and are initialized correctly\n",
      "Testing forward shapes ...\n",
      "Test Passed: Forward shapes are as expected\n",
      "Testing sublayer interaction ...\n",
      "Test Passed: Sublayers interact correctly\n",
      "Testing bidirectional attention ...\n",
      "Test Passed: Bidirectional attention is working correctly\n",
      "\u001b[92m[01/01]    PASSED:   Test the self-attention encoder layer\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[95m================================================================================\n",
      "                                  Test Summary                                  \n",
      "================================================================================\u001b[0m\n",
      "\u001b[93mCategory:    SelfAttentionEncoderLayer     \n",
      "Results:     1/1 tests passed (100.0%)\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python -m tests.test_encoderlayer_selfattention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder-Decoder Transformer\n",
    "\n",
    "- `TODO`: Implement the  `EncoderDecoderTransformer` class in `hw4lib/model/transformers.py`.\n",
    "- `TODO`: Then run the cell below to check your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[95m================================================================================\n",
      "Running tests for category: EncoderDecoderTransformer\n",
      "--------------------------------------------------------------------------------\u001b[0m\n",
      "\n",
      "\u001b[94m[01/01]    Running:  Test the encoder-decoder transformer\u001b[0m\n",
      "Testing initialization...\n",
      "Test Passed: All components initialized correctly\n",
      "Testing encode method...\n",
      "Test Passed: Encode method works correctly\n",
      "Testing decode method...\n",
      "Test Passed: Decode method works correctly\n",
      "Testing forward pass...\n",
      "Test Passed: Forward pass works correctly\n",
      "Testing encoder-decoder integration...\n",
      "Test Passed: Encoder-decoder integration works correctly\n",
      "Testing CTC integration...\n",
      "Test Passed: CTC integration works correctly\n",
      "Testing forward propagation order...\n",
      "Test Passed: Forward propagation order is correct\n",
      "\u001b[92m[01/01]    PASSED:   Test the encoder-decoder transformer\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[95m================================================================================\n",
      "                                  Test Summary                                  \n",
      "================================================================================\u001b[0m\n",
      "\u001b[93mCategory:    EncoderDecoderTransformer     \n",
      "Results:     1/1 tests passed (100.0%)\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python -m tests.test_transformer_encoder_decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoding Implementations \n",
    "- `TODO`: We highly recommend you to implement the `generate_beam` method of the `SequenceGenerator` class in `hw4lib/decoding/sequence_generator.py`.\n",
    "- `TODO`: Then run the cell below to check your implementation.\n",
    "- `NOTE`: This is an optional but highly recommended task for `HW4P2` to ease the journey to high cutoffs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[95m================================================================================\n",
      "Running tests for category: Decoding\n",
      "--------------------------------------------------------------------------------\u001b[0m\n",
      "\n",
      "\u001b[94m[01/01]    Running:  Test beam decoding\u001b[0m\n",
      "Testing Single Batch Beam Search ...\n",
      "Beam 0  : generated: HELLO WORLD  | expected: HELLO WORLD \n",
      "Beam 1  : generated: YELLOW WORLD | expected: YELLOW WORLD\n",
      "Beam 2  : generated: MELLOW WORLD | expected: MELLOW WORLD\n",
      "Testing Multi Batch Beam Search ...\n",
      "Batch 0  : Beam 0  : generated: HELLO WORLD  | expected: HELLO WORLD \n",
      "Batch 0  : Beam 1  : generated: YELLOW WORLD | expected: YELLOW WORLD\n",
      "Batch 0  : Beam 2  : generated: MELLOW WORLD | expected: MELLOW WORLD\n",
      "Batch 1  : Beam 0  : generated: GOOD BYE     | expected: GOOD BYE    \n",
      "Batch 1  : Beam 1  : generated: GREAT DAY    | expected: GREAT DAY   \n",
      "Batch 1  : Beam 2  : generated: GUD NIGHT    | expected: GUD NIGHT   \n",
      "\u001b[92m[01/01]    PASSED:   Test beam decoding\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[95m================================================================================\n",
      "                                  Test Summary                                  \n",
      "================================================================================\u001b[0m\n",
      "\u001b[93mCategory:    Decoding                      \n",
      "Results:     1/1 tests passed (100.0%)\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python -m tests.test_decoding --mode beam  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config \n",
    "\n",
    "- Set your config for your ablation study. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting config.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile config.yaml\n",
    "\n",
    "Name                      : \"Puru\"\n",
    "\n",
    "###### Tokenization ------------------------------------------------------------\n",
    "tokenization:\n",
    "  token_type                : \"1k\"       # [char, 1k, 5k, 10k]\n",
    "  token_map :\n",
    "      'char': 'hw4lib/data/tokenizer_jsons/tokenizer_char.json'\n",
    "      '1k'  : 'hw4lib/data/tokenizer_jsons/tokenizer_1000.json'\n",
    "      '5k'  : 'hw4lib/data/tokenizer_jsons/tokenizer_5000.json'\n",
    "      '10k' : 'hw4lib/data/tokenizer_jsons/tokenizer_10000.json'\n",
    "\n",
    "###### Dataset -----------------------------------------------------------------\n",
    "data:\n",
    "  root                 : \"hw4_data_subset/hw4p2_data\"  # TODO: Set the root path of your data\n",
    "  train_partition      : \"train-clean-100\"  # paired text-speech for ASR pre-training\n",
    "  val_partition        : \"dev-clean\"    # paired text-speech for ASR pre-training\n",
    "  test_partition       : \"test-clean\"   # paired text-speech for ASR pre-trainin\n",
    "  subset               : 1.0          # Load a subset of the data (for debugging, testing, etc\n",
    "  batch_size           : 8            # Set to 0 for CPU  \n",
    "  NUM_WORKERS          : 2            # Set to 0 for CPU\n",
    "  norm                 : 'global_mvn' # ['global_mvn', 'cepstral', 'none']\n",
    "  num_feats            : 80\n",
    "\n",
    "  ###### SpecAugment ---------------------------------------------------------------\n",
    "  specaug                   : True\n",
    "  specaug_conf:\n",
    "    apply_freq_mask         : True\n",
    "    freq_mask_width_range   : 4\n",
    "    num_freq_mask           : 2\n",
    "    apply_time_mask         : True\n",
    "    time_mask_width_range   : 10\n",
    "    num_time_mask           : 4\n",
    "\n",
    "###### Network Specs -------------------------------------------------------------\n",
    "model: # Encoder-Decoder Transformer (HW4P2)\n",
    "  # Speech embedding parameters\n",
    "  input_dim: 80              # Speech feature dimension\n",
    "  time_reduction: 2          # Time dimension downsampling factor\n",
    "  reduction_method: 'lstm'   # The source_embedding reduction method ['lstm', 'conv', 'both']\n",
    "  \n",
    "  # Architecture parameters\n",
    "  d_model: 256            # Model dimension\n",
    "  num_encoder_layers: 6  # Number of encoder layers\n",
    "  num_decoder_layers: 6  # Number of decoder layers\n",
    "  num_encoder_heads: 1   # Number of encoder attention heads\n",
    "  num_decoder_heads: 4   # Number of decoder attention heads\n",
    "  d_ff_encoder: 256      # Feed-forward dimension for encoder\n",
    "  d_ff_decoder: 1024     # Feed-forward dimension for decoder\n",
    "  skip_encoder_pe: False # Whether to skip positional encoding for encoder\n",
    "  skip_decoder_pe: False # Whether to skip positional encoding for decoder\n",
    "  \n",
    "  # Common parameters\n",
    "  dropout: 0.1          # Dropout rate\n",
    "  layer_drop_rate: 0.1  # Layer dropout rate\n",
    "  weight_tying: False   # Whether to use weight tying\n",
    "  \n",
    "  # Training specific\n",
    "  decoder_checkpoint: \"expts/Puru/checkpoints/checkpoint-best-metric-model.pth\"  # Path to pretrained decoder checkpoint\n",
    "  freeze_transferred: True  # Whether to freeze transferred decoder parameters\n",
    "  decoder_lr_factor: 0.1    # Learning rate factor for transferred parameters\n",
    "\n",
    "###### Common Training Parameters ------------------------------------------------\n",
    "training:\n",
    "  use_wandb                   : False\n",
    "  wandb_run_id                : \"none\" # \"none\" or \"run_id\"\n",
    "  resume                      : False\n",
    "  epochs                      : 5\n",
    "  gradient_accumulation_steps : 1\n",
    "\n",
    "###### Loss ----------------------------------------------------------------------\n",
    "loss: # Just good ol' CrossEntropy\n",
    "  label_smoothing: 0.0\n",
    "  ctc_weight: 0.3\n",
    "\n",
    "###### Optimizer -----------------------------------------------------------------\n",
    "optimizer:\n",
    "  name: \"adam\" # Options: sgd, adam, adamw\n",
    "  lr: 0.0001  # Base learning rate\n",
    "\n",
    "  # Common parameters\n",
    "  weight_decay: 0.0001\n",
    "\n",
    "  # Parameter groups\n",
    "  param_groups:\n",
    "    - name: self_attn\n",
    "      patterns: [\"self_attn\"]  # Will match all parameters containing \"encoder\"\n",
    "      lr: 0.0001  # LR for self_attn\n",
    "      layer_decay:\n",
    "        enabled: False\n",
    "        decay_rate: 0.8\n",
    "    \n",
    "    - name: ffn\n",
    "      patterns: [\"ffn\"]\n",
    "      lr: 0.0001  # LR for ffn\n",
    "      layer_decay:\n",
    "        enabled: False\n",
    "        decay_rate: 0.8\n",
    "  \n",
    "  # Layer-wise learning rates\n",
    "  layer_decay:\n",
    "    enabled: False\n",
    "    decay_rate: 0.75\n",
    "\n",
    "  # SGD specific parameters\n",
    "  sgd:\n",
    "    momentum: 0.9\n",
    "    nesterov: True\n",
    "    dampening: 0\n",
    "\n",
    "  # Adam specific parameters\n",
    "  adam:\n",
    "    betas: [0.9, 0.999]\n",
    "    eps: 1.0e-8\n",
    "    amsgrad: False\n",
    "\n",
    "  # AdamW specific parameters\n",
    "  adamw:\n",
    "    betas: [0.9, 0.999]\n",
    "    eps: 1.0e-8\n",
    "    amsgrad: False\n",
    "\n",
    "###### Scheduler -----------------------------------------------------------------\n",
    "scheduler:\n",
    "  name: \"cosine_warm\"  # Options: reduce_lr, cosine, cosine_warm\n",
    "\n",
    "  # ReduceLROnPlateau specific parameters\n",
    "  reduce_lr:\n",
    "    mode: \"min\"  # Options: min, max\n",
    "    factor: 0.1  # Factor to reduce learning rate by\n",
    "    patience: 10  # Number of epochs with no improvement after which LR will be reduced\n",
    "    threshold: 0.0001  # Threshold for measuring the new optimum\n",
    "    threshold_mode: \"rel\"  # Options: rel, abs\n",
    "    cooldown: 0  # Number of epochs to wait before resuming normal operation\n",
    "    min_lr: 0.0000001  # Minimum learning rate\n",
    "    eps: 1e-8  # Minimal decay applied to lr\n",
    "\n",
    "  # CosineAnnealingLR specific parameters\n",
    "  cosine:\n",
    "    T_max: 35  # Maximum number of iterations\n",
    "    eta_min: 0.0000001  # Minimum learning rate\n",
    "    last_epoch: -1\n",
    "\n",
    "  # CosineAnnealingWarmRestarts specific parameters\n",
    "  cosine_warm:\n",
    "    T_0: 4  # Number of iterations for the first restart\n",
    "    T_mult: 4  # Factor increasing T_i after each restart\n",
    "    eta_min: 0.0000001  # Minimum learning rate\n",
    "    last_epoch: -1\n",
    "\n",
    "  # Warmup parameters (can be used with any scheduler)\n",
    "  warmup:\n",
    "    enabled: False\n",
    "    type: \"exponential\"  # Options: linear, exponential\n",
    "    epochs: 5\n",
    "    start_factor: 0.1\n",
    "    end_factor: 1.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('config.yaml', 'r') as file:\n",
    "    config = yaml.safe_load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "                          Tokenizer Configuration (1k)                          \n",
      "--------------------------------------------------------------------------------\n",
      "Vocabulary size:     1000\n",
      "\n",
      "Special Tokens:\n",
      "PAD:              0\n",
      "UNK:              1\n",
      "MASK:             2\n",
      "SOS:              3\n",
      "EOS:              4\n",
      "BLANK:            5\n",
      "\n",
      "Validation Example:\n",
      "--------------------------------------------------------------------------------\n",
      "Input text:  [SOS]HI DEEP LEARNERS[EOS]\n",
      "Tokens:      ['[SOS]', 'H', 'I', 'Ä DE', 'EP', 'Ä LE', 'AR', 'N', 'ERS', '[EOS]']\n",
      "Token IDs:   [3, 14, 15, 159, 290, 228, 71, 20, 214, 4]\n",
      "Decoded:     [SOS]HI DEEP LEARNERS[EOS]\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "Tokenizer = H4Tokenizer(\n",
    "    token_map  = config['tokenization']['token_map'], \n",
    "    token_type = config['tokenization']['token_type']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data for train-clean-100 partition...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28/28 [00:00<00:00, 394.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global stats computed from training set.\n",
      "Loading data for dev-clean partition...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 813.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data for test-clean partition...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 1154.66it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "159"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = ASRDataset(\n",
    "    partition=config['data']['train_partition'],\n",
    "    config=config['data'],\n",
    "    tokenizer=Tokenizer,\n",
    "    isTrainPartition=True,\n",
    "    global_stats=None  # Will compute stats from training data\n",
    ")\n",
    "\n",
    "# TODO: Get the computed global stats from training set\n",
    "global_stats = None\n",
    "if config['data']['norm'] == 'global_mvn':\n",
    "    global_stats = (train_dataset.global_mean, train_dataset.global_std)\n",
    "    print(f\"Global stats computed from training set.\")\n",
    "\n",
    "val_dataset = ASRDataset(\n",
    "    partition=config['data']['val_partition'],\n",
    "    config=config['data'],\n",
    "    tokenizer=Tokenizer,\n",
    "    isTrainPartition=False,\n",
    "    global_stats=global_stats\n",
    ")\n",
    "\n",
    "test_dataset = ASRDataset(\n",
    "    partition=config['data']['test_partition'],\n",
    "    config=config['data'],\n",
    "    tokenizer=Tokenizer,\n",
    "    isTrainPartition=False,\n",
    "    global_stats=global_stats\n",
    ")\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader    = DataLoader(\n",
    "    dataset     = train_dataset,\n",
    "    batch_size  = config['data']['batch_size'],\n",
    "    shuffle     = True,\n",
    "    num_workers = config['data']['NUM_WORKERS'] if device == 'cuda' else 0,\n",
    "    pin_memory  = True,\n",
    "    collate_fn  = train_dataset.collate_fn   \n",
    ")\n",
    "\n",
    "val_loader      = DataLoader(\n",
    "    dataset     = val_dataset,\n",
    "    batch_size  = config['data']['batch_size'],\n",
    "    shuffle     = False,\n",
    "    num_workers = config['data']['NUM_WORKERS'] if device == 'cuda' else 0,\n",
    "    pin_memory  = True,\n",
    "    collate_fn  = val_dataset.collate_fn   \n",
    ")\n",
    "\n",
    "test_loader     = DataLoader(\n",
    "    dataset     = test_dataset,\n",
    "    batch_size  = config['data']['batch_size'],\n",
    "    shuffle     = False,\n",
    "    num_workers = config['data']['NUM_WORKERS'] if device == 'cuda' else 0,\n",
    "    pin_memory  = True,\n",
    "    collate_fn  = test_dataset.collate_fn   \n",
    ")\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataloader Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "             Dataloader Verification              \n",
      "==================================================\n",
      "Dataloader Partition     : train-clean-100\n",
      "--------------------------------------------------\n",
      "Number of Batches        : 4\n",
      "Batch Size               : 8\n",
      "--------------------------------------------------\n",
      "Checking shapes of the data...                    \n",
      "\n",
      "Feature Shape            : [8, 1959, 80]\n",
      "Shifted Transcript Shape : [8, 94]\n",
      "Golden Transcript Shape  : [8, 94]\n",
      "Feature Lengths Shape    : [8]\n",
      "Transcript Lengths Shape : [8]\n",
      "--------------------------------------------------\n",
      "Max Feature Length       : 2047\n",
      "Max Transcript Length    : 101\n",
      "Avg. Chars per Token     : 3.11\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "verify_dataloader(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "             Dataloader Verification              \n",
      "==================================================\n",
      "Dataloader Partition     : dev-clean\n",
      "--------------------------------------------------\n",
      "Number of Batches        : 1\n",
      "Batch Size               : 8\n",
      "--------------------------------------------------\n",
      "Checking shapes of the data...                    \n",
      "\n",
      "Feature Shape            : [2, 732, 80]\n",
      "Shifted Transcript Shape : [2, 37]\n",
      "Golden Transcript Shape  : [2, 37]\n",
      "Feature Lengths Shape    : [2]\n",
      "Transcript Lengths Shape : [2]\n",
      "--------------------------------------------------\n",
      "Max Feature Length       : 732\n",
      "Max Transcript Length    : 37\n",
      "Avg. Chars per Token     : 2.76\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "verify_dataloader(val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "             Dataloader Verification              \n",
      "==================================================\n",
      "Dataloader Partition     : test-clean\n",
      "--------------------------------------------------\n",
      "Number of Batches        : 1\n",
      "Batch Size               : 8\n",
      "--------------------------------------------------\n",
      "Checking shapes of the data...                    \n",
      "\n",
      "Feature Shape            : [2, 1305, 80]\n",
      "Feature Lengths Shape    : [2]\n",
      "--------------------------------------------------\n",
      "Max Feature Length       : 1305\n",
      "Max Transcript Length    : 0\n",
      "Avg. Chars per Token     : 0.00\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "verify_dataloader(test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Max Lengths\n",
    "Calculating the maximum transcript length across your dataset is a crucial step when working with certain transformer models. \n",
    "-  We'll use sinusoidal positional encodings that must be precomputed up to a fixed maximum length.\n",
    "- This maximum length is a hyperparameter that determines:\n",
    "  - How long of a sequence your model can process\n",
    "  - The size of your positional encoding matrix\n",
    "  - Memory requirements during training and inference\n",
    "- `Requirements`: For this assignment, ensure your positional encodings can accommodate at least the longest sequence in your dataset to prevent truncation. However, you can set this value higher if you anticipate using your languagemodel to work with longer sequences in future tasks (hint: this might be useful for P2! ðŸ˜‰).\n",
    "- `NOTE`: We'll be using the same positional encoding matrix for all sequences in your dataset. Take this into account when setting your maximum length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Max Feature Length             : 2047\n",
      "Max Transcript Length          : 101\n",
      "Overall Max Length             : 2047\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "max_feat_len       = max(train_dataset.feat_max_len, val_dataset.feat_max_len, test_dataset.feat_max_len)\n",
    "max_transcript_len = max(train_dataset.text_max_len, val_dataset.text_max_len, test_dataset.text_max_len)\n",
    "max_len            = max(max_feat_len, max_transcript_len)\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(f\"{'Max Feature Length':<30} : {max_feat_len}\")\n",
    "print(f\"{'Max Transcript Length':<30} : {max_transcript_len}\")\n",
    "print(f\"{'Overall Max Length':<30} : {max_len}\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Initializing Encoder-Decoder from Pretrained Decoder ===\n",
      "Loading checkpoint from: expts/Puru/checkpoints/checkpoint-best-metric-model.pth\n",
      "\n",
      "Creating new encoder-decoder model...\n",
      "Loading pretrained decoder weights...\n",
      "\n",
      "Transferring shared components:\n",
      "  - Transferring target_embedding. (1 parameters)\n",
      "    â†’ Parameters frozen\n",
      "  - Transferring final_linear. (2 parameters)\n",
      "    â†’ Parameters frozen\n",
      "  - Transferring norm. (2 parameters)\n",
      "    â†’ Parameters frozen\n",
      "\n",
      "Transferring decoder layers (found 6 layers):\n",
      "\n",
      "Layer 1/6:\n",
      "  - Transferring dec_layers.0.self_attn. (6 parameters)\n",
      "    â†’ Parameters frozen\n",
      "  - Transferring dec_layers.0.ffn. (6 parameters)\n",
      "    â†’ Parameters frozen\n",
      "\n",
      "Layer 2/6:\n",
      "  - Transferring dec_layers.1.self_attn. (6 parameters)\n",
      "    â†’ Parameters frozen\n",
      "  - Transferring dec_layers.1.ffn. (6 parameters)\n",
      "    â†’ Parameters frozen\n",
      "\n",
      "Layer 3/6:\n",
      "  - Transferring dec_layers.2.self_attn. (6 parameters)\n",
      "    â†’ Parameters frozen\n",
      "  - Transferring dec_layers.2.ffn. (6 parameters)\n",
      "    â†’ Parameters frozen\n",
      "\n",
      "Layer 4/6:\n",
      "  - Transferring dec_layers.3.self_attn. (6 parameters)\n",
      "    â†’ Parameters frozen\n",
      "  - Transferring dec_layers.3.ffn. (6 parameters)\n",
      "    â†’ Parameters frozen\n",
      "\n",
      "Layer 5/6:\n",
      "  - Transferring dec_layers.4.self_attn. (6 parameters)\n",
      "    â†’ Parameters frozen\n",
      "  - Transferring dec_layers.4.ffn. (6 parameters)\n",
      "    â†’ Parameters frozen\n",
      "\n",
      "Layer 6/6:\n",
      "  - Transferring dec_layers.5.self_attn. (6 parameters)\n",
      "    â†’ Parameters frozen\n",
      "  - Transferring dec_layers.5.ffn. (6 parameters)\n",
      "    â†’ Parameters frozen\n",
      "\n",
      "Collecting new parameters...\n",
      "\n",
      "Creating parameter groups:\n",
      "  - New parameters group: 130 parameters\n",
      "\n",
      "=== Initialization Complete ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/puruboii/Desktop/11785-TA/s25/HW4/hw4lib/model/transformers.py:488: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(decoder_checkpoint_path, map_location='cpu')\n"
     ]
    }
   ],
   "source": [
    "model_config = config['model'].copy()\n",
    "model_config.pop('decoder_checkpoint')\n",
    "model_config.pop('freeze_transferred')\n",
    "model_config.pop('decoder_lr_factor')\n",
    "model_config.update({\n",
    "    'max_len': max_len,\n",
    "    'num_classes': Tokenizer.vocab_size\n",
    "})\n",
    "\n",
    "model = EncoderDecoderTransformer.from_pretrained_decoder(\n",
    "    decoder_checkpoint_path=config['model']['decoder_checkpoint'],\n",
    "    config=model_config,\n",
    "    freeze_transferred=config['model']['freeze_transferred'],\n",
    "    decoder_lr_factor=config['model']['decoder_lr_factor']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================================================================================\n",
      "Layer (type:depth-idx)                        Output Shape              Param #\n",
      "===============================================================================================\n",
      "EncoderDecoderTransformer                     [8, 94, 1000]             --\n",
      "â”œâ”€SpeechEmbedding: 1-1                        [8, 1023, 64]             --\n",
      "â”‚    â””â”€StackedBLSTMEmbedding: 2-1             [8, 1023, 64]             --\n",
      "â”‚    â”‚    â””â”€LSTM: 3-1                         [14165, 64]               29,184\n",
      "â”‚    â”‚    â””â”€MaxPool1d: 3-2                    [8, 64, 1023]             --\n",
      "â”‚    â”‚    â””â”€LSTM: 3-3                         [7080, 64]                25,088\n",
      "â”‚    â”‚    â””â”€MaxPool1d: 3-4                    [8, 64, 1023]             --\n",
      "â”‚    â”‚    â””â”€Linear: 3-5                       [8, 1023, 64]             4,160\n",
      "â”‚    â”‚    â””â”€Dropout: 3-6                      [8, 1023, 64]             --\n",
      "â”œâ”€PositionalEncoding: 1-2                     [8, 1023, 64]             --\n",
      "â”œâ”€Dropout: 1-3                                [8, 1023, 64]             --\n",
      "â”œâ”€ModuleList: 1-4                             --                        --\n",
      "â”‚    â””â”€SelfAttentionEncoderLayer: 2-2         [8, 1023, 64]             --\n",
      "â”‚    â”‚    â””â”€SelfAttentionLayer: 3-7           [8, 1023, 64]             16,768\n",
      "â”‚    â”‚    â””â”€FeedForwardLayer: 3-8             [8, 1023, 64]             33,216\n",
      "â”‚    â””â”€SelfAttentionEncoderLayer: 2-3         [8, 1023, 64]             --\n",
      "â”‚    â”‚    â””â”€SelfAttentionLayer: 3-9           [8, 1023, 64]             16,768\n",
      "â”‚    â”‚    â””â”€FeedForwardLayer: 3-10            [8, 1023, 64]             33,216\n",
      "â”‚    â””â”€SelfAttentionEncoderLayer: 2-4         [8, 1023, 64]             --\n",
      "â”‚    â”‚    â””â”€SelfAttentionLayer: 3-11          [8, 1023, 64]             16,768\n",
      "â”‚    â”‚    â””â”€FeedForwardLayer: 3-12            [8, 1023, 64]             33,216\n",
      "â”‚    â””â”€SelfAttentionEncoderLayer: 2-5         [8, 1023, 64]             --\n",
      "â”‚    â”‚    â””â”€SelfAttentionLayer: 3-13          [8, 1023, 64]             16,768\n",
      "â”‚    â”‚    â””â”€FeedForwardLayer: 3-14            [8, 1023, 64]             33,216\n",
      "â”‚    â””â”€SelfAttentionEncoderLayer: 2-6         [8, 1023, 64]             --\n",
      "â”‚    â”‚    â””â”€SelfAttentionLayer: 3-15          [8, 1023, 64]             16,768\n",
      "â”‚    â”‚    â””â”€FeedForwardLayer: 3-16            [8, 1023, 64]             33,216\n",
      "â”‚    â””â”€SelfAttentionEncoderLayer: 2-7         [8, 1023, 64]             --\n",
      "â”‚    â”‚    â””â”€SelfAttentionLayer: 3-17          [8, 1023, 64]             16,768\n",
      "â”‚    â”‚    â””â”€FeedForwardLayer: 3-18            [8, 1023, 64]             33,216\n",
      "â”œâ”€LayerNorm: 1-5                              [8, 1023, 64]             128\n",
      "â”œâ”€Sequential: 1-6                             [1023, 8, 1000]           --\n",
      "â”‚    â””â”€Linear: 2-8                            [1023, 8, 1000]           65,000\n",
      "â”‚    â””â”€LogSoftmax: 2-9                        [1023, 8, 1000]           --\n",
      "â”œâ”€Embedding: 1-7                              [8, 94, 64]               64,000\n",
      "â”œâ”€PositionalEncoding: 1-8                     [8, 94, 64]               --\n",
      "â”œâ”€Dropout: 1-9                                [8, 94, 64]               --\n",
      "â”œâ”€ModuleList: 1-10                            --                        --\n",
      "â”‚    â””â”€CrossAttentionDecoderLayer: 2-10       [8, 94, 64]               --\n",
      "â”‚    â”‚    â””â”€SelfAttentionLayer: 3-19          [8, 94, 64]               16,768\n",
      "â”‚    â”‚    â””â”€CrossAttentionLayer: 3-20         [8, 94, 64]               16,768\n",
      "â”‚    â”‚    â””â”€FeedForwardLayer: 3-21            [8, 94, 64]               33,216\n",
      "â”‚    â””â”€CrossAttentionDecoderLayer: 2-11       [8, 94, 64]               --\n",
      "â”‚    â”‚    â””â”€SelfAttentionLayer: 3-22          [8, 94, 64]               16,768\n",
      "â”‚    â”‚    â””â”€CrossAttentionLayer: 3-23         [8, 94, 64]               16,768\n",
      "â”‚    â”‚    â””â”€FeedForwardLayer: 3-24            [8, 94, 64]               33,216\n",
      "â”‚    â””â”€CrossAttentionDecoderLayer: 2-12       [8, 94, 64]               --\n",
      "â”‚    â”‚    â””â”€SelfAttentionLayer: 3-25          [8, 94, 64]               16,768\n",
      "â”‚    â”‚    â””â”€CrossAttentionLayer: 3-26         [8, 94, 64]               16,768\n",
      "â”‚    â”‚    â””â”€FeedForwardLayer: 3-27            [8, 94, 64]               33,216\n",
      "â”‚    â””â”€CrossAttentionDecoderLayer: 2-13       [8, 94, 64]               --\n",
      "â”‚    â”‚    â””â”€SelfAttentionLayer: 3-28          [8, 94, 64]               16,768\n",
      "â”‚    â”‚    â””â”€CrossAttentionLayer: 3-29         [8, 94, 64]               16,768\n",
      "â”‚    â”‚    â””â”€FeedForwardLayer: 3-30            [8, 94, 64]               33,216\n",
      "â”‚    â””â”€CrossAttentionDecoderLayer: 2-14       [8, 94, 64]               --\n",
      "â”‚    â”‚    â””â”€SelfAttentionLayer: 3-31          [8, 94, 64]               16,768\n",
      "â”‚    â”‚    â””â”€CrossAttentionLayer: 3-32         [8, 94, 64]               16,768\n",
      "â”‚    â”‚    â””â”€FeedForwardLayer: 3-33            [8, 94, 64]               33,216\n",
      "â”‚    â””â”€CrossAttentionDecoderLayer: 2-15       [8, 94, 64]               --\n",
      "â”‚    â”‚    â””â”€SelfAttentionLayer: 3-34          [8, 94, 64]               16,768\n",
      "â”‚    â”‚    â””â”€CrossAttentionLayer: 3-35         [8, 94, 64]               16,768\n",
      "â”‚    â”‚    â””â”€FeedForwardLayer: 3-36            [8, 94, 64]               33,216\n",
      "â”œâ”€LayerNorm: 1-11                             [8, 94, 64]               128\n",
      "â”œâ”€Linear: 1-12                                [8, 94, 1000]             65,000\n",
      "===============================================================================================\n",
      "Total params: 953,104\n",
      "Trainable params: 953,104\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (Units.GIGABYTES): 37.90\n",
      "===============================================================================================\n",
      "Input size (MB): 5.25\n",
      "Forward/backward pass size (MB): 285.99\n",
      "Params size (MB): 2.61\n",
      "Estimated Total Size (MB): 293.85\n",
      "===============================================================================================\n"
     ]
    }
   ],
   "source": [
    "model_config = config['model'].copy()\n",
    "model_config.pop('decoder_checkpoint')\n",
    "model_config.pop('freeze_transferred')\n",
    "model_config.pop('decoder_lr_factor')\n",
    "model_config.update({\n",
    "    'max_len': max_len,\n",
    "    'num_classes': Tokenizer.vocab_size\n",
    "})\n",
    "\n",
    "model = EncoderDecoderTransformer(**model_config)\n",
    "\n",
    "# Get some inputs from the train dataloader\n",
    "for batch in train_loader:\n",
    "    padded_feats, padded_shifted, padded_golden, feat_lengths, transcript_lengths = batch\n",
    "    break\n",
    "\n",
    "\n",
    "model_stats = summary(model, input_data=[padded_feats, padded_shifted, feat_lengths, transcript_lengths])\n",
    "print(model_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainer\n",
    "You will have to do some minor in-filling for the `ASRTrainer` class in `hw4lib/trainers/asr_trainer.py` before you can use it.\n",
    "- `TODO`: Fill in the `TODO`s in the `__init__`.\n",
    "- `TODO`: Fill in the `TODO`s in the `_train_epoch`.\n",
    "- `TODO`: Fill in the `TODO`s in the `recognize` method.\n",
    "- `TODO`: Fill in the `TODO`s in the `_validate_epoch`.\n",
    "- `TODO`: Fill in the `TODO`s in the `train` method.\n",
    "- `TODO`: Fill in the `TODO`s in the `evaluate` method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "\n",
      "ðŸ”§ Configuring Optimizer:\n",
      "â”œâ”€â”€ Type: ADAM\n",
      "â”œâ”€â”€ Base LR: 0.0001\n",
      "â”œâ”€â”€ Weight Decay: 0.0001\n",
      "â”œâ”€â”€ Parameter Groups:\n",
      "â”‚   â”œâ”€â”€ Group: self_attn\n",
      "â”‚   â”‚   â”œâ”€â”€ LR: 0.0001\n",
      "â”‚   â”‚   â””â”€â”€ Patterns: ['self_attn']\n",
      "â”‚   â”œâ”€â”€ Group: ffn\n",
      "â”‚   â”‚   â”œâ”€â”€ LR: 0.0001\n",
      "â”‚   â”‚   â””â”€â”€ Patterns: ['ffn']\n",
      "â”‚   â””â”€â”€ Default Group (unmatched parameters)\n",
      "â””â”€â”€ Adam Specific:\n",
      "    â”œâ”€â”€ Betas: [0.9, 0.999]\n",
      "    â”œâ”€â”€ Epsilon: 1e-08\n",
      "    â””â”€â”€ AMSGrad: False\n"
     ]
    }
   ],
   "source": [
    "trainer = ASRTrainer(\n",
    "    model=model,\n",
    "    tokenizer=Tokenizer,\n",
    "    config=config,\n",
    "    run_name=\"Puru\",\n",
    "    config_file=\"config.yaml\",\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“ˆ Configuring Learning Rate Scheduler:\n",
      "â”œâ”€â”€ Type: COSINE_WARM\n",
      "â”œâ”€â”€ Cosine Annealing Warm Restarts Settings:\n",
      "â”‚   â”œâ”€â”€ T_0: 4 epochs (16 steps)\n",
      "â”‚   â”œâ”€â”€ T_mult: 4\n",
      "â”‚   â””â”€â”€ Min LR: 1e-07\n",
      "â””â”€â”€ Warmup: Disabled\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š Training Metrics (Epoch 0):\n",
      "â”œâ”€â”€ TRAIN:\n",
      "â”‚   â”œâ”€â”€ ce_loss: 6.9397\n",
      "â”‚   â”œâ”€â”€ ctc_loss: 68.6921\n",
      "â”‚   â”œâ”€â”€ joint_loss: 27.5473\n",
      "â”‚   â”œâ”€â”€ perplexity_char: 9.3305\n",
      "â”‚   â””â”€â”€ perplexity_token: 1032.4197\n",
      "â””â”€â”€ VAL:\n",
      "    â”œâ”€â”€ cer: 3956.7902\n",
      "    â”œâ”€â”€ wer: 3214.8148\n",
      "    â””â”€â”€ word_dist: 3205.0000\n",
      "â””â”€â”€ TRAINING:\n",
      "    â””â”€â”€ learning_rate: 0.000085\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š Training Metrics (Epoch 1):\n",
      "â”œâ”€â”€ TRAIN:\n",
      "â”‚   â”œâ”€â”€ ce_loss: 6.9119\n",
      "â”‚   â”œâ”€â”€ ctc_loss: 67.0600\n",
      "â”‚   â”œâ”€â”€ joint_loss: 27.0299\n",
      "â”‚   â”œâ”€â”€ perplexity_char: 9.2474\n",
      "â”‚   â””â”€â”€ perplexity_token: 1004.1275\n",
      "â””â”€â”€ VAL:\n",
      "    â”œâ”€â”€ cer: 3958.6418\n",
      "    â”œâ”€â”€ wer: 3288.8889\n",
      "    â””â”€â”€ word_dist: 3206.5000\n",
      "â””â”€â”€ TRAINING:\n",
      "    â””â”€â”€ learning_rate: 0.000050\n"
     ]
    }
   ],
   "source": [
    "trainer.train(train_loader, val_loader, epochs=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating with greedy config\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating with beam_8 config\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating with beam_16 config\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating with beam_32 config\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Recognizing ASR] : beam_32:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhw4p2_sol.json\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m      2\u001b[0m     solution \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[0;32m----> 4\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msolution\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtest_dataset\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m config_name, metrics \u001b[38;5;129;01min\u001b[39;00m results\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m50\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/11785-TA/s25/HW4/hw4lib/trainers/asr_trainer.py:280\u001b[0m, in \u001b[0;36mASRTrainer.evaluate\u001b[0;34m(self, dataloader, solution)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m config_name, config \u001b[38;5;129;01min\u001b[39;00m recognition_configs\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    279\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEvaluating with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m config\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 280\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecognize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    281\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(results) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(solution_data)\n\u001b[1;32m    283\u001b[0m     \u001b[38;5;66;03m# Calculate metrics on full batch\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/11785-TA/s25/HW4/hw4lib/trainers/asr_trainer.py:367\u001b[0m, in \u001b[0;36mASRTrainer.recognize\u001b[0;34m(self, dataloader, recognition_config, config_name)\u001b[0m\n\u001b[1;32m    365\u001b[0m \u001b[38;5;66;03m# TODO: Generate sequences using beam search or greedy search\u001b[39;00m\n\u001b[1;32m    366\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recognition_config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbeam_width\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 367\u001b[0m     seqs, scores \u001b[38;5;241m=\u001b[39m \u001b[43mgenerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_beam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    368\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    369\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeam_width\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrecognition_config\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbeam_width\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    370\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrecognition_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    371\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepeat_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrecognition_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrepeat_penalty\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    372\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    373\u001b[0m     \u001b[38;5;66;03m# Pick best beam\u001b[39;00m\n\u001b[1;32m    374\u001b[0m     seqs \u001b[38;5;241m=\u001b[39m seqs[:, \u001b[38;5;241m0\u001b[39m, :]\n",
      "File \u001b[0;32m~/Desktop/11785-TA/s25/HW4/hw4lib/decoding/sequence_generator.py:250\u001b[0m, in \u001b[0;36mSequenceGenerator.generate_beam\u001b[0;34m(self, x, beam_width, temperature, repeat_penalty)\u001b[0m\n\u001b[1;32m    248\u001b[0m next_token_scores \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(beam_width):\n\u001b[0;32m--> 250\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscore_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    251\u001b[0m     next_token_scores\u001b[38;5;241m.\u001b[39mappend(logits)\n\u001b[1;32m    252\u001b[0m next_token_scores \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack(next_token_scores, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m# (batch_size, beam_width, vocab_size)\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/11785-TA/s25/HW4/hw4lib/trainers/asr_trainer.py:352\u001b[0m, in \u001b[0;36mASRTrainer.recognize.<locals>.get_score\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    351\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_score\u001b[39m(x):\n\u001b[0;32m--> 352\u001b[0m     asr_logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscore\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpad_mask_src\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    353\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m recognition_config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlm_model\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    354\u001b[0m         lm_logits \u001b[38;5;241m=\u001b[39m recognition_config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlm_model\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mscore(x)\n",
      "File \u001b[0;32m~/Desktop/11785-TA/s25/HW4/hw4lib/model/transformers.py:460\u001b[0m, in \u001b[0;36mEncoderDecoderTransformer.score\u001b[0;34m(self, batch_prompts, encoder_output, pad_mask_src)\u001b[0m\n\u001b[1;32m    457\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscore method is not supported during training\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    459\u001b[0m \u001b[38;5;66;03m# TODO: Use decode function with no target lengths (no padding mask for targets)\u001b[39;00m\n\u001b[0;32m--> 460\u001b[0m seq_out, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_prompts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpad_mask_src\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    462\u001b[0m \u001b[38;5;66;03m# Return only the last token's logits\u001b[39;00m\n\u001b[1;32m    463\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m seq_out[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :]\n",
      "File \u001b[0;32m~/Desktop/11785-TA/s25/HW4/hw4lib/model/transformers.py:384\u001b[0m, in \u001b[0;36mEncoderDecoderTransformer.decode\u001b[0;34m(self, padded_targets, encoder_output, target_lengths, pad_mask_src)\u001b[0m\n\u001b[1;32m    382\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_drop_rate \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m random\u001b[38;5;241m.\u001b[39mrandom() \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_drop_rate:\n\u001b[1;32m    383\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m--> 384\u001b[0m x_dec, self_attn, cross_attn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdec_layers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    385\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx_dec\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    386\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    387\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpad_mask_tgt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    388\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpad_mask_src\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    389\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcausal_mask\u001b[49m\n\u001b[1;32m    390\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    391\u001b[0m running_att[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlayer\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_dec_self\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m self_attn\n\u001b[1;32m    392\u001b[0m running_att[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlayer\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_dec_cross\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m cross_attn\n",
      "File \u001b[0;32m~/miniconda3/envs/hw4_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/hw4_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Desktop/11785-TA/s25/HW4/hw4lib/model/decoder_layers.py:114\u001b[0m, in \u001b[0;36mCrossAttentionDecoderLayer.forward\u001b[0;34m(self, x, enc_output, dec_key_padding_mask, enc_key_padding_mask, attn_mask)\u001b[0m\n\u001b[1;32m    112\u001b[0m x, self_attn_weights  \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself_attn(x, dec_key_padding_mask, attn_mask)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;66;03m# TODO: Apply cross-attention\u001b[39;00m\n\u001b[0;32m--> 114\u001b[0m x, cross_attn_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_attn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menc_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menc_key_padding_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;66;03m# TODO: Apply feed-forward network\u001b[39;00m\n\u001b[1;32m    116\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mffn(x)\n",
      "File \u001b[0;32m~/miniconda3/envs/hw4_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/hw4_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Desktop/11785-TA/s25/HW4/hw4lib/model/sublayers.py:142\u001b[0m, in \u001b[0;36mCrossAttentionLayer.forward\u001b[0;34m(self, x, y, key_padding_mask, attn_mask)\u001b[0m\n\u001b[1;32m    140\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm(x)\n\u001b[1;32m    141\u001b[0m \u001b[38;5;66;03m# TODO: Cross-attention with dropout (weights need to be stored)\u001b[39;00m\n\u001b[0;32m--> 142\u001b[0m x, mha_attn_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmha\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m                \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m                  \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m                \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey_padding_mask\u001b[49m\u001b[43m     \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mkey_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[43m    \u001b[49m\u001b[43mneed_weights\u001b[49m\u001b[43m         \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[43m            \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[43m    \u001b[49m\u001b[43maverage_attn_weights\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;66;03m# TODO: Add residual connection \u001b[39;00m\n\u001b[1;32m    152\u001b[0m x \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(x)\n",
      "File \u001b[0;32m~/miniconda3/envs/hw4_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/hw4_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/hw4_env/lib/python3.12/site-packages/torch/nn/modules/activation.py:1368\u001b[0m, in \u001b[0;36mMultiheadAttention.forward\u001b[0;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights, is_causal)\u001b[0m\n\u001b[1;32m   1342\u001b[0m     attn_output, attn_output_weights \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmulti_head_attention_forward(\n\u001b[1;32m   1343\u001b[0m         query,\n\u001b[1;32m   1344\u001b[0m         key,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1365\u001b[0m         is_causal\u001b[38;5;241m=\u001b[39mis_causal,\n\u001b[1;32m   1366\u001b[0m     )\n\u001b[1;32m   1367\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1368\u001b[0m     attn_output, attn_output_weights \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmulti_head_attention_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1369\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1370\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1371\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1372\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1373\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1374\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43min_proj_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1375\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43min_proj_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1376\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1377\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias_v\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1378\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_zero_attn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1379\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1380\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_proj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1381\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_proj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1382\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1383\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1384\u001b[0m \u001b[43m        \u001b[49m\u001b[43mneed_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mneed_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1385\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1386\u001b[0m \u001b[43m        \u001b[49m\u001b[43maverage_attn_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maverage_attn_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1387\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_causal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1388\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1389\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first \u001b[38;5;129;01mand\u001b[39;00m is_batched:\n\u001b[1;32m   1390\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m attn_output\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m), attn_output_weights\n",
      "File \u001b[0;32m~/miniconda3/envs/hw4_env/lib/python3.12/site-packages/torch/nn/functional.py:6237\u001b[0m, in \u001b[0;36mmulti_head_attention_forward\u001b[0;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v, average_attn_weights, is_causal)\u001b[0m\n\u001b[1;32m   6232\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\n\u001b[1;32m   6233\u001b[0m     is_causal \u001b[38;5;129;01mand\u001b[39;00m attn_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   6234\u001b[0m ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFIXME: is_causal not implemented for need_weights\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   6236\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attn_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 6237\u001b[0m     attn_output_weights \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbaddbmm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   6238\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mq_scaled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   6239\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   6240\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   6241\u001b[0m     attn_output_weights \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mbmm(q_scaled, k\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with open(\"hw4p2_sol.json\", \"r\") as f:\n",
    "    solution = json.load(f)\n",
    "\n",
    "results = trainer.evaluate(test_loader, solution[:len(test_dataset)])\n",
    "\n",
    "for config_name, metrics in results.items():\n",
    "    print(\"-\"*50)\n",
    "    print(f\"Config: {config_name}\")\n",
    "    print(f\"WER: {metrics['wer']:.2f}%\")\n",
    "    print(f\"CER: {metrics['cer']:.2f}%\")\n",
    "    print(f\"Word Distance: {metrics['word_dist']:.2f}\")\n",
    "    print(\"-\"*50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hw4_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
