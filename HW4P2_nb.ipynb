{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup \n",
    "- `TA TODO`: Setup based on your environment. Reach out to me if you face any issues. Also, any feedback/improvements to the setup process for the students based on your experience setting up here would be very appreciated! (PS: I am still figuring out the best way to do this.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local \n",
    "\n",
    "The assignment is designed in a manner that you can do most of the work `implementation` locally. We would recommend that you pass all the tests locally using the `hw4_data_subset` we've provided before moving to a GPU runtime. To do this simply:\n",
    "- Create a new conda environment with `conda create -n hw4 python=3.12.4`\n",
    "- Activate the conda environment with `conda activate hw4`\n",
    "- Install the dependencies with `pip install -r requirements.txt`\n",
    "- Ensure that your notebook is in the same directory as the `handout` folder. (See the expected directory structure in the `README.md`)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Colab (`TA TODO`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Get Repo (TA-Only, will be handout for students)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `INTERNAL TODO`: Need to switch this to handout upload for students."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GITHUB_USERNAME = \"puru-samal\"\n",
    "REPO_NAME = \"IDL-HW4\"\n",
    "BRANCH_NAME = \"TA\"\n",
    "ACCESS_TOKEN = \"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\"\n",
    "repo_url = f\"https://{GITHUB_USERNAME}:{ACCESS_TOKEN}@github.com/{GITHUB_USERNAME}/{REPO_NAME}.git\"\n",
    "!git clone -b {BRANCH_NAME} {repo_url}\n",
    "#!git clone {repo_url}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### If I announce a new commit, please delete and re-clone the repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf IDL-HW4/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Get Data\n",
    "- `INTERNAL TODO`: Need to switch this download from kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gdown 1-0e9Gnl4nm6wbIuE_Yxl2wRZI8yGxHm6 --output hw4_data.tar.gz\n",
    "!tar -xf hw4_data.tar.gz\n",
    "!rm -rf hw4_data.tar.gz\n",
    "!du -h max-depth=3 hw4_data_kaggle/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Install Dependencies\n",
    "- `NOTE`: Colab may prompt you to restart your runtime. Do so then proceed to the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --no-deps -r IDL-HW4/colab_requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Move to Project Directory\n",
    "- `NOTE`: You may have to repeat this on restarting your runtime. You can do a `pwd` to check if you are in the right directory.\n",
    "- `NOTE`: Your data directory should be one level up from your project directory. Keep this in mind when you are setting your `root` in the config file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('IDL-HW4')\n",
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PSC (`TA TODO`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Preliminaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `Step 0:` ssh into Bridges2 with `ssh username@bridges2.psc.edu`\n",
    "- `Step 1:` cd into your project directory with `cd $PROJECT`\n",
    "- `Step 2:` Load the anaconda module with `module load anaconda3`\n",
    "- `Step 3:` Activate the HW4 envirtonent that was created for you with `conda activate /jet/home/psamal/hw_envs/idl_hw4` (Make sure to deactivate any existing conda environment first with `conda deactivate`)\n",
    "- `Step 4:` Get a compute node with `interact -p GPU-shared --gres=gpu:v100-32:1 -t 8:00:00`\n",
    "- `Step 5:` Run `conda deactivate` if your conda environment was deactivated due to node allocation. Ensure you are in the HW4 environment.\n",
    "- `Step 6:` Now follow your usual steps to start a jupyter notebook. For me this is:\n",
    "  - Start a jupyter notebook with `jupyter notebook --no-browser --ip=0.0.0.0` \n",
    "  - On a separate terminal, start a tunnel with `ssh -L 8888:{hostname}:{port} bridges2.psc.edu -l username`\n",
    "  - Select the appropriate kernel on VSCode: Kernel -> Select Another Kernel -> Existing Jupyter Server -> `http://127.0.0.1:{port}/tree?token={token}`\n",
    "- `Step 7:` Now follow the instructions below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Get Repo (TA-Only, will be handout for students)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GITHUB_USERNAME = \"puru-samal\"\n",
    "REPO_NAME = \"IDL-HW4\"\n",
    "BRANCH_NAME = \"TA\"\n",
    "ACCESS_TOKEN = \"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\"\n",
    "repo_url = f\"https://{GITHUB_USERNAME}:{ACCESS_TOKEN}@github.com/{GITHUB_USERNAME}/{REPO_NAME}.git\"\n",
    "#!git clone -b {BRANCH_NAME} {repo_url} # TA ONLY\n",
    "!git clone {repo_url}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### If I announce a new commit, please delete and re-clone the repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf IDL-HW4/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Move to Project Directory\n",
    "- `NOTE`: You may have to repeat this on anytime you restart your runtime. You can do a `pwd` or `ls` to check if you are in the right directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('IDL-HW4')\n",
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Get Data\n",
    "- `NOTE`: We are using `$LOCAL`: the scratch storage on local disk on the node running a job to store out data. Disk accesses are much faster than what you would get from `$PROJECT` storage, but `IT IS NOT PERSISTENT`. \n",
    "- `NOTE`: Make sure you have a node allocated to you with `interact -p GPU-shared --gres=gpu:v100-32:1 -t 8:00:00`\n",
    "- Read more about it PSC File Spaces [here](https://www.psc.edu/resources/bridges-2/user-guide#file-spaces)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gdown 1-0e9Gnl4nm6wbIuE_Yxl2wRZI8yGxHm6 --output $LOCAL/hw4_data.tar.gz\n",
    "!ls $LOCAL/\n",
    "!tar -xf $LOCAL/hw4_data.tar.gz -C $LOCAL/\n",
    "!rm -rf $LOCAL/hw4_data.tar.gz\n",
    "!du --max-depth=3 $LOCAL/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hw4lib.data import (\n",
    "    H4Tokenizer,\n",
    "    ASRDataset,\n",
    "    verify_dataloader\n",
    ")\n",
    "from hw4lib.model import (\n",
    "    DecoderOnlyTransformer,\n",
    "    EncoderDecoderTransformer\n",
    ")\n",
    "from hw4lib.utils import (\n",
    "    create_scheduler,\n",
    "    create_optimizer,\n",
    "    plot_lr_schedule\n",
    ")\n",
    "from hw4lib.trainers import (\n",
    "    ASRTrainer,\n",
    "    ProgressiveTrainer\n",
    ")\n",
    "from torch.utils.data import DataLoader\n",
    "import yaml\n",
    "import gc\n",
    "import torch\n",
    "from torchinfo import summary\n",
    "import os\n",
    "import json\n",
    "import wandb\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementations\n",
    "- `TA TODO`: \n",
    "  - `MANDATORY`: Run these cells to verify that the testing works in your chosen environment. Lmk if it doesn't.\n",
    "  - `OPTIONAL`: Do read through the implementations. Any feedback regarding them would be very appreciated!\n",
    "- `NOTE`: All of these implementations have detailed specification, implementation details, and hints in their respective source files. Make sure to read all of them in their entirety to understand the implementation details!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Implementation\n",
    "- `TODO`: Implement the `ASRDataset` class in `hw4lib/data/asr_dataset.py`. \n",
    "- You will have to implement parts of `__init__` and completely implement the `__len__`, `__getitem__` and `collate_fn` methods. \n",
    "- `TODO`: Then run the cell below to check your implementation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m tests.test_dataset_asr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Implementations\n",
    "\n",
    "- `TODO`: Implement the `CrossAttentionLayer` class in `hw4lib/model/sublayers.py`.\n",
    "- `TODO`: Implement the `CrossAttentionDecoderLayer` class in `hw4lib/model/decoder_layers.py`.\n",
    "- `TODO`: Implement the `SelfAttentionEncoderLayer` class in `hw4lib/model/encoder_layers.py`. This will be mostly a copy-paste of the `SelfAttentionDecoderLayer` class in `hw4lib/model/decoder_layers.py` with one minor diffrence: it can attend to all positions in the input sequence.\n",
    "- `TODO`: Implement the `EncoderDecoderTransformer` class in `hw4lib/model/transformers.py`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer Sublayers\n",
    "- `TODO`: Now, Implement the `CrossAttentionLayer` class in `hw4lib/model/sublayers.py`.\n",
    "- `NOTE`: You should have already implemented the `SelfAttentionLayer`, and `FeedForwardLayer` classes in `hw4lib/model/sublayers.py`.\n",
    "- `TODO`: Then run the cell below to check your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m tests.test_sublayer_crossattention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer Cross-Attention Decoder Layer\n",
    "- `TODO`: Implement the `CrossAttentionDecoderLayer` class in `hw4lib/model/decoder_layers.py`.\n",
    "- `TODO`: Then run the cell below to check your implementation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m tests.test_decoderlayer_crossattention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer Self-Attention Encoder Layer\n",
    "- `TODO`: Implement the `SelfAttentionEncoderLayer` class in `hw4lib/model/encoder_layers.py`.\n",
    "- `TODO`: Then run the cell below to check your implementation.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m tests.test_encoderlayer_selfattention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder-Decoder Transformer\n",
    "\n",
    "- `TODO`: Implement the  `EncoderDecoderTransformer` class in `hw4lib/model/transformers.py`.\n",
    "- `TODO`: Then run the cell below to check your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m tests.test_transformer_encoder_decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoding Implementations \n",
    "- `TODO`: We highly recommend you to implement the `generate_beam` method of the `SequenceGenerator` class in `hw4lib/decoding/sequence_generator.py`.\n",
    "- `TODO`: Then run the cell below to check your implementation.\n",
    "- `NOTE`: This is an optional but highly recommended task for `HW4P2` to ease the journey to high cutoffs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m tests.test_decoding --mode beam  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments\n",
    "- Please keep an eye out for the `TA TODO`'s in the following cells."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config\n",
    "- `TA TODO`: You can use the `config.yaml` file to set your config for your ablation study.\n",
    "- `TA TODO`: Remember to change the `root` path!\n",
    "- `NOTE`: For the values not provided in the ablation sheet, feel free to set as you see fit.\n",
    "- `NOTE`: If warmup is enabled in `scheduler` section, the warmup phase will happen first before switching to the base scheduler.\n",
    "- `NOTE`: `warmup` is currently not supported with `ReduceLROnPlateau` scheduler.\n",
    "\n",
    "For our purposes, we define the following terms:\n",
    "- Light SpecAug  : `5 freq_mask_width_range, 1 num_freq_mask, 20 time_mask_width_range, 1 num_time_mask`\n",
    "- Medium SpecAug : `5 freq_mask_width_range, 2 num_freq_mask, 40 time_mask_width_range, 2 num_time_mask`\n",
    "- Heavy SpecAug  : `5 freq_mask_width_range, 4 num_freq_mask, 40 time_mask_width_range, 4 num_time_mask`\n",
    "\n",
    "- `IMPORTANT`: You are required to run 70 epochs in total. \n",
    "\n",
    "\n",
    "### Experimental \n",
    "- `NOTE`: There is one experimental setup for the optimizer configuration, i.e Pattern-matching to group parameters by their names and apply different learning rates to them. \n",
    "- Eg. `self_attn` will match all parameters containing `self_attn` in their names. \n",
    "- See `hw4lib/utils/create_optimizer.py` for more details. Again, experiment if you want with it but I am still testing it out. \n",
    "- Motivation is to use it to set lower learning rates for `self-attn` and `ffn` modules while initializing an Encoder-Decoder Transformer with weights from a pre-trained Decoder-Only Transformer.\n",
    "- This is for Internal Testing only, wont be available for student use for simplicity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting config.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile config.yaml\n",
    "\n",
    "Name                      : \"Puru\"\n",
    "\n",
    "###### Tokenization ------------------------------------------------------------\n",
    "tokenization:\n",
    "  token_type                : \"10k\"       # [char, 1k, 5k, 10k]\n",
    "  token_map :\n",
    "      'char': 'hw4lib/data/tokenizer_jsons/tokenizer_char.json'\n",
    "      '1k'  : 'hw4lib/data/tokenizer_jsons/tokenizer_1000.json'\n",
    "      '5k'  : 'hw4lib/data/tokenizer_jsons/tokenizer_5000.json'\n",
    "      '10k' : 'hw4lib/data/tokenizer_jsons/tokenizer_10000.json'\n",
    "\n",
    "###### Dataset -----------------------------------------------------------------\n",
    "data:\n",
    "  root                 : \"/local/hw4_data_kaggle/hw4p2_data\"  # TODO: Set the root path of your data\n",
    "  train_partition      : \"train-clean-100\"  # paired text-speech for ASR pre-training\n",
    "  val_partition        : \"dev-clean\"        # paired text-speech for ASR pre-training\n",
    "  test_partition       : \"test-clean\"       # paired text-speech for ASR pre-training\n",
    "  subset               : 1.0                # Load a subset of the data (for debugging, testing, etc\n",
    "  batch_size           : 32           #   \n",
    "  NUM_WORKERS          : 2            # Set to 0 for CPU\n",
    "  norm                 : 'global_mvn' # ['global_mvn', 'cepstral', 'none']\n",
    "  num_feats            : 80\n",
    "\n",
    "  ###### SpecAugment ---------------------------------------------------------------\n",
    "  specaug                   : False  # TODO: Set to True if you want to use SpecAugment\n",
    "  # Light  :  5, 1, 20, 1\n",
    "  # Medium :  5, 2, 40, 2\n",
    "  # Heavy  :  5, 4, 40, 4\n",
    "  # Currently set to Light\n",
    "  specaug_conf:\n",
    "    apply_freq_mask         : True\n",
    "    freq_mask_width_range   : 5\n",
    "    num_freq_mask           : 1\n",
    "    apply_time_mask         : True\n",
    "    time_mask_width_range   : 20\n",
    "    num_time_mask           : 1\n",
    "\n",
    "###### Network Specs -------------------------------------------------------------\n",
    "model: # Encoder-Decoder Transformer (HW4P2)\n",
    "  # Speech embedding parameters\n",
    "  input_dim: 80              # Speech feature dimension\n",
    "  time_reduction: 2          # Time dimension downsampling factor\n",
    "  reduction_method: 'conv'   # The source_embedding reduction method ['lstm', 'conv', 'both']\n",
    "  \n",
    "  # Architecture parameters\n",
    "  d_model: 256            # Model dimension\n",
    "  num_encoder_layers: 8  # Number of encoder layers\n",
    "  num_decoder_layers: 6  # Number of decoder layers\n",
    "  num_encoder_heads: 8   # Number of encoder attention heads\n",
    "  num_decoder_heads: 8   # Number of decoder attention heads\n",
    "  d_ff_encoder: 1536     # Feed-forward dimension for encoder\n",
    "  d_ff_decoder: 3072     # Feed-forward dimension for decoder\n",
    "  skip_encoder_pe: True  # Whether to skip positional encoding for encoder\n",
    "  skip_decoder_pe: False # Whether to skip positional encoding for decoder\n",
    "  \n",
    "  # Common parameters\n",
    "  dropout: 0.0          # Dropout rate\n",
    "  layer_drop_rate: 0.0  # Layer dropout rate\n",
    "  weight_tying: False   # Whether to use weight tying\n",
    "  \n",
    "###### Common Training Parameters ------------------------------------------------\n",
    "training:\n",
    "  use_wandb                   : False\n",
    "  wandb_run_id                : \"none\" # \"none\" or \"run_id\"\n",
    "  resume                      : False\n",
    "  epochs                      : 70\n",
    "  gradient_accumulation_steps : 1\n",
    "  wandb_project               : \"S25-HW4P2-TA\"\n",
    "\n",
    "###### Loss ----------------------------------------------------------------------\n",
    "loss: # Just good ol' CrossEntropy\n",
    "  label_smoothing: 0.1\n",
    "  ctc_weight: 0.3\n",
    "\n",
    "###### Optimizer -----------------------------------------------------------------\n",
    "optimizer:\n",
    "  name: \"adam\" # Options: sgd, adam, adamw\n",
    "  lr: 0.0002  # Base learning rate\n",
    "\n",
    "  # Common parameters\n",
    "  weight_decay: 0.000001\n",
    "\n",
    "  # Parameter groups\n",
    "  # You can add more param groups as you want and set their learning rates and patterns\n",
    "  param_groups:\n",
    "    - name: self_attn\n",
    "      patterns: []  # Will match all parameters containing \"encoder\"\n",
    "      lr: 0.0002  # LR for self_attn\n",
    "      layer_decay:\n",
    "        enabled: False\n",
    "        decay_rate: 0.8\n",
    "    \n",
    "    - name: ffn\n",
    "      patterns: []\n",
    "      lr: 0.0002  # LR for ffn\n",
    "      layer_decay:\n",
    "        enabled: False\n",
    "        decay_rate: 0.8\n",
    "  \n",
    "  # Layer-wise learning rates\n",
    "  layer_decay:\n",
    "    enabled: False\n",
    "    decay_rate: 0.75\n",
    "\n",
    "  # SGD specific parameters\n",
    "  sgd:\n",
    "    momentum: 0.9\n",
    "    nesterov: True\n",
    "    dampening: 0\n",
    "\n",
    "  # Adam specific parameters\n",
    "  adam:\n",
    "    betas: [0.9, 0.999]\n",
    "    eps: 1.0e-8\n",
    "    amsgrad: False\n",
    "\n",
    "  # AdamW specific parameters\n",
    "  adamw:\n",
    "    betas: [0.9, 0.999]\n",
    "    eps: 1.0e-8\n",
    "    amsgrad: False\n",
    "\n",
    "###### Scheduler -----------------------------------------------------------------\n",
    "scheduler:\n",
    "  name: \"cosine\"  # Options: reduce_lr, cosine, cosine_warm\n",
    "\n",
    "  # ReduceLROnPlateau specific parameters\n",
    "  reduce_lr:\n",
    "    mode: \"min\"  # Options: min, max\n",
    "    factor: 0.1  # Factor to reduce learning rate by\n",
    "    patience: 10  # Number of epochs with no improvement after which LR will be reduced\n",
    "    threshold: 0.0001  # Threshold for measuring the new optimum\n",
    "    threshold_mode: \"rel\"  # Options: rel, abs\n",
    "    cooldown: 0  # Number of epochs to wait before resuming normal operation\n",
    "    min_lr: 0.0000001  # Minimum learning rate\n",
    "    eps: 1e-8  # Minimal decay applied to lr\n",
    "\n",
    "  # CosineAnnealingLR specific parameters\n",
    "  cosine:\n",
    "    T_max: 20  # Maximum number of iterations\n",
    "    eta_min: 0.0000001  # Minimum learning rate\n",
    "    last_epoch: -1\n",
    "\n",
    "  # CosineAnnealingWarmRestarts specific parameters\n",
    "  cosine_warm:\n",
    "    T_0: 10    # Number of iterations for the first restart\n",
    "    T_mult: 10 # Factor increasing T_i after each restart\n",
    "    eta_min: 0.0000001  # Minimum learning rate\n",
    "    last_epoch: -1\n",
    "\n",
    "  # Warmup parameters (can be used with any scheduler)\n",
    "  warmup:\n",
    "    enabled: True\n",
    "    type: \"exponential\"  # Options: linear, exponential\n",
    "    epochs: 10\n",
    "    start_factor: 0.1\n",
    "    end_factor: 1.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('config.yaml', 'r') as file:\n",
    "    config = yaml.safe_load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "                          Tokenizer Configuration (1k)                          \n",
      "--------------------------------------------------------------------------------\n",
      "Vocabulary size:     1000\n",
      "\n",
      "Special Tokens:\n",
      "PAD:              0\n",
      "UNK:              1\n",
      "MASK:             2\n",
      "SOS:              3\n",
      "EOS:              4\n",
      "BLANK:            5\n",
      "\n",
      "Validation Example:\n",
      "--------------------------------------------------------------------------------\n",
      "Input text:  [SOS]HI DEEP LEARNERS[EOS]\n",
      "Tokens:      ['[SOS]', 'H', 'I', 'ĠDE', 'EP', 'ĠLE', 'AR', 'N', 'ERS', '[EOS]']\n",
      "Token IDs:   [3, 14, 15, 159, 290, 228, 71, 20, 214, 4]\n",
      "Decoded:     [SOS]HI DEEP LEARNERS[EOS]\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "Tokenizer = H4Tokenizer(\n",
    "    token_map  = config['tokenization']['token_map'], \n",
    "    token_type = config['tokenization']['token_type']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data for train-clean-100 partition...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 28539/28539 [00:23<00:00, 1221.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global stats computed from training set.\n",
      "Loading data for dev-clean partition...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 2703/2703 [00:01<00:00, 2378.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data for test-clean partition...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 2620/2620 [00:00<00:00, 4150.93it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1104"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = ASRDataset(\n",
    "    partition=config['data']['train_partition'],\n",
    "    config=config['data'],\n",
    "    tokenizer=Tokenizer,\n",
    "    isTrainPartition=True,\n",
    "    global_stats=None  # Will compute stats from training data\n",
    ")\n",
    "\n",
    "# TODO: Get the computed global stats from training set\n",
    "global_stats = None\n",
    "if config['data']['norm'] == 'global_mvn':\n",
    "    global_stats = (train_dataset.global_mean, train_dataset.global_std)\n",
    "    print(f\"Global stats computed from training set.\")\n",
    "\n",
    "val_dataset = ASRDataset(\n",
    "    partition=config['data']['val_partition'],\n",
    "    config=config['data'],\n",
    "    tokenizer=Tokenizer,\n",
    "    isTrainPartition=False,\n",
    "    global_stats=global_stats\n",
    ")\n",
    "\n",
    "test_dataset = ASRDataset(\n",
    "    partition=config['data']['test_partition'],\n",
    "    config=config['data'],\n",
    "    tokenizer=Tokenizer,\n",
    "    isTrainPartition=False,\n",
    "    global_stats=global_stats\n",
    ")\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader    = DataLoader(\n",
    "    dataset     = train_dataset,\n",
    "    batch_size  = config['data']['batch_size'],\n",
    "    shuffle     = True,\n",
    "    num_workers = config['data']['NUM_WORKERS'] if device == 'cuda' else 0,\n",
    "    pin_memory  = True,\n",
    "    collate_fn  = train_dataset.collate_fn   \n",
    ")\n",
    "\n",
    "val_loader      = DataLoader(\n",
    "    dataset     = val_dataset,\n",
    "    batch_size  = config['data']['batch_size'],\n",
    "    shuffle     = False,\n",
    "    num_workers = config['data']['NUM_WORKERS'] if device == 'cuda' else 0,\n",
    "    pin_memory  = True,\n",
    "    collate_fn  = val_dataset.collate_fn   \n",
    ")\n",
    "\n",
    "test_loader     = DataLoader(\n",
    "    dataset     = test_dataset,\n",
    "    batch_size  = config['data']['batch_size'],\n",
    "    shuffle     = False,\n",
    "    num_workers = config['data']['NUM_WORKERS'] if device == 'cuda' else 0,\n",
    "    pin_memory  = True,\n",
    "    collate_fn  = test_dataset.collate_fn   \n",
    ")\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataloader Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "             Dataloader Verification              \n",
      "==================================================\n",
      "Dataloader Partition     : train-clean-100\n",
      "--------------------------------------------------\n",
      "Number of Batches        : 892\n",
      "Batch Size               : 32\n",
      "--------------------------------------------------\n",
      "Checking shapes of the data...                    \n",
      "\n",
      "Feature Shape            : [32, 2039, 80]\n",
      "Shifted Transcript Shape : [32, 93]\n",
      "Golden Transcript Shape  : [32, 93]\n",
      "Feature Lengths Shape    : [32]\n",
      "Transcript Lengths Shape : [32]\n",
      "--------------------------------------------------\n",
      "Max Feature Length       : 3066\n",
      "Max Transcript Length    : 139\n",
      "Avg. Chars per Token     : 3.13\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "verify_dataloader(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "             Dataloader Verification              \n",
      "==================================================\n",
      "Dataloader Partition     : dev-clean\n",
      "--------------------------------------------------\n",
      "Number of Batches        : 85\n",
      "Batch Size               : 32\n",
      "--------------------------------------------------\n",
      "Checking shapes of the data...                    \n",
      "\n",
      "Feature Shape            : [32, 3676, 80]\n",
      "Shifted Transcript Shape : [32, 132]\n",
      "Golden Transcript Shape  : [32, 132]\n",
      "Feature Lengths Shape    : [32]\n",
      "Transcript Lengths Shape : [32]\n",
      "--------------------------------------------------\n",
      "Max Feature Length       : 4081\n",
      "Max Transcript Length    : 179\n",
      "Avg. Chars per Token     : 3.08\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "verify_dataloader(val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "             Dataloader Verification              \n",
      "==================================================\n",
      "Dataloader Partition     : test-clean\n",
      "--------------------------------------------------\n",
      "Number of Batches        : 82\n",
      "Batch Size               : 32\n",
      "--------------------------------------------------\n",
      "Checking shapes of the data...                    \n",
      "\n",
      "Feature Shape            : [32, 2099, 80]\n",
      "Feature Lengths Shape    : [32]\n",
      "--------------------------------------------------\n",
      "Max Feature Length       : 4370\n",
      "Max Transcript Length    : 0\n",
      "Avg. Chars per Token     : 0.00\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "verify_dataloader(test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Max Lengths\n",
    "Calculating the maximum transcript length across your dataset is a crucial step when working with certain transformer models. \n",
    "-  We'll use sinusoidal positional encodings that must be precomputed up to a fixed maximum length.\n",
    "- This maximum length is a hyperparameter that determines:\n",
    "  - How long of a sequence your model can process\n",
    "  - The size of your positional encoding matrix\n",
    "  - Memory requirements during training and inference\n",
    "- `Requirements`: For this assignment, ensure your positional encodings can accommodate at least the longest sequence in your dataset to prevent truncation. However, you can set this value higher if you anticipate using your languagemodel to work with longer sequences in future tasks (hint: this might be useful for P2! 😉).\n",
    "- `NOTE`: We'll be using the same positional encoding matrix for all sequences in your dataset. Take this into account when setting your maximum length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_feat_len       = max(train_dataset.feat_max_len, val_dataset.feat_max_len, test_dataset.feat_max_len)\n",
    "max_transcript_len = max(train_dataset.text_max_len, val_dataset.text_max_len, test_dataset.text_max_len)\n",
    "max_len            = max(max_feat_len, max_transcript_len)\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(f\"{'Max Feature Length':<30} : {max_feat_len}\")\n",
    "print(f\"{'Max Transcript Length':<30} : {max_transcript_len}\")\n",
    "print(f\"{'Overall Max Length':<30} : {max_len}\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.login(key=\"31888e0ba72a18d4a57ea02c19a9687bc4481f37\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training \n",
    "\n",
    "You will have to do some minor in-filling for the `ASRTrainer` class in `hw4lib/trainers/asr_trainer.py` before you can use it.\n",
    "- `TODO`: Fill in the `TODO`s in the `__init__`.\n",
    "- `TODO`: Fill in the `TODO`s in the `_train_epoch`.\n",
    "- `TODO`: Fill in the `TODO`s in the `recognize` method.\n",
    "- `TODO`: Fill in the `TODO`s in the `_validate_epoch`.\n",
    "- `TODO`: Fill in the `TODO`s in the `train` method.\n",
    "- `TODO`: Fill in the `TODO`s in the `evaluate` method.\n",
    "\n",
    "Every time you run the trainer, it will create a new directory in the `expts` folder with the following structure:\n",
    "```\n",
    "expts/\n",
    "    └── {run_name}/\n",
    "        ├── config.yaml\n",
    "        ├── model_arch.txt\n",
    "        ├── checkpoints/\n",
    "        │   ├── checkpoint-best-metric-model.pth\n",
    "        │   └── checkpoint-last-epoch-model.pth\n",
    "        ├── attn/\n",
    "        │   └── {attention visualizations}\n",
    "        └── text/\n",
    "            └── {generated text outputs}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Strategy 1: Cold-Start Trainer\n",
    "\n",
    "- `TA TODO`: Run this section if you are assigned the `Cold-Start` task. Nothing special here, just the standard training loop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Load (Default)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "Layer (type:depth-idx)                             Output Shape              Param #\n",
      "====================================================================================================\n",
      "EncoderDecoderTransformer                          [32, 92, 1000]            --\n",
      "├─SpeechEmbedding: 1-1                             [32, 984, 256]            --\n",
      "│    └─Conv2DSubsampling: 2-1                      [32, 984, 256]            --\n",
      "│    │    └─Sequential: 3-1                        [32, 256, 984, 76]        592,640\n",
      "│    │    └─Linear: 3-2                            [32, 984, 256]            4,980,992\n",
      "│    │    └─Dropout: 3-3                           [32, 984, 256]            --\n",
      "├─Dropout: 1-2                                     [32, 984, 256]            --\n",
      "├─ModuleList: 1-3                                  --                        --\n",
      "│    └─SelfAttentionEncoderLayer: 2-2              [32, 984, 256]            --\n",
      "│    │    └─SelfAttentionLayer: 3-4                [32, 984, 256]            263,680\n",
      "│    │    └─FeedForwardLayer: 3-5                  [32, 984, 256]            788,736\n",
      "│    └─SelfAttentionEncoderLayer: 2-3              [32, 984, 256]            --\n",
      "│    │    └─SelfAttentionLayer: 3-6                [32, 984, 256]            263,680\n",
      "│    │    └─FeedForwardLayer: 3-7                  [32, 984, 256]            788,736\n",
      "│    └─SelfAttentionEncoderLayer: 2-4              [32, 984, 256]            --\n",
      "│    │    └─SelfAttentionLayer: 3-8                [32, 984, 256]            263,680\n",
      "│    │    └─FeedForwardLayer: 3-9                  [32, 984, 256]            788,736\n",
      "│    └─SelfAttentionEncoderLayer: 2-5              [32, 984, 256]            --\n",
      "│    │    └─SelfAttentionLayer: 3-10               [32, 984, 256]            263,680\n",
      "│    │    └─FeedForwardLayer: 3-11                 [32, 984, 256]            788,736\n",
      "│    └─SelfAttentionEncoderLayer: 2-6              [32, 984, 256]            --\n",
      "│    │    └─SelfAttentionLayer: 3-12               [32, 984, 256]            263,680\n",
      "│    │    └─FeedForwardLayer: 3-13                 [32, 984, 256]            788,736\n",
      "│    └─SelfAttentionEncoderLayer: 2-7              [32, 984, 256]            --\n",
      "│    │    └─SelfAttentionLayer: 3-14               [32, 984, 256]            263,680\n",
      "│    │    └─FeedForwardLayer: 3-15                 [32, 984, 256]            788,736\n",
      "│    └─SelfAttentionEncoderLayer: 2-8              [32, 984, 256]            --\n",
      "│    │    └─SelfAttentionLayer: 3-16               [32, 984, 256]            263,680\n",
      "│    │    └─FeedForwardLayer: 3-17                 [32, 984, 256]            788,736\n",
      "│    └─SelfAttentionEncoderLayer: 2-9              [32, 984, 256]            --\n",
      "│    │    └─SelfAttentionLayer: 3-18               [32, 984, 256]            263,680\n",
      "│    │    └─FeedForwardLayer: 3-19                 [32, 984, 256]            788,736\n",
      "├─LayerNorm: 1-4                                   [32, 984, 256]            512\n",
      "├─Sequential: 1-5                                  [984, 32, 1000]           --\n",
      "│    └─Linear: 2-10                                [984, 32, 1000]           257,000\n",
      "│    └─LogSoftmax: 2-11                            [984, 32, 1000]           --\n",
      "├─Embedding: 1-6                                   [32, 92, 256]             256,000\n",
      "├─PositionalEncoding: 1-7                          [32, 92, 256]             --\n",
      "├─Dropout: 1-8                                     [32, 92, 256]             --\n",
      "├─ModuleList: 1-9                                  --                        --\n",
      "│    └─CrossAttentionDecoderLayer: 2-12            [32, 92, 256]             --\n",
      "│    │    └─SelfAttentionLayer: 3-20               [32, 92, 256]             263,680\n",
      "│    │    └─CrossAttentionLayer: 3-21              [32, 92, 256]             263,680\n",
      "│    │    └─FeedForwardLayer: 3-22                 [32, 92, 256]             1,576,704\n",
      "│    └─CrossAttentionDecoderLayer: 2-13            [32, 92, 256]             --\n",
      "│    │    └─SelfAttentionLayer: 3-23               [32, 92, 256]             263,680\n",
      "│    │    └─CrossAttentionLayer: 3-24              [32, 92, 256]             263,680\n",
      "│    │    └─FeedForwardLayer: 3-25                 [32, 92, 256]             1,576,704\n",
      "│    └─CrossAttentionDecoderLayer: 2-14            [32, 92, 256]             --\n",
      "│    │    └─SelfAttentionLayer: 3-26               [32, 92, 256]             263,680\n",
      "│    │    └─CrossAttentionLayer: 3-27              [32, 92, 256]             263,680\n",
      "│    │    └─FeedForwardLayer: 3-28                 [32, 92, 256]             1,576,704\n",
      "│    └─CrossAttentionDecoderLayer: 2-15            [32, 92, 256]             --\n",
      "│    │    └─SelfAttentionLayer: 3-29               [32, 92, 256]             263,680\n",
      "│    │    └─CrossAttentionLayer: 3-30              [32, 92, 256]             263,680\n",
      "│    │    └─FeedForwardLayer: 3-31                 [32, 92, 256]             1,576,704\n",
      "│    └─CrossAttentionDecoderLayer: 2-16            [32, 92, 256]             --\n",
      "│    │    └─SelfAttentionLayer: 3-32               [32, 92, 256]             263,680\n",
      "│    │    └─CrossAttentionLayer: 3-33              [32, 92, 256]             263,680\n",
      "│    │    └─FeedForwardLayer: 3-34                 [32, 92, 256]             1,576,704\n",
      "│    └─CrossAttentionDecoderLayer: 2-17            [32, 92, 256]             --\n",
      "│    │    └─SelfAttentionLayer: 3-35               [32, 92, 256]             263,680\n",
      "│    │    └─CrossAttentionLayer: 3-36              [32, 92, 256]             263,680\n",
      "│    │    └─FeedForwardLayer: 3-37                 [32, 92, 256]             1,576,704\n",
      "├─LayerNorm: 1-10                                  [32, 92, 256]             512\n",
      "├─Linear: 1-11                                     [32, 92, 1000]            257,000\n",
      "====================================================================================================\n",
      "Total params: 27,388,368\n",
      "Trainable params: 27,388,368\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (Units.TERABYTES): 1.42\n",
      "====================================================================================================\n",
      "Input size (MB): 20.23\n",
      "Forward/backward pass size (MB): 15579.68\n",
      "Params size (MB): 88.50\n",
      "Estimated Total Size (MB): 15688.41\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "model_config = config['model'].copy()\n",
    "model_config.update({\n",
    "    'max_len': max_len,\n",
    "    'num_classes': Tokenizer.vocab_size\n",
    "})\n",
    "\n",
    "model = EncoderDecoderTransformer(**model_config)\n",
    "\n",
    "# Get some inputs from the train dataloader\n",
    "for batch in train_loader:\n",
    "    padded_feats, padded_shifted, padded_golden, feat_lengths, transcript_lengths = batch\n",
    "    break\n",
    "\n",
    "\n",
    "model_stats = summary(model, input_data=[padded_feats, padded_shifted, feat_lengths, transcript_lengths])\n",
    "print(model_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize Trainer\n",
    "- `TA TODO`: Please change the run name to the run name that was assigned to you in the ablation sheet for easy referencing.\n",
    "- `NOTE`: `optimizer` gets initialized in the `trainer` constructor based on the config.\n",
    "\n",
    "If you need to reload the model from a checkpoint, you can do so by calling the `load_checkpoint` method.\n",
    "\n",
    "```python\n",
    "checkpoint_path = \"path/to/checkpoint.pth\"\n",
    "trainer.load_checkpoint(checkpoint_path)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = ASRTrainer(\n",
    "    model=model,\n",
    "    tokenizer=Tokenizer,\n",
    "    config=config,\n",
    "    run_name=\"Puru-Test-Cold-Start-PSC\",\n",
    "    config_file=\"config.yaml\",\n",
    "    device=device\n",
    ")\n",
    "\n",
    "checkpoint_path = \"/ocean/projects/cis220031p/psamal/expts/Puru-Pretrained-Decoder-Test/checkpoints/checkpoint-best-metric-model.pth\"\n",
    "trainer.load_checkpoint(checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train\n",
    "- `TA TODO`: You can set your epochs here or in the config. If you set in config, make sure you remove the epoch argument here.\n",
    "- `NOTE`: A `scheduler` gets initialized in this call based on the config. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train(train_loader, val_loader, epochs=60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate\n",
    "\n",
    "- `TA TODO`: There will be 3 sequential evaluations here: with greedy decoding and beam search decoding with beam sizes 10 and 20.\n",
    "- `TA TODO`: Make sure you report the results for each of these cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"hw4p2_sol.json\", \"r\") as f:\n",
    "    solution = json.load(f)\n",
    "\n",
    "results = trainer.evaluate(test_loader, solution, max_length=max_transcript_len)\n",
    "\n",
    "# Cleanup\n",
    "trainer.cleanup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Strategy 2: Progressive Trainer\n",
    "\n",
    "- `TA TODO`: Run this section if you are assigned the `Progressive-Train` task. This section is a bit more involved. Read carefully. Reach out if you require any clarifications.\n",
    "\n",
    "In this mode of training, you will start with a model with only 1 encoder and 1 decoder layer, and then increase the number of layers after every pretrain iteration, optionally freezing the previous layers and scheduling regularization such as dropout  and label smoothing, which you will keep low or disabled initially and then later enable. Finally, you will unfreeze all layers and train the model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Load (Default)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = config['model'].copy()\n",
    "model_config.update({\n",
    "    'max_len': max_len,\n",
    "    'num_classes': Tokenizer.vocab_size\n",
    "})\n",
    "\n",
    "model = EncoderDecoderTransformer(**model_config)\n",
    "\n",
    "# Get some inputs from the train dataloader\n",
    "for batch in train_loader:\n",
    "    padded_feats, padded_shifted, padded_golden, feat_lengths, transcript_lengths = batch\n",
    "    break\n",
    "\n",
    "\n",
    "model_stats = summary(model, input_data=[padded_feats, padded_shifted, feat_lengths, transcript_lengths])\n",
    "print(model_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize Progressive Trainer\n",
    "- `TA TODO`: Please change the run name to the run name that was assigned to you in the ablation sheet for easy referencing.\n",
    "- `NOTE`: `optimizer` gets initialized in the `trainer` constructor based on the config.\n",
    "\n",
    "If you need to reload the model from a checkpoint, you can do so by calling the `load_checkpoint` method.\n",
    "\n",
    "```python\n",
    "checkpoint_path = \"path/to/checkpoint.pth\"\n",
    "trainer.load_checkpoint(checkpoint_path)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = ProgressiveTrainer(\n",
    "    model=model,\n",
    "    tokenizer=Tokenizer,\n",
    "    config=config,\n",
    "    run_name=\"Puru-Progressive-Test\",\n",
    "    config_file=\"config.yaml\",\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `TA TODO`: Define your training stages\n",
    "The `ProgressiveTrainer` class implements a curriculum learning approach where model complexity and regularization are gradually increased through defined training stages.\n",
    "\n",
    "##### Stage Configuration\n",
    "\n",
    "Each stage is defined as a dictionary with the following parameters:\n",
    "```python\n",
    "{\n",
    "    'name': str,                        # Name of the training stage\n",
    "    'epochs': int,                      # Number of epochs to train in this stage\n",
    "    'encoder_active_layers': List[int], # Which encoder layers to use\n",
    "    'decoder_active_layers': List[int], # Which decoder layers to use\n",
    "    'encoder_freeze': List[bool],       # Whether to freeze each encoder layer\n",
    "    'decoder_freeze': List[bool],       # Whether to freeze each decoder layer\n",
    "    'dropout': float,                   # Dropout rate for this stage\n",
    "    'label_smoothing': float,           # Label smoothing value\n",
    "    'data_subset': float                # Fraction of training data to use (0.0-1.0)\n",
    "}\n",
    "```\n",
    "\n",
    "It is best understood by an example. Here is a breakdown of the stages defined below for a model with 6 encoder and 6 decoder layers:\n",
    "\n",
    "- `Initial (1 layers)`: \n",
    "   - This stage starts with a model with only 1 encoder and 1 decoder layer. \n",
    "   - No freezing or regularization is applied. \n",
    "   - It uses 20% of the training data.\n",
    "- `2 layers`: \n",
    "   - This stage increases the number of layers to 2 for both the encoder and decoder. \n",
    "   - The previous layer (encoder layer 1 and decoder layer 1) are frozen. \n",
    "   - No regularization is applied. \n",
    "   - It uses 20% of the training data.\n",
    "- `4 layers`: \n",
    "   - This stage increases the number of layers to 4 for both the encoder and decoder. \n",
    "   - The previous layers (encoder layers 1 and 2 and decoder layers 1 and 2) are frozen. \n",
    "   - Dropout is set to 0.05 and label smoothing is set to 0.0. \n",
    "   - It uses 20% of the training data.\n",
    "- `All 6 layers`: \n",
    "   - This stage uses all 6 encoder and 6 decoder layers. \n",
    "   - The 4 previous layers are frozen and the last 2 layers are trained. \n",
    "   - Dropout is set to 0.1 and label smoothing is set to 0.0. \n",
    "   - It uses 20% of the training data.\n",
    "- `Final (with label smoothing)`: \n",
    "   - This stage uses all 6 encoder and 6 decoder layers. \n",
    "   - All layers are unfrozen and trained. \n",
    "   - Dropout is set to 0.1 and label smoothing is set to 0.1. \n",
    "   - It uses 20% of the training data.    \n",
    "\n",
    "`TA TODO`: Define your stages here. The design is left to you. Also, the active_layers do not have to be contiguous. For example, for stage 2, you could have had layer 1 and 6 as active layers. \n",
    "\n",
    "##### Important Notes\n",
    "- Ensure `encoder_freeze` and `decoder_freeze` lists match the length of their respective `active_layers`\n",
    "- `data_subset` should be between 0 and 1\n",
    "- Stage transitions are handled automatically by the trainer\n",
    "- The same optimizer and scheduler are used for all stages so keep that in mind while setting the learning rates and other parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Example with a model with 6 encoder and 6 decoder layers\n",
    "stages = [\n",
    "            {\n",
    "                'name': 'Initial (1 layers)',\n",
    "                'epochs': 5,\n",
    "                'encoder_active_layers': list(range(1)),  # layers 1 \n",
    "                'decoder_active_layers': list(range(1)),  # layers 1\n",
    "                'encoder_freeze': [False],\n",
    "                'decoder_freeze': [False],\n",
    "                'dropout': 0.0,\n",
    "                'label_smoothing': 0.0,\n",
    "                'data_subset': 0.2\n",
    "            },\n",
    "            {\n",
    "                'name': '2 layers',\n",
    "                'epochs': 5,\n",
    "                'encoder_active_layers': list(range(2)),\n",
    "                'decoder_active_layers': list(range(2)),\n",
    "                'encoder_freeze': [True, False],\n",
    "                'decoder_freeze': [True, False],\n",
    "                'dropout': 0.0,\n",
    "                'label_smoothing': 0.0,\n",
    "                'data_subset': 0.2\n",
    "            },\n",
    "            {\n",
    "                'name': '4 layers',\n",
    "                'epochs': 5,\n",
    "                'encoder_active_layers': list(range(4)),\n",
    "                'decoder_active_layers': list(range(4)),\n",
    "                'encoder_freeze': [True, True, False, False],\n",
    "                'decoder_freeze': [True, True, False, False],\n",
    "                'dropout': 0.05,\n",
    "                'label_smoothing': 0.0,\n",
    "                'data_subset': 0.2\n",
    "            },\n",
    "            {\n",
    "                'name': 'All 6 layers',\n",
    "                'epochs': 5,\n",
    "                'encoder_active_layers': list(range(6)),\n",
    "                'decoder_active_layers': list(range(6)),\n",
    "                'encoder_freeze': [True, True, True, True, False, False],\n",
    "                'decoder_freeze': [True, True, True, True, False, False],\n",
    "                'dropout': 0.1,\n",
    "                'label_smoothing': 0.0,\n",
    "                'data_subset': 0.2\n",
    "            },\n",
    "            {\n",
    "                'name': 'Final (with label smoothing)',\n",
    "                'epochs': 5,\n",
    "                'encoder_active_layers': list(range(6)),\n",
    "                'decoder_active_layers': list(range(6)),\n",
    "                'encoder_freeze': [False, False, False, False, False, False],\n",
    "                'decoder_freeze': [False, False, False, False, False, False],\n",
    "                'dropout': 0.1,\n",
    "                'label_smoothing': 0.1,\n",
    "                'data_subset': 0.2\n",
    "            }\n",
    "        ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`TA TODO`: You might want to revisit and change the settings of your optimizer and scheduler. \n",
    "- Just go back up, change the config optimizer and scheduler parameters, and return to this cell. \n",
    "- The same optimizer and scheduler are used for all stages so keep that in mind while setting the learning rates and other parameters. \n",
    "- The example below assumes that the same subset of the training data is used for all stages, this is the easiest way to do it. \n",
    "- I would not recommend having variable data subsets for each stage without understanding the `ProgressiveTrainer` and it's parent `ASRTrainer` classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create scheduler before progressive training\n",
    "trainer.optimizer = create_optimizer(model, config['optimizer'])\n",
    "subset_train_dataloader = trainer.get_subset_dataloader(train_loader, stages[0]['data_subset'])\n",
    "trainer.scheduler = create_scheduler(trainer.optimizer, config['scheduler'], subset_train_dataloader, gradient_accumulation_steps=config['training']['gradient_accumulation_steps'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Progressively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.progressive_train(train_loader, val_loader, stages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unfreeze all layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        param.requires_grad = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reload Optimizer and Scheduler\n",
    "- `TA TODO`: You might want to revisit and change the settings of your optimizer and scheduler. \n",
    "- Just go back up, change the config optimizer and scheduler parameters, and return to this cell. \n",
    "- The same optimizer and scheduler are used for all stages so keep that in mind while setting the learning rates and other parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create scheduler before full training\n",
    "trainer.optimizer = create_optimizer(model, config['optimizer'])\n",
    "trainer.scheduler = create_scheduler(trainer.optimizer, config['scheduler'], train_loader, gradient_accumulation_steps=config['training']['gradient_accumulation_steps'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Full\n",
    "- `TA TODO`: You can set your epochs here or in the config. If you set in config, make sure you remove the epoch argument here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train(train_loader, val_loader, epochs=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate\n",
    "\n",
    "- `TA TODO`: There will be 3 sequential evaluations here: with greedy decoding and beam search decoding with beam sizes 10 and 20.\n",
    "- `TA TODO`: Make sure you report the results for each of these cases.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"hw4p2_sol.json\", \"r\") as f:\n",
    "    solution = json.load(f)\n",
    "\n",
    "results = trainer.evaluate(test_loader, solution, max_length=max_transcript_len)\n",
    "\n",
    "# Cleanup\n",
    "trainer.cleanup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Strategy 3: Pretrained Decoder Initialized \n",
    "\n",
    "- `TA TODO`: Run this section if you are assigned the `Decoder-Initialized Train` task. This section is a bit more involved. Read carefully. Reach out if you require any clarifications.\n",
    "\n",
    "In this mode of training, you will: \n",
    "- Initialize the Encoder-Decoder Transformer with the shared weights from a pretrained Decoder-Only Transformer (self-attn's, ffn's, etc). \n",
    "- You will then first freeze these pre-trained weights and train the encoder and just the cross-attention decoder layers on the ASR task. \n",
    "- After that, you will unfreeze all weights and train the model, optionally setting a lower learning rate for the pre-trained weights.\n",
    "\n",
    "`NOTE`: You can get a bit adventurous if you'd like, for ex, combining this with the progressive training strategy. It will be well appreciated but is not required."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decoder-Only Initialized Load\n",
    "\n",
    "- `TA TODO`: Be sure to set the `decoder_checkpoint` below to the path of the `COMPATIBLE` decoder checkpoint you trained during your `HW4P1` ablation study. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = config['model'].copy()\n",
    "\n",
    "# TODO: Set the path to the decoder checkpoint.\n",
    "decoder_checkpoint = \"/path/to/decoder/checkpoint.pth\"\n",
    "model_config.update({\n",
    "    'max_len': max_len,\n",
    "    'num_classes': Tokenizer.vocab_size\n",
    "})\n",
    "\n",
    "model, param_info = EncoderDecoderTransformer.from_pretrained_decoder(\n",
    "    decoder_checkpoint_path=decoder_checkpoint,\n",
    "    config=model_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Freeze Pre-trained Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "transferred_params = [name for (name, _) in param_info['transferred']]\n",
    "for name, param in model.named_parameters():\n",
    "    if name in transferred_params:\n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize Trainer\n",
    "- `TA TODO`: Please change the run name to the run name that was assigned to you in the ablation sheet for easy referencing.\n",
    "- `NOTE`: `optimizer` gets initialized in the `trainer` constructor based on the config.\n",
    "\n",
    "If you need to reload the model from a checkpoint, you can do so by calling the `load_checkpoint` method.\n",
    "\n",
    "```python\n",
    "checkpoint_path = \"path/to/checkpoint.pth\"\n",
    "trainer.load_checkpoint(checkpoint_path)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = ASRTrainer(\n",
    "    model=model,\n",
    "    tokenizer=Tokenizer,\n",
    "    config=config,\n",
    "    run_name=\"Puru-Pretrained-Decoder-Test\",\n",
    "    config_file=\"config.yaml\",\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Encoder and Cross-Attention Decoder Layers with frozen pre-trained weights\n",
    "- `TA TODO`: You can set your epochs here or in the config. If you set in config, make sure you remove the epoch argument here.\n",
    "- `TA TODO`: You might want to revisit and change the settings of your optimizer and scheduler. \n",
    "- Just go back up, change the config optimizer and scheduler parameters, and return to this cell. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train(train_loader, val_loader, epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unfreeze all weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "transferred_params = [name for (name, _) in param_info['transferred']]\n",
    "for name, param in model.named_parameters():\n",
    "    if name in transferred_params:\n",
    "        param.requires_grad = True\n",
    "\n",
    "# Check that all parameters are being trained\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        assert param.requires_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reload Optimizer and Scheduler\n",
    "- `TA TODO`: You might want to revisit and change the settings of your optimizer and scheduler. \n",
    "- Just go back up, change the config optimizer and scheduler parameters, and return to this cell. \n",
    "- `TA TODO`: We are creating two separate groups for the pre-trained and new parameters. This is because we will experiment with different learning rates for the pre-trained and new parameters. Set the `lr_factor` below based on your desired ratio of learning rates for the pre-trained and new parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create diffrent groups for the pre-trained and new parameters\n",
    "transfered_patterns = [name for (name, param) in param_info['transferred']]\n",
    "new_patterns = [name for (name, param) in param_info['new']]\n",
    "lr_factor = 0.1 # TODO: Set\n",
    "\n",
    "\n",
    "optimizer_config = config['optimizer']\n",
    "optimizer_config['param_groups'] = [\n",
    "    {\n",
    "        'name': 'transferred_params',\n",
    "        'patterns': transfered_patterns,\n",
    "        'lr': config['optimizer']['lr'] * lr_factor # TODO: Set\n",
    "    },\n",
    "    {\n",
    "        'name': 'new_params',\n",
    "        'patterns': new_patterns,\n",
    "        'lr': config['optimizer']['lr']\n",
    "    }\n",
    "]\n",
    "trainer.optimizer = create_optimizer(model, optimizer_config)\n",
    "trainer.scheduler = create_scheduler(trainer.optimizer, config['scheduler'], train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Full\n",
    "- `TA TODO`: You can set your epochs here or in the config. If you set in config, make sure you remove the epoch argument here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train(train_loader, val_loader, epochs=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"hw4p2_sol.json\", \"r\") as f:\n",
    "    solution = json.load(f)\n",
    "\n",
    "results = trainer.evaluate(test_loader, solution, max_length=max_transcript_len)\n",
    "\n",
    "# Cleanup\n",
    "trainer.cleanup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shallow Fusion Inference\n",
    "\n",
    "Here you will use an external language model (i.e. the one you trained in HW4P1) to potentially improve the ASR performance of your model. On a high level, each step of beam search will involve a log-linear interpolation between the ASR model's logits and the language model's logits.\n",
    "\n",
    "- `TA TODO`: Set the `path_to_lm_checkpoint` below to the path of the `COMPATIBLE` language model checkpoint you trained during your `HW4P1` ablation study. By compatible, we mean that the tokenization type should match the tokenizer used in the ASR model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Set the path to your best performing LM checkpoint from HW4P1 ablation study.\n",
    "# NOTE: The tokenization type should match the tokenizer used in the ASR model.\n",
    "path_to_lm_checkpoint = \"/ocean/projects/cis220031p/psamal/expts/test-lm/checkpoints/checkpoint-best-metric-model.pth\"\n",
    "\n",
    "# Load the LM checkpoint\n",
    "lm_dict = torch.load(path_to_lm_checkpoint, map_location=trainer.device, weights_only=True)\n",
    "\n",
    "# Get the model config\n",
    "lm_model_config = lm_dict['config']['model']\n",
    "lm_max_len = lm_dict['model_state_dict']['positional_encoding.pe'].shape[1]\n",
    "lm_model_config.update({\n",
    "    'max_len': lm_max_len,\n",
    "    'num_classes': Tokenizer.vocab_size\n",
    "})\n",
    "\n",
    "# Initialize the LM model\n",
    "lm_model = DecoderOnlyTransformer(**lm_model_config)\n",
    "\n",
    "# Get some inputs from the train dataloader\n",
    "for batch in train_loader:\n",
    "    padded_feats, padded_shifted, padded_golden, feat_lengths, transcript_lengths = batch\n",
    "    break\n",
    "\n",
    "\n",
    "model_stats = summary(lm_model, input_data=[padded_shifted, transcript_lengths])\n",
    "print(model_stats)\n",
    "\n",
    "lm_model.load_state_dict(lm_dict['model_state_dict'], strict=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference\n",
    "- `TA TODO`: Set the `lm_weight` below to determine the weight given to the LM model's predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"hw4p2_sol.json\", \"r\") as f:\n",
    "    solution = json.load(f)\n",
    "\n",
    "lm_weight = 0.5 # TODO: Set the weight for the LM model\n",
    "\n",
    "# Define the recognition config: Beam search with width 10 + LM blending\n",
    "recognition_config = {\n",
    "    'num_batches': None,\n",
    "    'temperature': 1.0,\n",
    "    'repeat_penalty': 1.0,\n",
    "    'lm_weight': lm_weight,\n",
    "    'lm_model': lm_model,\n",
    "    'beam_width': 10,\n",
    "}\n",
    "\n",
    "# Recognize with the shallow fusion config\n",
    "config_name = \"shallow-fusion\"\n",
    "print(f\"Evaluating with {config_name} config\")\n",
    "results = trainer.recognize(test_loader, recognition_config, config_name=config_name, max_length=min(max_transcript_len, lm_max_len))\n",
    "assert len(results) == len(solution)           \n",
    "\n",
    "\n",
    "# Calculate metrics on full batch\n",
    "generated = [r['generated'] for r in results]\n",
    "metrics = trainer._calculate_asr_metrics(solution, generated)\n",
    "print(\"-\"*50)\n",
    "print(f\"Config: {config_name}\")\n",
    "print(f\"WER: {metrics['wer']:.2f}%\")\n",
    "print(f\"CER: {metrics['cer']:.2f}%\")\n",
    "print(f\"Word Distance: {metrics['word_dist']:.2f}\")\n",
    "print(\"-\"*50)\n",
    "trainer._save_generated_text(results, f'test_{config_name}_results')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
